{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Executor with force function callign\n",
    "\n",
    "We can make the agent use a tool at the beginning of its execution using langgraph. This is useful when you want to force agents to call particular tools, but still want flexibility of what happens after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from typing import TypedDict, Sequence, Annotated\n",
    "import operator\n",
    "import json\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base  import create_openai_functions_agent\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the tools that would be used by our agent and create the agent\n",
    "In this first section we will be defining our agent to use the Tavily search tool, and we will be pulling the openai function agent prompt from langchain hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [TavilySearchResults(max_results=1), PythonREPLTool()]\n",
    "llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent_runnable = create_openai_functions_agent(llm, tools=tools, prompt=prompt)\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the graph state\n",
    "In this section we will be defining the state of our langchain graph, the state is the value is passed across all the nodes in the graph,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    agent_outcome: Annotated[Union[AgentAction, AgentFinish, None], operator.setitem]\n",
    "    intermediate_steps: Annotated[Sequence[Tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the nodes\n",
    "We now need to define a few different nodes in our graph. In `langgraph`, a node can be either a function or a runnable, There are two main noes we need for this\n",
    "- The agent\n",
    "- The function invocation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "\n",
    "def run_agent(state: AgentState):\n",
    "    agent_outcome = agent_runnable.invoke(state)\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "\n",
    "def run_tool(state: AgentState):\n",
    "    agent_action = state[\"agent_outcome\"]\n",
    "    result = tool_executor.invoke(agent_action)\n",
    "    return {\"intermediate_steps\": [(agent_action, str(result))]}\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    agent_outcome = state[\"agent_outcome\"]\n",
    "    \n",
    "    if isinstance(agent_outcome, AgentFinish):\n",
    "        return \"end\"\n",
    "    \n",
    "    else:\n",
    "        return \"contine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentActionMessageLog\n",
    "\n",
    "\n",
    "def first_agent(state: AgentState):\n",
    "    action = AgentActionMessageLog(\n",
    "        tool=\"tavily_search_results_json\",\n",
    "        tool_input=state[\"input\"],\n",
    "        log=\"\",\n",
    "        message_log=[]\n",
    "    )\n",
    "    return {\"agent_outcome\": action}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the graph\n",
    "Now that we have individualy defined all the tools, agents, state and nodes, we can then put it all together into defining the langgraph graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"starting\", first_agent)\n",
    "workflow.add_node(\"action\", run_tool)\n",
    "\n",
    "\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "workflow.set_entry_point(\"starting\")\n",
    "workflow.add_edge(\"starting\", \"action\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"action\", \"end\": END})\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the langgraph graph\n",
    "\n",
    "Now we have the graph defined and compiled, we can now use it to perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for node starting\n",
      "----\n",
      "{'agent_outcome': AgentActionMessageLog(tool='tavily_search_results_json', tool_input='what is the weather in sf?', log='', message_log=[])}\n",
      "----\n",
      "output for node action\n",
      "----\n",
      "{'intermediate_steps': [(AgentActionMessageLog(tool='tavily_search_results_json', tool_input='what is the weather in sf?', log='', message_log=[]), '[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"Weather in San Francisco is {\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1708084290, \\'localtime\\': \\'2024-02-16 3:51\\'}, \\'current\\': {\\'last_updated_epoch\\': 1708083900, \\'last_updated\\': \\'2024-02-16 03:45\\', \\'temp_c\\': 10.0, \\'temp_f\\': 50.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 100, \\'wind_dir\\': \\'E\\', \\'pressure_mb\\': 1021.0, \\'pressure_in\\': 30.16, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 89, \\'cloud\\': 75, \\'feelslike_c\\': 9.2, \\'feelslike_f\\': 48.5, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 1.0, \\'gust_mph\\': 7.5, \\'gust_kph\\': 12.1}}\"}]')]}\n",
      "----\n",
      "output for node agent\n",
      "----\n",
      "{'agent_outcome': AgentFinish(return_values={'output': ''}, log='')}\n",
      "----\n",
      "output for node __end__\n",
      "----\n",
      "{'input': 'what is the weather in sf?', 'chat_history': [], 'agent_outcome': AgentFinish(return_values={'output': ''}, log=''), 'intermediate_steps': [(AgentActionMessageLog(tool='tavily_search_results_json', tool_input='what is the weather in sf?', log='', message_log=[]), '[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"Weather in San Francisco is {\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1708084290, \\'localtime\\': \\'2024-02-16 3:51\\'}, \\'current\\': {\\'last_updated_epoch\\': 1708083900, \\'last_updated\\': \\'2024-02-16 03:45\\', \\'temp_c\\': 10.0, \\'temp_f\\': 50.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 100, \\'wind_dir\\': \\'E\\', \\'pressure_mb\\': 1021.0, \\'pressure_in\\': 30.16, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 89, \\'cloud\\': 75, \\'feelslike_c\\': 9.2, \\'feelslike_f\\': 48.5, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 1.0, \\'gust_mph\\': 7.5, \\'gust_kph\\': 12.1}}\"}]')]}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input\": \"what is the weather in sf?\", \n",
    "    \"chat_history\": []\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output for node {key}\")\n",
    "        print(\"----\")\n",
    "        print(value)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
