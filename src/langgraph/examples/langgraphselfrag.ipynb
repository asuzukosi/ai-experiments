{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-RAG\n",
    "Self-reflection can enhance RAG, enabling correction of poor quality retrieval or generations.\n",
    "Several recent papers focus on thes theme, but implementing the ideas can be tricky. \n",
    "\n",
    "Self RAG details\n",
    "\n",
    "Self-RAG is a recent paper that introduces an interesting approach for self-reflection RAG.\n",
    "1. Should I retrieve for retriever, `R`\n",
    "   - Token: Retrieve\n",
    "   - Input `x (question)` or `x (question)`, `y(generation)`\n",
    "   - Decides when to retrieve `D` chunks with `R`\n",
    "   - output `yes, no, continue`\n",
    "2. Are the retrieved pages `D` relevant to the question `x`\n",
    "   - Token: `ISREL`\n",
    "   - Input (`x (question)`, `d(chunk)` for `d` in `D`)\n",
    "   - `d` provides useful information to solve `x`\n",
    "   - Output: `relevant, irrelevant`\n",
    "3. Are the LLM generation from each chunk in `D` is relevant to the chunk (hallucinations, etc)\n",
    "   - Token: `ISSUP`\n",
    "   - Input `x (question)`, `d(chunk)`, `y(generation)` for `d` in `D`\n",
    "   - All of the verification-worthy statements in `y(generation)` are supported by `d`\n",
    "   - Output: `{fully supported, partially supported, no support}`\n",
    "4. The LLM generation from each chunk in `D` is a useful response to `x(question)`\n",
    "    - Token: `ISREL`\n",
    "    - Input `x(question)`, `y(generation)` for `d` in `D`\n",
    "    - `y(generation)` is useful response to `x(question)`\n",
    "    - output:  `{5, 4, 3, 2, 1}`\n",
    "\n",
    "We can represent this as a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstores = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstores.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State\n",
    "We will define a graph.\n",
    "Our state will be `dict`\n",
    "We can access this from any graph node as `state['keys']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph\n",
    "    \"\"\"\n",
    "    keys: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nodes and Edges\n",
    "Each `node` will modify the `state`\n",
    "Each `edge` will choose which `node` to call next\n",
    "We can lay out `self-RAG` as a graph\n",
    "Here is our graph flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: GraphState):\n",
    "    \"\"\" \n",
    "    Retrieve documents\n",
    "    \"\"\"\n",
    "    print(\"--retrieve--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Generate answer\n",
    "    \"\"\"\n",
    "    print(\"--generate--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": documents,\n",
    "            \"generation\": generation,\n",
    "            \"question\": question,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determin whether the retrieved documents are relevant to the question\n",
    "    \"\"\"\n",
    "    print(\"check relevance\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    class Grade(BaseModel):\n",
    "        \"\"\" Binary score for relevance check \"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "        \n",
    "    model = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    grade_tool_oai = convert_to_openai_tool(Grade)\n",
    "    \n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\n",
    "            \"name\": \"grade\"\n",
    "        }}\n",
    "    )\n",
    "    \n",
    "    parser_tool = PydanticToolsParser(tools=[Grade])\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "    \n",
    "    filtered_docs = []\n",
    "    \n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = chain.invoke({\"question\": question, \"context\": d.page_content})\n",
    "            grade = score[0].binary_score\n",
    "        except Exception as e: \n",
    "            grade = \"yes\"\n",
    "        \n",
    "        if grade == \"yes\":\n",
    "            print(\"document relevant\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"document not relevant\")\n",
    "            continue\n",
    "    \n",
    "    return {\"keys\": {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state: GraphState):\n",
    "    \"\"\" \n",
    "    Transform the query to produce a better question\n",
    "    \"\"\"\n",
    "    print(\"tranform query\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question: \"\"\",\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    \n",
    "    model = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "    return {    \n",
    "                \"keys\": {\n",
    "                \"documents\": documents,\n",
    "                \"question\": better_question\n",
    "        }}\n",
    "    \n",
    "    \n",
    "    \n",
    "def prepare_for_final_grade(state: GraphState):\n",
    "    \"\"\"\n",
    "    Passthrough state for final grade.\n",
    "    \"\"\"\n",
    "    print(\"final grade\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    \n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determine whether to generate an answer, or re-generate a question\n",
    "    \"\"\"\n",
    "    print(\"decide to generate\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    \n",
    "    if not filtered_documents:\n",
    "        print(\"tranform query\")\n",
    "        return \"transform_query\"\n",
    "    \n",
    "    else:\n",
    "        print(\"generate\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generate_v_documents(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document\n",
    "    \"\"\"\n",
    "    print(\"generation vs documents\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    \n",
    "    \n",
    "    class Grade(BaseModel):\n",
    "        \"\"\" \n",
    "        Binary score for relevance check\n",
    "        \"\"\"\n",
    "        binary_score: str = Field(description=\"Supported score 'yes' or 'no'\")\n",
    "        \n",
    "    model = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    grade_tool_oai = convert_to_openai_tool(Grade)\n",
    "    \n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\n",
    "            \"name\": \"grade\"\n",
    "        }}\n",
    "    )\n",
    "    \n",
    "    parser_tool = PydanticToolsParser(tools=[Grade])\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "        Here are the facts:\n",
    "        \\n ------- \\n\n",
    "        {documents} \n",
    "        \\n ------- \\n\n",
    "        Here is the answer: {generation}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by a set of facts.\"\"\",\n",
    "        input_variables=[\"generation\", \"documents\"]\n",
    "\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "    \n",
    "    score = chain.invoke({\"generation\": generation, \"documents\": documents, \"question\": question})\n",
    "    \n",
    "    grade = score[0].binary_score\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"move to final grade\")\n",
    "        return \"supported\"\n",
    "    \n",
    "    else:\n",
    "        print(\"not supported, generate again\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generate_v_question(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determine whether the generation addersses the question\n",
    "    \"\"\"\n",
    "    print(\"grade generation vs question\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    \n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    \n",
    "    class Grade(BaseModel):\n",
    "        \"\"\"\n",
    "        Binary score for relevance check\n",
    "        \"\"\"\n",
    "        binary_score: str = Field(description=\"Useful score 'yes' or 'no'\")\n",
    "        \n",
    "    model = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    grade_tool_oai = convert_to_openai_tool(Grade)\n",
    "    \n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice = {\n",
    "            \"type\" : \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"grade\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    parser_tool = PydanticToolsParser(tools=[Grade])\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" \n",
    "        You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "        Here is the answer:\n",
    "        \\n ------- \\n\n",
    "        {generation} \n",
    "        \\n ------- \\n\n",
    "        Here is the question: {question}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n",
    "        \"\"\",\n",
    "        input_variables=[\"generation\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "    \n",
    "    try:\n",
    "        score = chain.invoke({\"generation\": generation, \"question\": question})\n",
    "        grade = score[0].binary_score\n",
    "    except Exception as e:\n",
    "        grade = \"yes\"\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"decision useful\")\n",
    "        return \"useful\"\n",
    "    \n",
    "    else:\n",
    "        print(\"not useful\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Graph\n",
    "\n",
    "The just follows the flow we outlined in the figure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"prepare_for_final_grade\", prepare_for_final_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", \n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generate_v_documents,\n",
    "    {\n",
    "        \"supported\": \"prepare_for_final_grade\",\n",
    "        \"not supported\": \"generate\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"prepare_for_final_grade\",\n",
    "    grade_generate_v_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--retrieve--\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "check relevance\n",
      "document relevant\n",
      "document relevant\n",
      "document relevant\n",
      "document relevant\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "decide to generate\n",
      "generate\n",
      "--generate--\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "generation vs documents\n",
      "move to final grade\n",
      "final grade\n",
      "\"Node 'prepare_for_final_grade':\"\n",
      "'\\n---\\n'\n",
      "grade generation vs question\n",
      "decision useful\n",
      "\"Node '__end__':\"\n",
      "'\\n---\\n'\n",
      "(' In a LLM-powered autonomous agent system, memory is a key component that '\n",
      " 'works alongside the large language model (LLM) brain of the agent. There are '\n",
      " 'different types of memory:\\n'\n",
      " '\\n'\n",
      " '1. Sensory Memory: This is the earliest stage, providing the ability to '\n",
      " 'retain impressions of sensory information for up to a few seconds.\\n'\n",
      " \"2. Long-term memory, called the 'memory stream', is an external database \"\n",
      " 'that records a comprehensive list of agentsâ€™ experience in natural '\n",
      " 'language.\\n'\n",
      " '\\n'\n",
      " 'Additional memory mechanisms include a retrieval model for surfacing '\n",
      " 'context, a reflection mechanism for synthesizing memories into higher-level '\n",
      " 'inferences, and planning & reacting which translates reflections and '\n",
      " 'environment information into actions.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"keys\": {\"question\": \"Explain how the different types of agent memory work?\", \"documents\": []}}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"keys\"][\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
