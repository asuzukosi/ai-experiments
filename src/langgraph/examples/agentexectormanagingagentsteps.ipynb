{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Executor with managing agent steps\n",
    "In this example, we will be experimenting with the implementation of builing an agent executor that limits the number of chat messages that gets sent to the LLM during agent execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents.openai_functions_agent.base import create_openai_functions_agent\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, FunctionMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [TavilySearchResults(max_results=1), PythonREPLTool()]\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "agent_runnable = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, Union, Tuple\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: Annotated[str, operator.setitem]\n",
    "    chat_messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    agent_outcome: Annotated[Union[AgentAction, AgentFinish, None], operator.setitem]\n",
    "    intermediate_steps: Annotated[Sequence[Tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(state: AgentState):\n",
    "    inputs = state.copy()\n",
    "    inputs[\"chat_messages\"] = inputs[\"chat_messages\"][-5:]\n",
    "    inputs[\"intermediate_steps\"] = inputs[\"chat_messages\"][-5:]\n",
    "    \n",
    "    agent_outcome = agent_runnable.invoke(inputs)\n",
    "    return {\"agent_outcome\": agent_outcome}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tools(state: AgentState):\n",
    "    agent_action = state[\"agent_outcome\"]\n",
    "    result = tool_executor.invoke(agent_action)\n",
    "    return {\"intermediate_steps\": [(agent_action, str(result))]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    if isinstance(state[\"agent_outcome\"], AgentFinish):\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"action\", \"end\": END})\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_outcome': AgentFinish(return_values={'output': ''}, log='')}\n",
      "----\n",
      "{'input': 'what is the weather in sf?', 'chat_messages': [], 'agent_outcome': AgentFinish(return_values={'output': ''}, log=''), 'intermediate_steps': []}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"what is the weather in sf?\", \"intermediate_steps\": [], \"chat_messages\": []}\n",
    "for output in app.stream(inputs):\n",
    "    print(list(output.values())[0])\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
