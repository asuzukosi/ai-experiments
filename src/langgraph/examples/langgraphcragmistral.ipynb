{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrective RAG \n",
    "Self correction can enhance RAG, enabling correction of poor quality retrieval or generations.\n",
    "Several recent papers focus on this theme, but implementing the ideas can be tricky.\n",
    "Here we show how to implement self-relfective RAG using `Mistral` and `LangGraph`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing\n",
    "First, let'sindex a popular blog post on agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag-chroma\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corrective RAG\n",
    "Let's implement self-reflective RAG with some ideas from CRAG (Corrective RAG)\n",
    "- Grade documents for relevance relative to the question\n",
    "- If any are irrelevant, then we will supplement the context used for generation with web search\n",
    "- For web search, we will re-phrase the question and use Tavily API\n",
    "- We will then pass retrieved documents and web results to an LLM for final answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    keys: Dict[str, Any]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nodes and edges\n",
    "Each node in the graph we laid out above has a function. \n",
    "Each node will modify the state in some way.\n",
    "Each edge will choose which node to call next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: GraphState):\n",
    "    \"\"\" \n",
    "    Retrieve documents\n",
    "    \"\"\"\n",
    "    print(\"--RETRIEVE--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    \n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\n",
    "        \"documents\": documents,\n",
    "        \"local\": local,\n",
    "        \"question\": question\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Generate answer\n",
    "    \"\"\"\n",
    "    print(\"--GENERATE--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    \n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    model = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": documents,\n",
    "            \"question\": question,\n",
    "            \"generation\": generation\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determines whether the retrieved documents are relevant to the question. \n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "        input_variables=[\"question\", \"context\"])\n",
    "    \n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    \n",
    "    filtered_docs = []\n",
    "    search = \"No\"\n",
    "    \n",
    "    for d in documents:\n",
    "        score = chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"context\": d.page_content,\n",
    "        })\n",
    "        \n",
    "        grade = score[\"score\"]\n",
    "        \n",
    "        if grade == \"yes\":\n",
    "            print(\"-- document relevant --\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"-- document not relevant --\")\n",
    "            search = \"Yes\"\n",
    "            continue\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"local\": local,\n",
    "            \"run_web_search\": search\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state: GraphState):\n",
    "    \"\"\"\n",
    "    Transform the query to produce better question\n",
    "    \"\"\"\n",
    "    print(\"-- transform query --\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=dedent(\"\"\"\n",
    "        You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Provide an improved question without any premable, only respond with the updated question: \n",
    "        \"\"\"),\n",
    "        input_variables = [\"question\"]\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "    \n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": documents,\n",
    "            \"question\": better_question,\n",
    "            \"local\": local\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state: GraphState):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased qeustion using Tavily API\n",
    "    \"\"\"\n",
    "    print(\"-- web search --\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    \n",
    "    tool = TavilySearchResults(max_results=2)\n",
    "    \n",
    "    docs = tool.invoke({\"query\": question})\n",
    "    \n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    \n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": documents,\n",
    "            \"local\": local,\n",
    "            \"question\": question\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determin whether to generat an answer or re-generate a question for web search\n",
    "    \"\"\"\n",
    "    print(\"-- decide to generate --\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    search = state_dict[\"run_web_search\"]\n",
    "    \n",
    "    if search == \"Yes\":\n",
    "        print(\"--Transform qeury and run web search --\")\n",
    "        return \"transform_query\"\n",
    "    \n",
    "    else:\n",
    "        print(\"-- generate response --\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Graph\n",
    "This just follows the flow we outlined in the figure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# define the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "\n",
    "# build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\"grade_documents\", decide_to_generate, {\n",
    "    \"transform_query\": \"transform_query\",\n",
    "    \"generate\": \"generate\",\n",
    "})\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\",\"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the app\n",
    "We will now run our implemented graph and see how the instruction flows between the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--RETRIEVE--\n",
      "'Node retrieve :'\n",
      "'\\n --- \\n'\n",
      "---CHECK RELEVANCE---\n",
      "-- document relevant --\n",
      "-- document relevant --\n",
      "-- document relevant --\n",
      "-- document relevant --\n",
      "'Node grade_documents :'\n",
      "'\\n --- \\n'\n",
      "-- decide to generate --\n",
      "-- generate response --\n",
      "--GENERATE--\n",
      "'Node generate :'\n",
      "'\\n --- \\n'\n",
      "'Node __end__ :'\n",
      "'\\n --- \\n'\n",
      "(' In the context of LLM-powered autonomous agents, memory is categorized into '\n",
      " 'short-term and long-term memory. Short-term memory, also known as working '\n",
      " 'memory, stores information currently needed for complex tasks. It has a '\n",
      " 'capacity of around 7 items and lasts for 20-30 seconds. Long-term memory, on '\n",
      " 'the other hand, can store information for extended periods, even decades, '\n",
      " 'with an essentially unlimited storage capacity. It includes explicit or '\n",
      " 'declarative memory for consciously recalling facts and events, and implicit '\n",
      " 'or procedural memory for unconsciously storing skills and routines. In terms '\n",
      " 'of operation, short-term memory is utilized for in-context learning, while '\n",
      " 'long-term memory leverages an external vector store for retaining and '\n",
      " 'recalling infinite information over time.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"keys\": {\n",
    "        \"question\": \"Explain how the different types of agent memory work?\",\n",
    "        \"local\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Node {key} :\")\n",
    "        \n",
    "    pprint(\"\\n --- \\n\")\n",
    "    \n",
    "pprint(value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--RETRIEVE--\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "-- document not relevant --\n",
      "-- document not relevant --\n",
      "-- document not relevant --\n",
      "-- document not relevant --\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "-- decide to generate --\n",
      "--Transform qeury and run web search --\n",
      "-- transform query --\n",
      "\"Node 'transform_query':\"\n",
      "'\\n---\\n'\n",
      "-- web search --\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "--GENERATE--\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "\"Node '__end__':\"\n",
      "'\\n---\\n'\n",
      "(' The Transformer architecture is built on the concept of attention, which '\n",
      " 'assigns weights to various parts of an input sequence. This allows the model '\n",
      " 'to focus on different parts of the sequence and understand it better. The '\n",
      " 'role of attention in the Transformer architecture is crucial, as it enables '\n",
      " 'the model to selectively concentrate on relevant features within the input '\n",
      " 'data.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"keys\": {\n",
    "        \"question\": \"Explain how attention works in the transformer archticture?\",\n",
    "        \"local\": False,\n",
    "    }\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "        pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"keys\"][\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
