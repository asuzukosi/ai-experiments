{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self RAG\n",
    "Self - reflection can enhance RAG, enabling correction of poor quality retrieval or generations\n",
    "Several recent papers focused on this theme, but implementing the ideas can be tricky. This section focuses on using mistral models for\n",
    "the implementation of self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.gpt4all import GPT4AllEmbeddings\n",
    "\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "docs = WebBaseLoader(url).load()\n",
    "\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size =500,\n",
    "    chunk_overlap =100)\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = GPT4AllEmbeddings()\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "retreiver = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### State\n",
    "Every node in our graph will modify `state`, which is dict that contains values (`question`, `documents`, etc) relevant to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\" \n",
    "    Represents the state of our graph\n",
    "    \"\"\"\n",
    "    keys: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node and Edges\n",
    "Every node in the graph we laid out above is a function\n",
    "Each ndoe will modify the state in some way\n",
    "Each edge will choose which node to call next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\" \n",
    "    Retrieve documents\n",
    "    \"\"\"\n",
    "    print(\"--retrieve--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retreiver.get_relevant_documents(question)\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": documents,\n",
    "            \"question\": question\n",
    "        }\n",
    "    }\n",
    "    \n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "    \"\"\"\n",
    "    print(\"--generate--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\":{\n",
    "            \"documents\": documents,\n",
    "            \"question\": question,\n",
    "            \"generation\": generation\n",
    "        }\n",
    "    }\n",
    "    \n",
    "def grade_documents(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    \"\"\"\n",
    "    print(\"--check relevance--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\", \"context\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    \n",
    "    # score\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"context\": d.page_content,\n",
    "            }\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"-- document relevant --\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"document not relevant --\")\n",
    "            continue\n",
    "        \n",
    "    return {\n",
    "        \"keys\":{\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state: GraphState):\n",
    "    \"\"\"\n",
    "    Trasnform the query to produce a better question\n",
    "    \"\"\"\n",
    "    print(\"tranform query\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question:\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "    \n",
    "    return { \"keys\": {\n",
    "        \"documents\": documents,\n",
    "        \"question\": better_question\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_final_grade(state: GraphState):\n",
    "    \"\"\"Passthrough state for final grade\"\"\"\n",
    "    print(\"final grade\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    \n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": documents, \n",
    "            \"question\": question,\n",
    "            \"generation\": generation\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determines whether to generate an answer, or re-generate a question\n",
    "    \"\"\"\n",
    "    print(\"--decide to generate--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filter_documents = state_dict[\"question\"]\n",
    "    \n",
    "    if not filter_documents:\n",
    "        print(\"tranform query\")\n",
    "        return \"transform_query\"\n",
    "    \n",
    "    else:\n",
    "        print(\"--generate--\")\n",
    "        return \"generate\"\n",
    "    \n",
    "\n",
    "def grade_generation_v_documents(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document.\n",
    "    \"\"\"\n",
    "    print(\"grade generation vs documents\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "        Here are the facts:\n",
    "        \\n ------- \\n\n",
    "        {documents} \n",
    "        \\n ------- \\n\n",
    "        Here is the answer: {generation}\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "        input_variables=[\"generation\", \"documents\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    score = chain.invoke({\"generation\": generation, \"documents\": documents})\n",
    "    grade = score[\"score\"]\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"decision supported, move to final grade\")\n",
    "        return \"supported\"\n",
    "    \n",
    "    else:\n",
    "        print(\"generate again\")\n",
    "        return \"not supported\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_question(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determine whether the generation address the question\n",
    "    \"\"\"\n",
    "    print(\"--grade generation vs question--\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    \n",
    "    \n",
    "    llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "        Here is the answer:\n",
    "        \\n ------- \\n\n",
    "        {generation} \n",
    "        \\n ------- \\n\n",
    "        Here is the question: {question}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "        input_variables=[\"generation\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    score = chain.invoke({\"generation\": generation, \"question\": question})\n",
    "    grade = score[\"score\"]\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"decision useful\")\n",
    "        return \"useful\"\n",
    "    \n",
    "    else:\n",
    "        print(\"decision not useful\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the Graph\n",
    "This just follows the flow we outlined in the figure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"prepare_for_final_grade\", prepare_for_final_grade)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge('retrieve', \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents,\n",
    "    {\n",
    "        \"supported\": \"prepare_for_final_grade\",\n",
    "        \"not_supported\": \"generate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"prepare_for_final_grade\",\n",
    "    grade_generation_v_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run \n",
    "Now we will run the grpah with the provided inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--retrieve--\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "--check relevance--\n",
      "-- document relevant --\n",
      "-- document relevant --\n",
      "-- document relevant --\n",
      "-- document relevant --\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "--decide to generate--\n",
      "--generate--\n",
      "--generate--\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "grade generation vs documents\n",
      "decision supported, move to final grade\n",
      "final grade\n",
      "\"Node 'prepare_for_final_grade':\"\n",
      "'\\n---\\n'\n",
      "--grade generation vs question--\n",
      "decision useful\n",
      "\"Node '__end__':\"\n",
      "'\\n---\\n'\n",
      "(' The different types of agent memory include short-term memory, which is '\n",
      " 'used for in-context learning, and long-term memory, which retains and '\n",
      " 'recalls information over extended periods. Short-term memory is like the '\n",
      " \"model's working memory, while long-term memory is often supported by an \"\n",
      " 'external vector store and fast retrieval. Additionally, the agent can use '\n",
      " 'tools to call external APIs for extra information that is missing from the '\n",
      " 'model weights.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"keys\": {\"question\": \"Explain how the different types of agent memory work?\"}}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value['keys']['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
