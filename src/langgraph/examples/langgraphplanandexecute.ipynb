{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plan and Execute\n",
    "\n",
    "this notebook shows how to create 'plan-and-execute' style agent. This is heavily inspired by the plan-and-solve paper as well as babyAGI. \n",
    "The core idea is to first come up with a multi-step plan, and then go through that plan one item at a time. After accomplishing a particular task, you can then revisit the plan and modify as appropriate. This compares to a typical ReAct style agent where you are thinking one step at a time. Teh advantanges of plan-and-execute style agents are:\n",
    "- Explicit long term planning\n",
    "- Ability to use smaller/weaker models for the execturion step, only using larger/better models for the planning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Tools\n",
    "We will first define the tools we want to use. For this simple example, we will use a builti in search tool via Tavily. However, it is really easy to create your own tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tools = [TavilySearchResults(max_results=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defin our execution agent\n",
    "Now we will create the execution agent we want to use to execute tasks. Note that for this example, we will be using the same execution agent for each task, although this is not a requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "agent_runnable = create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_agent_executor\n",
    "\n",
    "agent_executor = create_agent_executor(agent_runnable, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'who is the winner of the us open',\n",
       " 'chat_history': [],\n",
       " 'agent_outcome': AgentFinish(return_values={'output': ''}, log=''),\n",
       " 'intermediate_steps': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"who is the winner of the us open\", \"chat_history\": []\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the state\n",
    "Let's now start by defining the state the track of this agent.\n",
    "\n",
    "First, we will need to track the current plan. Let's represent that as a list of strings. \n",
    "Next, we should track previously executed steps. Let's represent that as a list tuples, theres tuples will contain the step and then the result.\n",
    "\n",
    "Finally, we need to have some state to represent the final respons as well as the original input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Tuple, Annotated, TypedDict\n",
    "import operator\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps = Annotated[List[Tuple], operator.add]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Planning step\n",
    "Lets think about creating the planning step. This will use function calling to create a plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "class Plan(BaseModel):\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    For the given objective, come up with a simple step by step plan. \\\n",
    "    This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "    {objective}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "planner = create_structured_output_chain(Plan, ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"), planner_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'what is the hometown of the current Australia open winnner?',\n",
       " 'function': Plan(steps=['Search for the current Australia Open winner', 'Locate their hometown in their bio or profile', 'Return the hometown as the final answer'])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner.invoke({\n",
    "    \"objective\": \"what is the hometown of the current Australia open winnner?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-Plan Step\n",
    "Now, Let's create a step that re-does the plan blased on the result of the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import create_openai_fn_runnable\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"\n",
    "    Respone the user\n",
    "    \"\"\"\n",
    "    response: str\n",
    "    \n",
    "    \n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    template=\"\"\"For the given objective, come up with a simple step by step plan\n",
    "    This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "    Your objective was this:\n",
    "    {input}\n",
    "\n",
    "    Your original plan was this:\n",
    "    {plan}\n",
    "\n",
    "    You have currently done the follow steps:\n",
    "    {past_steps}\n",
    "\n",
    "    Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "replanner = create_openai_fn_runnable(\n",
    "    [Plan, Response],\n",
    "    ChatOpenAI(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"),\n",
    "    replanner_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Graph\n",
    "We can now create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_step(state: PlanExecute):\n",
    "    task = state[\"plan\"][0]\n",
    "    agent_response = await agent_executor.ainvoke({\"input\": task, \"chat_history\":[]})\n",
    "    return {\n",
    "        \"past_steps\": (task, agent_response[\"agent_outcome\"].return_values[\"output\"])\n",
    "    }\n",
    "    \n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"objective\": state[\"input\"]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke({\"objective\": state[\"input\"]})\n",
    "    if isinstance(output, Response):\n",
    "        return {\"response\": output.response}\n",
    "    else:\n",
    "        return {\"plan\": output.steps}\n",
    "    \n",
    "def should_end(state: PlanExecute):\n",
    "    if state[\"response\"]:\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\"replan\", should_end, {\n",
    "    True: END, \n",
    "    False: \"agent\"\n",
    "})\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the hometown of the 2024 Australia open winner?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:657\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_stream\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matransform(\n\u001b[1;32m    658\u001b[0m     input_stream(),\n\u001b[1;32m    659\u001b[0m     config,\n\u001b[1;32m    660\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    661\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    663\u001b[0m ):\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:675\u001b[0m, in \u001b[0;36mPregel.atransform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matransform\u001b[39m(\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    674\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform,\n\u001b[1;32m    678\u001b[0m         config,\n\u001b[1;32m    679\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    680\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    682\u001b[0m     ):\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1595\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1595\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1597\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   1598\u001b[0m         )\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1600\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:524\u001b[0m, in \u001b[0;36mPregel._atransform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    517\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    518\u001b[0m     futures,\n\u001b[1;32m    519\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    520\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    527\u001b[0m _apply_writes(\n\u001b[1;32m    528\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    529\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4076\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4071\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4072\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4073\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4074\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4075\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4077\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4078\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4079\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4080\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2087\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2087\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2088\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2089\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2090\u001b[0m             patch_config(\n\u001b[1;32m   2091\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2092\u001b[0m             ),\n\u001b[1;32m   2093\u001b[0m         )\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3527\u001b[0m, in \u001b[0;36mRunnableLambda.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3525\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3526\u001b[0m the_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafunc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m-> 3527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[1;32m   3528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[1;32m   3529\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, the_func),\n\u001b[1;32m   3531\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3532\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1295\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m coro \u001b[38;5;241m=\u001b[39m acall_func_with_variable_args(\n\u001b[1;32m   1292\u001b[0m     func, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1293\u001b[0m )\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1295\u001b[0m     output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[0;32m~/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3474\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3472\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3474\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acall_func_with_variable_args(\n\u001b[1;32m   3475\u001b[0m         cast(Callable, afunc), \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3476\u001b[0m     )\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mplan_step\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplan_step\u001b[39m(state: PlanExecute):\n\u001b[1;32m      9\u001b[0m     plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m planner\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplan\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'steps'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the 2024 Australia open winner?\"}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
