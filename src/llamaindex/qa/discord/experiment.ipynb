{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Discord\n",
    "In this document we will be exploring managing conversation information from discord using llama index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message:\n",
    "    def __init__(self, message_id, message_text, author, timestamp, parent_message=None, child_message=None):\n",
    "        self.message_id = message_id\n",
    "        self.message_text = message_text\n",
    "        self.author = author\n",
    "        self.timestamp = timestamp\n",
    "        self.parent_message = parent_message\n",
    "        self.child_message = child_message\n",
    "        \n",
    "    def set_child(self, message):\n",
    "        self.child_message = message\n",
    "        \n",
    "    def set_parent(self, message):\n",
    "        self.parent_message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    data = None\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToConvDocs(filename):\n",
    "    data = readFile(filename)\n",
    "    \n",
    "    messages: Dict[int, Message] = {}\n",
    "    for message in data[\"messages\"]:\n",
    "        _id = message[\"id\"]\n",
    "        text =  message[\"content\"]\n",
    "        message_type = message[\"type\"]\n",
    "        author = message[\"author\"][\"name\"]\n",
    "        timestamp = message[\"timestamp\"]\n",
    "        \n",
    "        if message_type in (\"ThreadCreated\", \"ChannelPinnedMessage\"):\n",
    "            continue\n",
    "        \n",
    "        messages[_id] = Message(_id, text, author, timestamp)\n",
    "        if message_type == \"Reply\":\n",
    "            parent_id = message[\"reference\"][\"messageId\"]\n",
    "            try:\n",
    "                messages[_id].set_parent(messages[parent_id])\n",
    "            except:\n",
    "                continue\n",
    "            messages[parent_id].set_child(messages[_id])\n",
    "            \n",
    "    convo_docs = []\n",
    "    for msg in messages.values():\n",
    "        if not msg.parent_message:\n",
    "            metadata = {\n",
    "                \"timestamp\": msg.timestamp,\n",
    "                \"id\": msg.message_id,\n",
    "                \"author\": msg.author\n",
    "            }\n",
    "            convo = \"\"\n",
    "            convo += msg.author + \":\\n\"\n",
    "            convo += msg.message_text + \"\\n\"\n",
    "            \n",
    "            curr_msg:Message = msg\n",
    "            is_thread = False\n",
    "            while curr_msg.child_message is not None:\n",
    "                is_thread = True\n",
    "                curr_msg = curr_msg.child_message\n",
    "                convo += curr_msg.author + \":\\n\"\n",
    "                convo += curr_msg.message_text + \"\\n\"\n",
    "\n",
    "            if is_thread:\n",
    "                convo_docs.append({\"thread\": convo, \"metadata\": metadata})\n",
    "                \n",
    "    with open(\"converation_docs.json\", \"w\") as f:\n",
    "        json.dump(convo_docs, f)\n",
    "    \n",
    "    print(\"Written information to convo docs\")\n",
    "    return\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "The point of this document is to explore how to handle data from ever expanding datasources. We are using an ever updating discord channel information to create this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help_channel_dump_06_02_23.json', 'help_channel_dump_05_25_23.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/help_channel_dump_05_25_23.json\", \"r\") as f:\n",
    "    data:Dict[Any, Any] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['guild', 'channel', 'dateRange', 'messages', 'messageCount'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of messages are  5087\n"
     ]
    }
   ],
   "source": [
    "print(\"the number of messages are \", len(data[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Message Keys:  dict_keys(['id', 'type', 'timestamp', 'timestampEdited', 'callEndedTimestamp', 'isPinned', 'content', 'author', 'attachments', 'embeds', 'stickers', 'reactions', 'mentions']) \n",
      "\n",
      "First Message:  If you're running into any bugs, issues, or you have questions as to how to best use GPT Index, put those here! \n",
      "- If it's a bug, let's also track as a GH issue: https://github.com/jerryjliu/gpt_index/issues. \n",
      "\n",
      "Last Message:  Hello there! How can I use llama_index with GPU?\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Message Keys: \", data[\"messages\"][0].keys(), \"\\n\")\n",
    "print(\"First Message: \", data[\"messages\"][0][\"content\"], \"\\n\")\n",
    "print(\"Last Message: \", data[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written information to convo docs\n"
     ]
    }
   ],
   "source": [
    "writeToConvDocs(\"./data/help_channel_dump_05_25_23.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"converation_docs.json\", \"r\") as f:\n",
    "    threads:List[Dict[Any, Any]] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['thread', 'metadata'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': '2023-01-02T03:36:04.191+00:00', 'id': '1059314106907242566', 'author': 'arminta7'} \n",
      "\n",
      "arminta7:\n",
      "Hello all! Thanks to GPT_Index I've managed to put together a script that queries my extensive personal note collection which is a local directory of about 20k markdown files. Some of which are very long. I work in this folder all day everyday, so there are frequent changes. Currently I would need to rerun the entire indexing (is that the correct term?) when I want to incorporate edits I've made. \n",
      "\n",
      "So my question is... is there a way to schedule indexing to maybe once per day and only add information for files that have changed? Or even just manually run it but still only add edits? This would make a huge difference in saving time (I have to leave it running overnight for the entire directory) as well as cost ðŸ˜¬. \n",
      "\n",
      "Excuse me if this is a dumb question, I'm not a programmer and am sort of muddling around figuring this out ðŸ¤“ \n",
      "\n",
      "Thank you for making this sort of project accessible to someone like me!\n",
      "ragingWater_:\n",
      "I had a similar problem which I solved the following way in another world:\n",
      "- if you have a list of files, you want something which says that edits were made in the last day, possibly looking at the last_update_time of the file should help you.\n",
      "- for decreasing the cost, I would suggest maybe doing a keyword extraction or summarization of your notes and generating an embedding for it. Take your NLP query and get the most similar file (cosine similarity by pinecone db should help, GPTIndex also has a faiss) this should help with your cost needs\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(threads[0][\"metadata\"], \"\\n\")\n",
    "print(threads[0][\"thread\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['thread', 'metadata'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['timestamp', 'id', 'author'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads[0][\"metadata\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now its time for the fun part: Create the index\n",
    "We will use this extracted data to create the index for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: List[Document] = []\n",
    "for thread in threads:\n",
    "    text = thread[\"thread\"]\n",
    "    _id = thread[\"metadata\"][\"id\"]\n",
    "    author = thread[\"metadata\"][\"author\"]\n",
    "    timestamp = thread[\"metadata\"][\"timestamp\"]\n",
    "    metadata = thread[\"metadata\"]\n",
    "    documents.append(Document(id_=_id, text=text, metadata={\"date\": timestamp}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_docs ingested:  767\n",
      "number of input documents:  767\n"
     ]
    }
   ],
   "source": [
    "print(\"ref_docs ingested: \", len(index.ref_doc_info))\n",
    "print(\"number of input documents: \", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RefDocInfo(node_ids=['ad0b9bf8-040e-405f-8e9e-e48d1101a9c4'], metadata={'date': '2023-01-02T03:36:04.191+00:00'})\n"
     ]
    }
   ],
   "source": [
    "thread_id = threads[0][\"metadata\"][\"id\"]\n",
    "print(index.ref_doc_info[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets persist the index so we do not have to reindex to save const\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets reload the index from the saved storage context\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index.indices import load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refreshing the index with new data\n",
    "If we are working with dynamically changing data, we would want a situation where we can automatically update our index without having to rebuild it, because that would by resource (time, money) consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON keys:  dict_keys(['guild', 'channel', 'dateRange', 'messages', 'messageCount']) \n",
      "\n",
      "Message Count:  5286 \n",
      "\n",
      "Sample Message Keys:  dict_keys(['id', 'type', 'timestamp', 'timestampEdited', 'callEndedTimestamp', 'isPinned', 'content', 'author', 'attachments', 'embeds', 'stickers', 'reactions', 'mentions']) \n",
      "\n",
      "First Message:  If you're running into any bugs, issues, or you have questions as to how to best use GPT Index, put those here! \n",
      "- If it's a bug, let's also track as a GH issue: https://github.com/jerryjliu/gpt_index/issues. \n",
      "\n",
      "Last Message:  Started a thread.\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/help_channel_dump_06_02_23.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"JSON keys: \", data.keys(), \"\\n\")\n",
    "print(\"Message Count: \", len(data[\"messages\"]), \"\\n\")\n",
    "print(\"Sample Message Keys: \", data[\"messages\"][0].keys(), \"\\n\")\n",
    "print(\"First Message: \", data[\"messages\"][0][\"content\"], \"\\n\")\n",
    "print(\"Last Message: \", data[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written information to convo docs\n"
     ]
    }
   ],
   "source": [
    "writeToConvDocs(\"./data/help_channel_dump_06_02_23.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread keys:  dict_keys(['thread', 'metadata']) \n",
      "\n",
      "{'timestamp': '2023-01-02T03:36:04.191+00:00', 'id': '1059314106907242566', 'author': 'arminta7'} \n",
      "\n",
      "arminta7:\n",
      "Hello all! Thanks to GPT_Index I've managed to put together a script that queries my extensive personal note collection which is a local directory of about 20k markdown files. Some of which are very long. I work in this folder all day everyday, so there are frequent changes. Currently I would need to rerun the entire indexing (is that the correct term?) when I want to incorporate edits I've made. \n",
      "\n",
      "So my question is... is there a way to schedule indexing to maybe once per day and only add information for files that have changed? Or even just manually run it but still only add edits? This would make a huge difference in saving time (I have to leave it running overnight for the entire directory) as well as cost ðŸ˜¬. \n",
      "\n",
      "Excuse me if this is a dumb question, I'm not a programmer and am sort of muddling around figuring this out ðŸ¤“ \n",
      "\n",
      "Thank you for making this sort of project accessible to someone like me!\n",
      "ragingWater_:\n",
      "I had a similar problem which I solved the following way in another world:\n",
      "- if you have a list of files, you want something which says that edits were made in the last day, possibly looking at the last_update_time of the file should help you.\n",
      "- for decreasing the cost, I would suggest maybe doing a keyword extraction or summarization of your notes and generating an embedding for it. Take your NLP query and get the most similar file (cosine similarity by pinecone db should help, GPTIndex also has a faiss) this should help with your cost needs\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"converation_docs.json\", \"r\") as f:\n",
    "    threads = json.load(f)\n",
    "print(\"Thread keys: \", threads[0].keys(), \"\\n\")\n",
    "print(threads[0][\"metadata\"], \"\\n\")\n",
    "print(threads[0][\"thread\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "\n",
    "for thread in threads:\n",
    "    _id = thread[\"metadata\"][\"id\"]\n",
    "    text = thread[\"thread\"]\n",
    "    timestamp =  thread[\"metadata\"][\"timestamp\"]\n",
    "    metadata = thread[\"metadata\"]\n",
    "    \n",
    "    new_documents.append(Document(id_=_id, text=text, metadata={\"date\": timestamp}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new documents:  13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of new documents: \", len(new_documents) - len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "refreshed_docs = index.refresh(\n",
    "    new_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newly inserted/refreshed docs:  17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of newly inserted/refreshed docs: \", sum(refreshed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of documents is  780\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of documents is \", len(refreshed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(refreshed_docs[-14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(id_='1110938122902048809', embedding=None, metadata={'date': '2023-05-24T14:31:28.732+00:00'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Siddhant Saurabh:\\nhey facing error\\n```\\n*error_trace: Traceback (most recent call last):\\n File \"/app/src/chatbot/query_gpt.py\", line 248, in get_answer\\n   context_answer = self.call_pinecone_index(request)\\n File \"/app/src/chatbot/query_gpt.py\", line 229, in call_pinecone_index\\n   self.source.append(format_cited_source(source_node.doc_id))\\n File \"/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py\", line 172, in doc_id\\n   return self.node.ref_doc_id\\n File \"/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py\", line 87, in ref_doc_id\\n   return self.relationships.get(DocumentRelationship.SOURCE, None)\\nAttributeError: \\'Field\\' object has no attribute \\'get\\'\\n```\\nwith latest llama_index 0.6.9\\n@Logan M @jerryjliu98 @ravitheja\\nLogan M:\\nHow are you inserting nodes/documents? That attribute on the node should be set automatically usually\\nSiddhant Saurabh:\\nI think this happened because of the error mentioned by me here https://discord.com/channels/1059199217496772688/1106229492369850468/1108453477081948280\\nI think we need to re-preprocessing for such nodes, right?\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(new_documents[-21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='1110938122902048809', embedding=None, metadata={'date': '2023-05-24T14:31:28.732+00:00'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Siddhant Saurabh:\\nhey facing error\\n```\\n*error_trace: Traceback (most recent call last):\\n File \"/app/src/chatbot/query_gpt.py\", line 248, in get_answer\\n   context_answer = self.call_pinecone_index(request)\\n File \"/app/src/chatbot/query_gpt.py\", line 229, in call_pinecone_index\\n   self.source.append(format_cited_source(source_node.doc_id))\\n File \"/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py\", line 172, in doc_id\\n   return self.node.ref_doc_id\\n File \"/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py\", line 87, in ref_doc_id\\n   return self.relationships.get(DocumentRelationship.SOURCE, None)\\nAttributeError: \\'Field\\' object has no attribute \\'get\\'\\n```\\nwith latest llama_index 0.6.9\\n@Logan M @jerryjliu98 @ravitheja\\nLogan M:\\nHow are you inserting nodes/documents? That attribute on the node should be set automatically usually\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
