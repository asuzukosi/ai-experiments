{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain\n",
    "The chain is the core component of the langchain package. In this section we would be doing a deep dive into \n",
    "understanding how the langchain chains work and how we can use them in our applications. \n",
    "An entire startup can be composed purely using a set of langchain chains. Without any additional overhead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smartphone X</td>\n",
       "      <td>I love my new Smartphone X! The camera quality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wireless Headphones Y</td>\n",
       "      <td>Wireless Headphones Y provide excellent sound ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smart Watch Z</td>\n",
       "      <td>Smart Watch Z exceeded my expectations! It has...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Product                                             Review\n",
       "0           Smartphone X  I love my new Smartphone X! The camera quality...\n",
       "1  Wireless Headphones Y  Wireless Headphones Y provide excellent sound ...\n",
       "2          Smart Watch Z  Smart Watch Z exceeded my expectations! It has..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for three products and their reviews\n",
    "data = {\n",
    "    'Product': ['Smartphone X', 'Wireless Headphones Y', 'Smart Watch Z'],\n",
    "    'Review': [\n",
    "        'I love my new Smartphone X! The camera quality is amazing, and the performance is top-notch. The battery life is also impressive.',\n",
    "        'Wireless Headphones Y provide excellent sound quality and are very comfortable to wear. The noise cancellation feature works like a charm.',\n",
    "        'Smart Watch Z exceeded my expectations! It has a sleek design, and the fitness tracking features are incredibly accurate. The battery life is outstanding.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain allows for the creation of the following kinds of chains\n",
    "* LLM Chain\n",
    "* Sequential Chain\n",
    "  * Simple Sequential Chain\n",
    "  * Sequential Chain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       " 'stream': False,\n",
       " 'n': 1,\n",
       " 'temperature': 0.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_7b = ChatOpenAI(model=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.0)\n",
    "mistral_7b._default_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_template = \"\"\" What is the best name to describe a company that makes {product}?\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(p_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=mistral_7b, prompt=prompt_template)\n",
    "product = \"shoes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are many possible names for a company that makes bags, and the \"best\" name would depend on various factors such as the type of bags being produced, the target market, and the brand image the company wants to project. Here are a few suggestions:\\n\\n1. Sackville & Co. - This name has a classic, sophisticated feel and could work well for a company that makes high-end leather bags or luggage.\\n2. Tote-Tastic - This name is playful and fun, and could be a good fit for a company that makes casual tote bags or bags for kids.\\n3. Backpacker\\'s Haven - This name clearly communicates the type of bags the company makes and could appeal to outdoor enthusiasts.\\n4. The Satchel Society - This name has a literary, intellectual vibe and could work well for a company that makes stylish, functional bags for professionals.\\n5. Canvas & Cords - This name has a rugged, outdoorsy feel and could be a good fit for a company that makes bags made from durable materials like canvas and leather.\\n6. Bag-ology - This name plays off the word \"biology\" and could work well for a company that makes bags with a scientific or technical focus.\\n7. The Bag Lady - This name is quirky and memorable, and could appeal to a wide range of customers.\\n8. The Carry-On Co. - This name is straightforward and to-the-point, and could work well for a company that specializes in carry-on luggage.\\n9. The Satchel Shop - This name is simple and descriptive, and could work well for a company that sells satchels and other types of bags.\\n10. The Weekender\\'s Warehouse - This name has a fun, alliterative quality and could appeal to customers looking for bags to use for weekend getaways.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(product=\"bags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain SimpleSequentialChain\n",
    "What makes it simple? why is it called simple?\n",
    "SimpleSequentialChain only allows for direct one way communication, i.e the output of the previous step must be the input of the next step. Without any other possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Generate a single name for a company that manufactures {product}:\n",
    "\"\"\"\n",
    "prompt_template1  = ChatPromptTemplate.from_template(prompt1)\n",
    "chain1 = LLMChain(llm=mistral_7b, prompt=prompt_template1)\n",
    "chain1.input_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introducing \"ApexTech\": A premier smartphone manufacturer dedicated to delivering high-quality, innovative, and sleek devices that exceed customer expectations.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.predict(product=\"high quality phones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company_name']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template2 = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "chain2 = LLMChain(llm=mistral_7b, prompt=prompt_template2)\n",
    "chain2.input_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skyline Phones: Innovative provider of unlocked, international cell phones and accessories, offering affordable rates and excellent customer service.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.predict(company_name=\"Skyline Phones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mIntroducing \"Aurum Tech\": A premier technology company specializing in the design and manufacturing of Gold infused mobile devices.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\"Aurum Tech\": Leading tech firm crafting gold-infused mobile devices through innovative design and manufacturing processes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Aurum Tech\": Leading tech firm crafting gold-infused mobile devices through innovative design and manufacturing processes.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "sequential_chain.run(\"Gold phones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a more complex sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"Translate the following review to english: \\n\\n{review}\")\n",
    "# initialize the first chain\n",
    "chain1 = LLMChain(llm=mistral_7b, prompt=prompt1, output_key=\"english_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's a very good product, I like it a lot.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.predict(review=\"C'est un très bon produit je l'aime beaucoup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = ChatPromptTemplate.from_template(\"Summerize this review in one sentence: \\n\\n {english_review}\")\n",
    "chain2 = LLMChain(llm=mistral_7b, prompt=prompt2, output_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The review expresses a positive sentiment towards the product, with the reviewer stating that they like it a great deal.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.predict(english_review=\"It's a very good product, I like it a lot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = ChatPromptTemplate.from_template(\"Which language was this review written in: \\n\\n{review}\")\n",
    "chain3 = LLMChain(llm=mistral_7b, prompt=prompt3, output_key=\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This review was written in French. The sentence translates to \"This is a very good product, I love it a lot\" in English.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3.predict(review=\"C'est un très bon produit je l'aime beaucoup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = ChatPromptTemplate.from_template(\"Write a follow up response to the following summary in only the specified language: \\n\\n Summary: {summary} \\n\\n Language: {language} \\n\\n Reponse:\")\n",
    "chain4 = LLMChain(llm=mistral_7b, prompt=prompt4, output_key=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"review\"], \n",
    "    output_variables=[\"english_review\", \"summary\", \"language\", \"response\"], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review': \"C'est un très bon produit je l'aime beaucoup\",\n",
       " 'english_review': \"It's a very good product, I like it a lot.\",\n",
       " 'summary': 'The review expresses a positive sentiment towards the product, with the reviewer stating that they like it a great deal.',\n",
       " 'language': 'This review was written in French. The sentence translates to \"This is a very good product, I love it a lot\" in English.',\n",
       " 'response': \"Réponse : Ce produit est réellement excellent, je l'aime beaucoup. (Response: This product is really excellent, I love it a lot.)\"}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain(\"C'est un très bon produit je l'aime beaucoup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"name\": \"Kosi\",\\n\"lastname\": \"Smith\",\\n\"address\": \"#1 Samuel St.\"\\n}'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompttemplatex = ChatPromptTemplate.from_template(\"\"\"[INST] You are a helpful code assistant. Your task is to generate a valid JSON object based on the given information:\\nname: {name}\\nlastname: Smith\\naddress: #1 Samuel St.\\nJust generate the JSON object without explanations:\\n[/INST]\"\"\")\n",
    "testchain = LLMChain(llm=mistral_7b, prompt=prompttemplatex)\n",
    "testchain.predict(name=\"Kosi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Router Chains\n",
    "Router chains decide which path they should follow based on the reasoning output of the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"[INST] You are a very smart physics professor. \\nYou are great at answering questions about physics in a concise and easy to understand manner. \\nWhen you don't know the answer to a question you admit that you don't know.\\nHere is a question:\\n{input}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_template = \"\"\"[INST] You are a very good mathematician.\\nYou are great at answering math questions.\\nYou are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\\nHere is a question:\\n{input}[/INST]\"\"\"\n",
    "\n",
    "history_template = \"\"\"[INST] You are a very good historian. \\nYou have an excellent knowledge of and understanding of people, events and contexts from a range of historical periods. \\nYou have the ability to think, reflect, debate, discuss and evaluate the past.\\n You have a respect for historical evidence and the ability to make use of it to support your explanations and judgements.\\nHere is a question:\\n{input}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "computerscience_template = \"\"\"[INST] You are a successful computer scientist.\\nYou have a passion for creativity, collaboration, forward-thinking, confidence, strong problem-solving capabilities, understanding of theories and algorithms, and excellent communication skills. \\nYou are great at answering coding questions. \\nYou are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity. Here is a question:\\n{input}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=mistral_7b, prompt=prompt)\n",
    "    destination_chains[name] = chain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'physics': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=\"[INST] You are a very smart physics professor. \\nYou are great at answering questions about physics in a concise and easy to understand manner. \\nWhen you don't know the answer to a question you admit that you don't know.\\nHere is a question:\\n{input}\\nJust generate the answer without explanations:\\n[/INST]\"))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x152584a10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x152564510>, model_name='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.0, openai_api_key='fe1f4854dd8970c1d52e05e795d053db950947b1cc4fe010db76f3557f93b3bf', openai_api_base='https://api.together.xyz/v1', openai_proxy='')),\n",
       " 'math': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='[INST]You are a very good mathematician.\\nYou are great at answering math questions.\\nYou are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\\nHere is a question:\\n{input}[/INST]'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x152584a10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x152564510>, model_name='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.0, openai_api_key='fe1f4854dd8970c1d52e05e795d053db950947b1cc4fe010db76f3557f93b3bf', openai_api_base='https://api.together.xyz/v1', openai_proxy='')),\n",
       " 'History': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='[INST]You are a very good historian. \\nYou have an excellent knowledge of and understanding of people, events and contexts from a range of historical periods. \\nYou have the ability to think, reflect, debate, discuss and evaluate the past.\\n You have a respect for historical evidence and the ability to make use of it to support your explanations and judgements.\\nHere is a question:\\n{input}[/INST]'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x152584a10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x152564510>, model_name='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.0, openai_api_key='fe1f4854dd8970c1d52e05e795d053db950947b1cc4fe010db76f3557f93b3bf', openai_api_base='https://api.together.xyz/v1', openai_proxy='')),\n",
       " 'computer science': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='[INST] You are a successful computer scientist.\\nYou have a passion for creativity, collaboration, forward-thinking, confidence, strong problem-solving capabilities, understanding of theories and algorithms, and excellent communication skills. \\nYou are great at answering coding questions. \\nYou are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity. Here is a question:\\n{input}[/INST]'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x152584a10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x152564510>, model_name='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.0, openai_api_key='fe1f4854dd8970c1d52e05e795d053db950947b1cc4fe010db76f3557f93b3bf', openai_api_base='https://api.together.xyz/v1', openai_proxy=''))}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'physics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=mistral_7b, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"[INST]Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\n",
    "Just generate the JSON object without explanations\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(), # why do promt templates have output parsers, it is because this is the output that would be sent to later sections of the router to find the right model to perform the operation\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(mistral_7b, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[INST]Given a raw text input to a language model select the model prompt '\n",
      " 'best suited for the input. You will be given the names of the available '\n",
      " 'prompts and a description of what the prompt is best suited for. You may '\n",
      " 'also revise the original input if you think that revisingit will ultimately '\n",
      " 'lead to a better response from the language model.\\n'\n",
      " '\\n'\n",
      " '<< FORMATTING >>\\n'\n",
      " 'Return a markdown code snippet with a JSON object formatted to look like:\\n'\n",
      " '```json\\n'\n",
      " '{{\\n'\n",
      " '    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n'\n",
      " '    \"next_inputs\": string \\\\ a potentially modified version of the original '\n",
      " 'input\\n'\n",
      " '}}\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'REMEMBER: \"destination\" MUST be one of the candidate prompt names specified '\n",
      " 'below OR it can be \"DEFAULT\" if the input is notwell suited for any of the '\n",
      " 'candidate prompts.\\n'\n",
      " 'REMEMBER: \"next_inputs\" can just be the original input if you don\\'t think '\n",
      " 'any modifications are needed.\\n'\n",
      " '\\n'\n",
      " '<< CANDIDATE PROMPTS >>\\n'\n",
      " 'physics: Good for answering questions about physics\\n'\n",
      " 'math: Good for answering math questions\\n'\n",
      " 'History: Good for answering history questions\\n'\n",
      " 'computer science: Good for answering computer science questions\\n'\n",
      " '\\n'\n",
      " '<< INPUT >>\\n'\n",
      " '{input}\\n'\n",
      " '\\n'\n",
      " '<< OUTPUT (remember to include the ```json)>>\\n'\n",
      " 'Just generate the JSON object without explanations\\n'\n",
      " '[/INST]')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosisochukwuasuzu/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation is electromagnetic radiation emitted by a perfect absorber at a specific temperature. The spectral distribution of this radiation follows Planck's law.\""
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosisochukwuasuzu/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math: {'input': '2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thank you for the kind compliment! Regarding your question, the expression \"2 + 2\" is a basic arithmetic problem. To solve it, we follow the order of operations, which is often remembered by the acronym PEMDAS: Parentheses, Exponents, Multiplication and Division (from left to right), Addition and Subtraction (from left to right). In this case, there are no parentheses, exponents, or multiplication/division, so we can directly apply the addition operation.\\n\\nSo, the solution to the expression \"2 + 2\" is:\\n\\n1. Add the numbers: 2 + 2 = 4\\n\\nTherefore, the answer to the expression \"2 + 2\" is 4.'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosisochukwuasuzu/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: {'input': 'Why did every cell in our body come to contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I appreciate your kind compliment, but I must clarify that as a historian, I specialize in the study of past human civilizations, cultures, and societies. I don\\'t have a background in biology or genetics to answer your question directly. However, I can provide you with some context and information about the scientific theory that explains why every cell in our body contains DNA.\\n\\nThe presence of DNA in every cell of the human body is a fundamental aspect of genetics and biology. DNA, or deoxyribonucleic acid, is the molecule that carries the genetic instructions used in the growth, development, and reproduction of all living organisms.\\n\\nThe theory that explains how every cell in our body came to contain DNA is called the \"Central Dogma of Molecular Biology.\" This theory was proposed by Francis Crick in 1958. According to this theory, genetic information flows from DNA to RNA to proteins. In other words, DNA is the blueprint for making RNA and proteins, which are essential for the functioning of cells.\\n\\nDuring the process of cell division, the DNA in the nucleus of a cell is replicated, creating two identical copies. These copies are then distributed to the two new cells that form during cell division. This ensures that each new cell receives a complete set of genetic instructions, allowing for the continuity of life and the development of complex organisms.\\n\\nTherefore, the presence of DNA in every cell of our body is a result of the fundamental biological processes of replication and cell division, which have been occurring since the earliest forms of life on Earth.'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
