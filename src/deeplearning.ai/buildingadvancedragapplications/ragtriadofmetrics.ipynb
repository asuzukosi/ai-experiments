{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Triad of metrics\n",
    "This section describes the metrics used for evaluating RAG applications.\n",
    "It includes the metrics for:\n",
    "- Answer relevance\n",
    "- Context relevance\n",
    "- Groundedness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.readers.file.base import SimpleDirectoryReader\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"./MIV2 - LLM paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-Robot interaction through joint robot planning\n",
      "with Large Language Models\n",
      "Kosi Asuzu1*\n",
      "1*Birmingham City University.\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalisation capa-\n",
      "bilities, expanding their utility beyond natural language processing into various applications.\n",
      "Leveraging extensive web knowledge, these models generate meaningful text data in response\n",
      "to user-defined prompts, introducing a novel mode of interaction with software applications.\n",
      "Recent investigations have extended the generalizability of LLMs into the domain of robotics,\n",
      "addressing challenges in existing robot learning techniques such as reinforcement learning and\n",
      "imitation learning. This paper explores the application of LLMs for robot planning as an alter-\n",
      "native approach to generate high-level robot plans based on prompts provided to the language\n",
      "model. The proposed methodology facilitates continuous user interaction and adjustment of task\n",
      "execution plans in real-time. A pre-trained LLM is utilized for collaborative human-robot plan-\n",
      "ning using natural language, complemented by Vision Language Models (VLMs) responsible for\n",
      "generating scene descriptions incorporated into the prompt for contextual grounding.\n",
      "Evaluation of the system is conducted within the VIMABench benchmark simulated environ-\n",
      "ment. Further practicality assessment involves experimentation with the system on a robotic arm\n",
      "engaged in tabletop manipulation activities. The observed results reveal that soliciting human\n",
      "feedback for zero-shot plans generated by LLMs in robot manipulation yields an 8.6% perfor-\n",
      "mance increase in simulated evaluations and a 14% increase in physical evaluations compared to\n",
      "the baseline, showcasing the efficacy of the proposed approach.\n",
      "Keywords: human-robot interaction, robotics, robot-arm, large language models, computer vision,\n",
      "natural language processing\n",
      "1 Introduction\n",
      "The field of robotics has experienced notable progress, primarily propelled by the integration of deep\n",
      "learning techniques in perception, planning, and manipulation [3]. Recent advancements in Large\n",
      "Language Models (LLMs), exemplified by GPT and its successors, have positioned these models as\n",
      "transformative tools across diverse robot learning applications [4][5]. With the impending realisation\n",
      "of General-Purpose Robots (GPRs), LLMs have garnered significant attention for their potential\n",
      "to revolutionize robot planning, a crucial aspect of autonomous systems [12]. The capacity to plan\n",
      "actions, make informed decisions based on language-defined instructions, and adapt to dynamic\n",
      "environments is paramount for robots operating in real-world scenarios.\n",
      "This research explores the application of LLMs in the domain of robot planning, emphasizing\n",
      "the incorporation of human feedback to enhance the robustness and accuracy of the generated task\n",
      "execution plan. The proposed end-to-end solution leverages zero-shot learning vision models for\n",
      "object detection, vision-language models for scene description, and the manipulation of a physical\n",
      "robot arm using generated instructions. Traditionally, robot planning relied on carefully crafted\n",
      "algorithms, heuristics, or learning-based techniques like reinforcement learning or imitation learning.\n",
      "While fruitful, these approaches often encounter challenges in unstructured environments, diverse\n",
      "tasks, and nuanced human interactions [14].\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), print(documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join(doc.text for doc in documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human-Robot interaction through joint robot planning\\nwith Large Language Models\\nKosi Asuzu1*\\n1*Birmi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_sentence_window_index\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1)\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_sentence_window_query_engine\n",
    "\n",
    "sentence_window_engine = \\\n",
    "get_sentence_window_query_engine(sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sentence_window_engine.query(\n",
    "    \"How do you create your AI portfolio?\")\n",
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using feedback functions for evaluations\n",
    "In this section we will cover the use of feedback functions for evaluating the outputs of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import OpenAI as fOpenAI\n",
    "\n",
    "provider = fOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback\n",
    "\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "\n",
    "context_selection = TruLlama.select_source_nodes().node.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contex Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons,\n",
    "             name=\"Groundedness\"\n",
    "            )\n",
    "    .on(context_selection)\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating RAG applications\n",
    "We will add the feedback functions as callbacks during our RAG pipeline, which will allow for evaluationf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "from trulens_eval import FeedbackMode\n",
    "\n",
    "tru_recorder = TruLlama(\n",
    "    sentence_window_engine,\n",
    "    app_id=\"App_1\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_qs_relevance,\n",
    "        f_groundedness\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions.append(\"How can I be successful in AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder as recording:\n",
    "        sentence_window_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "records[[\"input\", \"output\"] + feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
