{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building advanced RAG applications using LlamaIndex\n",
    "LlamaIndex is a powerful datamanagement tool for LLM applications, it allows developers load, transform, index and search over very large datasets which are then added to the LLM to generate more meaningful synthesis from the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.base import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"./MIV2 - LLM paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-Robot interaction through joint robot planning\n",
      "with Large Language Models\n",
      "Kosi Asuzu1*\n",
      "1*Birmingham City University.\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalisation capa-\n",
      "bilities, expanding their utility beyond natural language processing into various applications.\n",
      "Leveraging extensive web knowledge, these models generate meaningful text data in response\n",
      "to user-defined prompts, introducing a novel mode of interaction with software applications.\n",
      "Recent investigations have extended the generalizability of LLMs into the domain of robotics,\n",
      "addressing challenges in existing robot learning techniques such as reinforcement learning and\n",
      "imitation learning. This paper explores the application of LLMs for robot planning as an alter-\n",
      "native approach to generate high-level robot plans based on prompts provided to the language\n",
      "model. The proposed methodology facilitates continuous user interaction and adjustment of task\n",
      "execution plans in real-time. A pre-trained LLM is utilized for collaborative human-robot plan-\n",
      "ning using natural language, complemented by Vision Language Models (VLMs) responsible for\n",
      "generating scene descriptions incorporated into the prompt for contextual grounding.\n",
      "Evaluation of the system is conducted within the VIMABench benchmark simulated environ-\n",
      "ment. Further practicality assessment involves experimentation with the system on a robotic arm\n",
      "engaged in tabletop manipulation activities. The observed results reveal that soliciting human\n",
      "feedback for zero-shot plans generated by LLMs in robot manipulation yields an 8.6% perfor-\n",
      "mance increase in simulated evaluations and a 14% increase in physical evaluations compared to\n",
      "the baseline, showcasing the efficacy of the proposed approach.\n",
      "Keywords: human-robot interaction, robotics, robot-arm, large language models, computer vision,\n",
      "natural language processing\n",
      "1 Introduction\n",
      "The field of robotics has experienced notable progress, primarily propelled by the integration of deep\n",
      "learning techniques in perception, planning, and manipulation [3]. Recent advancements in Large\n",
      "Language Models (LLMs), exemplified by GPT and its successors, have positioned these models as\n",
      "transformative tools across diverse robot learning applications [4][5]. With the impending realisation\n",
      "of General-Purpose Robots (GPRs), LLMs have garnered significant attention for their potential\n",
      "to revolutionize robot planning, a crucial aspect of autonomous systems [12]. The capacity to plan\n",
      "actions, make informed decisions based on language-defined instructions, and adapt to dynamic\n",
      "environments is paramount for robots operating in real-world scenarios.\n",
      "This research explores the application of LLMs in the domain of robot planning, emphasizing\n",
      "the incorporation of human feedback to enhance the robustness and accuracy of the generated task\n",
      "execution plan. The proposed end-to-end solution leverages zero-shot learning vision models for\n",
      "object detection, vision-language models for scene description, and the manipulation of a physical\n",
      "robot arm using generated instructions. Traditionally, robot planning relied on carefully crafted\n",
      "algorithms, heuristics, or learning-based techniques like reinforcement learning or imitation learning.\n",
      "While fruitful, these approaches often encounter challenges in unstructured environments, diverse\n",
      "tasks, and nuanced human interactions [14].\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "17 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: 26029736-135b-4824-bc41-107558227130\n",
      "Text: Human-Robot interaction through joint robot planning with Large\n",
      "Language Models Kosi Asuzu1* 1*Birmingham City University. Abstract\n",
      "Large Language Models (LLMs) have demonstrated remarkable zero-shot\n",
      "generalisation capa- bilities, expanding their utility beyond natural\n",
      "language processing into various applications. Leveraging extensive\n",
      "web knowl...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple RAG pipeline with llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Document\n",
    "document  = Document(text=\"\\n\\n\".join([ doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54037\n"
     ]
    }
   ],
   "source": [
    "print(len(document.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucci, I. and Arezes, P.M., 2019. A brief overview of the use of\n",
      "collaborative robots in industry 4.0: Human role and safety. Occupational and environmental\n",
      "safety and health, pp.641-650.\n",
      "[55] Bragan¸ ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019. A brief overview of the use of\n",
      "collaborative robots in industry 4.0: Human role and safety. Occupational and environmental\n",
      "safety and health, pp.641-650.\n",
      "[56] Park, J.S., O’Brien, J., Cai, C.J., Morris, M.R., Liang, P. and Bernstein, M.S., 2023, October.\n",
      "Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual\n",
      "ACM Symposium on User Interface Software and Technology (pp. 1-22).\n",
      "Appendix A Section title of first appendix\n",
      "An appendix contains supplementary information that is not an essential part of the text itself but\n",
      "which may be helpful in providing a more comprehensive understanding of the research problem or\n",
      "it is information that is too cumbersome to be included in the body of the paper.\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(document.text[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "from llama_index.service_context import ServiceContext\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral7b = OpenAI(temperature=0.0, model=\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_prompt': None,\n",
       " 'pydantic_program_mode': <PydanticProgramMode.DEFAULT: 'default'>,\n",
       " 'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       " 'temperature': 0.0,\n",
       " 'max_tokens': None,\n",
       " 'additional_kwargs': {},\n",
       " 'max_retries': 3,\n",
       " 'timeout': 60.0,\n",
       " 'default_headers': None,\n",
       " 'reuse_client': True,\n",
       " 'api_base': 'https://api.together.xyz/v1',\n",
       " 'api_version': '',\n",
       " 'class_name': 'openai_llm'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral7b.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosisochukwuasuzu/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mistral7b, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents([document],\n",
    "                                        service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the title of the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of a paper cannot be determined from the context information provided as each paper is identified by its unique title.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        print(item)\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = \"What is the right AI job for me?\"\n",
    "eval_questions.append(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Advanced RAG applications\n",
    "The powerful part about llama index is its vast retrieval techinques implemented for advanced retrieval usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Window Retreival\n",
    "Sentence window retrieval is a RAG technique that handles fetching surrounding context when querying the embedding space, this helps gather surroundign information when queried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_sentence_window_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    mistral7b,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_sentence_window_query_engine\n",
    "\n",
    "sentence_window_engine = get_sentence_window_query_engine(sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"how do I get started on a personal project in AI?\"\n",
    ")\n",
    "print(str(window_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()\n",
    "\n",
    "tru_recorder_sentence_window = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine,\n",
    "    app_id = \"Sentence Window Query Engine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder_sentence_window as recording:\n",
    "        response = sentence_window_engine.query(question)\n",
    "        print(question)\n",
    "        print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automerging retrieval\n",
    "Merges similar retrieval contents for more accurate retreival process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_automerging_index\n",
    "\n",
    "automerging_index = build_automerging_index(\n",
    "    documents,\n",
    "    mistral7b,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_automerging_query_engine\n",
    "\n",
    "automerging_query_engine = get_automerging_query_engine(\n",
    "    automerging_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_response = automerging_query_engine.query(\n",
    "    \"How do I build a portfolio of AI projects?\"\n",
    ")\n",
    "print(str(auto_merging_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()\n",
    "\n",
    "tru_recorder_automerging = get_prebuilt_trulens_recorder(automerging_query_engine,\n",
    "                                                         app_id=\"Automerging Query Engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder_automerging as recording:\n",
    "        response = automerging_query_engine.query(question)\n",
    "        print(question)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
