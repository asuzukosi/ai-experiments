{"docstore/data": {"c335fb7f-ba4c-48a7-89cf-ae97f51db883": {"__data__": {"id_": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, {"node_id": "00df92de-8dba-43a8-af11-19ce18c1df5e", "node_type": "1", "metadata": {}, "hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "class_name": "RelatedNodeInfo"}]}, "text": "Human-Robot interaction through joint robot planning\nwith Large Language Models\nKosi Asuzu1*\n1*Birmingham City University.\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot generalisation capa-\nbilities, expanding their utility beyond natural language processing into various applications.\nLeveraging extensive web knowledge, these models generate meaningful text data in response\nto user-defined prompts, introducing a novel mode of interaction with software applications.\nRecent investigations have extended the generalizability of LLMs into the domain of robotics,\naddressing challenges in existing robot learning techniques such as reinforcement learning and\nimitation learning. This paper explores the application of LLMs for robot planning as an alter-\nnative approach to generate high-level robot plans based on prompts provided to the language\nmodel. The proposed methodology facilitates continuous user interaction and adjustment of task\nexecution plans in real-time. A pre-trained LLM is utilized for collaborative human-robot plan-\nning using natural language, complemented by Vision Language Models (VLMs) responsible for\ngenerating scene descriptions incorporated into the prompt for contextual grounding.\nEvaluation of the system is conducted within the VIMABench benchmark simulated environ-\nment. Further practicality assessment involves experimentation with the system on a robotic arm\nengaged in tabletop manipulation activities. The observed results reveal that soliciting human\nfeedback for zero-shot plans generated by LLMs in robot manipulation yields an 8.6% perfor-\nmance increase in simulated evaluations and a 14% increase in physical evaluations compared to\nthe baseline, showcasing the efficacy of the proposed approach.\nKeywords: human-robot interaction, robotics, robot-arm, large language models, computer vision,\nnatural language processing\n1 Introduction\nThe field of robotics has experienced notable progress, primarily propelled by the integration of deep\nlearning techniques in perception, planning, and manipulation [3]. Recent advancements in Large\nLanguage Models (LLMs), exemplified by GPT and its successors, have positioned these models as\ntransformative tools across diverse robot learning applications [4][5]. With the impending realisation\nof General-Purpose Robots (GPRs), LLMs have garnered significant attention for their potential\nto revolutionize robot planning, a crucial aspect of autonomous systems [12]. The capacity to plan\nactions, make informed decisions based on language-defined instructions, and adapt to dynamic\nenvironments is paramount for robots operating in real-world scenarios.\nThis research explores the application of LLMs in the domain of robot planning, emphasizing\nthe incorporation of human feedback to enhance the robustness and accuracy of the generated task\nexecution plan. The proposed end-to-end solution leverages zero-shot learning vision models for\nobject detection, vision-language models for scene description, and the manipulation of a physical\nrobot arm using generated instructions. Traditionally, robot planning relied on carefully crafted\nalgorithms, heuristics, or learning-based techniques like reinforcement learning or imitation learning.\nWhile fruitful, these approaches often encounter challenges in unstructured environments, diverse\ntasks, and nuanced human interactions [14].\n1\n\nLLMs, with their proficiency in comprehending and generating open-ended, human-like text,\nintroduce new possibilities in natural language understanding, generation, and reasoning [15]. These\nmodels excel in processing and generating human-readable instructions, bridging the gap between\nhuman intent and robot actions. The proposed methodology utilizes GPT language models to gener-\nate high-level robot plans, facilitating the execution of robotic tasks. These plans consist of high-level\nAPI functions (referred to as \u2019skills\u2019[43]) employing rule-based motion and manipulation algorithms\nand deep learning models like Vision Transformers for zero-shot object detection and Vision Lan-\nguage Models (VLMs) for scene understanding [22]. This approach allows the language model to\ngenerate plans using these skills as interfaces for perception and manipulation.\nThis research endeavors to enrich the ongoing dialogue concerning the future trajectory of\nrobotics, where intelligent planning, natural language interaction, and the transformative influence\nof Large Language Models (LLMs) coalesce to redefine the manner in which robots perceive, navi-\ngate, and engage with their surroundings. Consequently, the delineated contributions of this paper\nare as follows:\n1.Human-interactive general purpose robotic system : This system extends LLM plan gener-\nation for robotics by incorporating a feedback loop and message stream, enabling users to provide\nfeedback on generated LLM plans.\n2.Scene understanding : Utilizing a VLM, this contribution involves incorporating a description of\nthe robot\u2019s scene. The description is then added to the prompt provided to the LLM for planning,\nserving as a grounding mechanism for the LLM.\nThis study employs LLMs to formulate robot plans for the execution of human instructions\nconveyed in natural language. It facilitates real-time interaction and allows plan adjustments by the\nhuman user, as illustrated in Figure 1. The feedback, instructions, and plans are stored as context,\nintegrated into the message stream to enhance the responsiveness of the LLM. Subsequent evaluations\nscrutinize the proposed approach within a simulated robot learning benchmark, complemented by\nassessments on an operational robotic platform tailored for specific tabletop manipulation tasks.\nFigure 1 : Human robot interaction through joint planning with LLM system diagram. The diagram\nemphasizes the intricate dynamics between the human user and the LLM, with the storage of inter-\naction context in the message stream and subsequent execution of the robot plan.\n1.1 Related Work\n1.1.1 Human-Robot Interaction\nHuman-robot interaction (HRI) is a multidisciplinary field that explores the design, development,\nand evaluation of systems where humans and robots interact in various settings. Over the years,\nHRI has witnessed significant advancements, leading to an array of research and applications [23].\nIn collaborative and assistive robotics, researchers explore scenarios where robots work alongside\nhumans to enhance productivity or provide assistance [24] [26]. Pioneering investigations in human-\nrobot interaction have explored various applications, such as robot-assisted therapy. Notable research\nin this domain includes efforts to leverage social robots for aiding low-functioning children with autism\n2\n\n[25]. The broader landscape of human-robot interaction encompasses a diverse spectrum of studies\nand advancements, covering aspects like social robotics, design principles, collaborative frameworks,\nethical considerations, and technological innovations[53][52]. In the context of this research project,\na dyadic interaction system is devised to facilitate human-robot interaction in the planning of robot\ntasks, involving collaboration between a single robot and a human partner.\n1.1.2 Collaborative Robot arms in Industry\nCollaborative robotics, also known as cobots, represent a transformative paradigm in the manufac-\nturing and industrial sectors [54]. These robots are designed to work alongside human operators,\nfacilitating safer, more efficient, and highly flexible production processes [27]. A convergence of\nresearch in safety, human-robot interaction, task allocation, programming interfaces, industry-specific\napplications, and connectivity has laid the foundation for the widespread adoption of collaborative\nrobots in a variety of industrial settings [28].\nCollaborative robotics has brought transformative changes to the industrial landscape, offer-\ning innovative solutions for improving efficiency, safety, and flexibility in manufacturing processes\n[29]. Research conducted by Marcel Bergerman and Silvio M. Maeta primarily concentrates on the\ndesign of collaborative robots and their seamless integration into established farming processes [31].\nInvestigations in this domain have delved into factors such as comprehending and optimizing the\ninteraction between human workers and robots to elevate productivity. Effectively allocating tasks\nbetween human and robot workers represents a crucial research challenge.\nAs the field continues to evolve, the development of even more intuitive and adaptive collabo-\nrative robots is expected, addressing increasingly complex challenges in modern manufacturing. A\nframework is proposed to enable joint real-time planning between humans and intelligent collabo-\nrative robots using natural language. This approach is intended for application in industries where\nhumans and robots collaborate in shared environments.\nPrior investigations have delved into the integration of collaborative robot arms within Indus-\ntry 4.0 manufacturing environments, demonstrating their capability to enhance overall production\nefficiency by aiding workers in both physical and cognitive tasks [55]. The study underscores the\ndynamic nature of human involvement in the manufacturing process and emphasizes the necessity\nfor human operators to adapt effectively to interact with these tools. The current research delves\ninto language as an interface for facilitating dynamic interaction within this context.\n1.1.3 Large Language Models\nThe Transformer architecture, introduced by Vaswani et al. [32], represents a foundational break-\nthrough in the development of LLMs. Transformers rely on self-attention mechanisms to capture\ncomplex dependencies between words in a sequence, enabling parallelism and scale. This architecture\nforms the basis of many subsequent language models, such as BERT, GPT, and XLNet [33].\nLLMs represent a revolutionary leap in natural language processing and understanding. The\ncombination of sophisticated architectures, massive data-sets, fine-tuning strategies and multilingual\ncapabilities has paved the way for their widespread adoption and applications across various domains\n[35].\nAs research in this field continues to evolve, the development of more sophisticated, context-aware,\nand ethically aligned language models is expected, offering exciting opportunities and challenges for\nthe future of AI-powered language understanding and generation. One of such future opportunities\ninclude enabling new forms of language inference for robotics application.\n1.1.4 Zero-shot object detection\nZero-shot learning aims to identify objects without available labels during training, enabling classifiers\nto recognize unseen classes [38]. Zero-shot detection (ZSD) is designed to simultaneously localize\nand recognize objects from novel categories [38][42].", "start_char_idx": 0, "end_char_idx": 10859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5": {"__data__": {"id_": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, {"node_id": "dac9c22d-586d-41fc-9efe-27c747d1c629", "node_type": "1", "metadata": {}, "hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "class_name": "RelatedNodeInfo"}]}, "text": "ZSD algorithms fall into classifier-based and\ninstance-based methods. Classifier-based methods concentrate on amalgamating a traditional object\ndetection framework with a zero-shot learning classifier. In the case of instance-based methods,\ninspired by synthesis techniques in zero-shot learning, the typical approach involves initially training\na traditional object detection with seen objects and subsequently updating the confidence predictor\nusing synthesized images or visual features.\n3\n\nThis paper employs two classifier-based methods, namely SAM-CLiP[39][40] and ViLD[41], for\nidentifying objects in a scene based on a query.\nSAM-CLiP utilizes contrastive learning on both natural language and images to enable zero-shot\nobject classification. The Segment Anything Model (SAM) is a segmentation model facilitating data\nannotation and zero-shot transfer through prompt engineering. CLiP replaces the original classifier\nwith zero-shot learning classifiers [1].\nIn the second vision approach, ViLD leverages vision and language knowledge distillation. This\ninvolves distilling knowledge from a pre-trained open-vocabulary image classification model into a\ntwo-stage detector.\nThe selection of ViLD and SAM-CLiP as the zero-shot detection models in this study is predicated\non their accessibility and prior utilization in analogous projects such as SayCan and Instruct2Act.\nIt\u2019s essential to note that this research does not conduct a comprehensive assessment of all existing\nzero-shot models or determine the optimal ones; rather, the choice of models is contingent upon their\navailability.\nViLD comprises two components, learning with text embeddings and image embeddings derived\nfrom an open vocabulary image classification model. The ViLD model, also used in SayCan and\nSocratic Models, serves as the vision model in this implementation [43][44]. This literature review\nhighlights key approaches in the realm of zero-shot learning and detection, providing a foundation\nfor the current research.\n1.1.5 LLMs for Robot Planning\nThe integration of LLMs with robotics has opened new frontiers in robot planning and execution\n[17]. These models empower robots with natural language understanding, enabling them to interact\nwith humans and external systems through text or speech [10].\nOne of the foundational applications of LLMs in robotics is the understanding of natural lan-\nguage commands [4] [9]. Research in this area has explored methods for mapping text or spoken\nlanguage into executable robot commands [1]. Seminal works by Brohan et al. and Jiang et al. have\ndemonstrated the feasibility of converting human instructions into robot actions. LLMs have been\nemployed to improve robotic task planning and execution [37] [20]. TidyBot show how LLMs can be\nused in the personalisation of robot policy [2].\nPrior studies have delved into the application of Large Language Models (LLMs) in planning\nwithin a communicative environment [47]. In their work, a specialized framework was formulated for\ncooperative agents operating within a multi-agent embodied environment. Showcasing the capabilities\nof the GPT-4 model, the investigation illustrated its capacity to outperform robust planning-based\nmethods, exemplifying emergent effective communication devoid of the necessity for fine-tuning.\nThe LLM-MCTS (Monte Carlo Tree Search) investigation contributed valuable insights by reveal-\ning that LLMs not only provide a policy for action but also offer a commonsense model of the world\n[45]. Monte Carlo Tree Search, a powerful search algorithm, was employed in this exploration. This\ninvolved integrating the world model into the policy for a search algorithm like MCTS. Concurrently,\nLLMs with Tree-of-Thought (LLM-ToT) approach adopted a tree-based strategy, enabling LLMs to\nengage in deliberate decision-making by assessing various reasoning paths and self-evaluating choices\nto determine the subsequent course of action [46].\nThe results of these studies highlight the effectiveness of search-based plan generation over policies\ninduced by LLMs. Nevertheless, given the straightforward nature of the tasks in these experiments,\nthe decision is made to refrain from employing the search-based approach. Instead, reliance is placed\non the LLM-induced policies in their original form.\nRecent research endeavors have prominently concentrated on formulating task plans grounded\nin high-level natural language descriptions, thereby endowing robots with the capability to execute\nintricate tasks with minimal human intervention [34] [21] [18]. Yu et al. advanced the field by utilizing\na language model to generate rewards applicable to robots for skill synthesis [19]. Another notable\napproach, denoted as LLMs with optimal Planning proficiency (LLM+P), leverages language models\nto produce planning directives that robots can subsequently employ for executing complex tasks\n[12]. LLM+P employs language models to address Planning Domain Definition Language (PDDL)-\nbased planning problems. Significantly, the researchers demonstrate that the language understanding\ncapabilities of LLMs empower them to apply common sense reasoning to terms within the PDDL\nproblem.\n4\n\nThe decision to omit the PDDL approach in this study is grounded in the intricate nature\nof parsing PDDL instructions into robot actions, as such intricacies fall beyond the scope of this\ninvestigation.\nLLM-Brain paper proposes a memory and action system for embodied control and configuration\nfor a nervous system with the LLM acting as the central brain[17]. Code-as-Policies uses language\nmodels to generate executable python code using API directives which can then be given to a robot\nas policies [7]. Methods such as Instruct2Act and ProgPrompt focus on generating instructions using\npredefined high level robot functions without any programming primitives [1] [6]. This approach\nstands as the prevailing method in contemporary literature and is the methodology employed in the\ndevelopment of the system in this research [8] [9] [11] [17] [18]. While ProgPrompt integrates program-\nming language structures, such as loops, lists, and function definitions, into its robot plan generation,\nthis approach diverges by refraining from the explicit utilisation of programming structures. Instead,\nmost implementation complexities are managed behind the API definition, akin to Instruct2Act.\nThis streamlined approach simplifies the generated LLM plans and the feedback process.\nThe synergy between LLMs and robotics has ushered in an era of more intuitive, versatile, and\nadaptive robotic systems [37]. As research in this field advances, the development of robots that under-\nstand and communicate with humans through natural language continues to unlock new possibilities\nin domains such as home automation, healthcare, education, and industry [2].\nTable 1 : Physical robot environment evaluation\nRelated Projects\nProject name Methodology Evaluation tech-\nniqueReference\nInstruct2Act Using multimodal instructions pro-\nvided to pre-trained model to gen-\nerate highlevel robot plan based on\npython APIsSimulation [1]\nTinyBot Using language models to generate\nrobot plans based on user prefer-\nences from a set of examples via\nprevious interactionsSimulation &\nPhysical Robot[2]\nPaLM-E Using multi-modal transformer\nmodel trained end-to-end on\nvision, language and continuous\nactions for sequential robot manip-\nulation planning, visual question\nanswering and captioningPhysical Robot [4]\nCode-As-Policies Using code writing language mod-\nels to write python code which is\nthen used as a robot policySimulation [7]\nProgPrompt Using program like instructions\nas input to pre-trained language\nmodel to generate programmatic\nplan instructions for robot controlSimulation [6]\nLLM-BRAIn Using a fine-tuned LLM to gener-\nate robot behaviour tree to control\nrobotSimulation [17]\nLLM-P Using language models to generate\nplans in the planning domain def-\ninition language (PDDL) given a\nhigh level planning problemSimulation [12]\nRT-2 Trained transformer model using\ninternet scale vision and language\ndata as well as low level robot con-\ntrol instructions which is then used\nfor generating robot actions given\nvisual and language instructionsPhysical Robot [37]\nSayCan Using pre-trained language models\nto suggest robot affordances based\non provided instructionPhysical Robot [43]\n5\n\n2 Joint robot planning with Large Language Models\nOur system integrates LLMs to enable collaborative robot planning through interactions with human\nusers as shown in Figure 2. The primary objective is to establish a dynamic planning environment\nwhere humans can work in unison with robots by providing high-level task instructions, feedback,\nand plan adjustments.\nIn this paper few-shot prompting is used to guide the model to generate outputs from a defined\ndistribution of actions (\u03a0). The set of low level skills (\u03a0) and the language descriptions (\u03a0 d) are\nprovided along with the instruction ( I) and a description of the scene ( S) where the robot is located.\n2.1 Language Grounding\nLLMs are not naturally grounded in the real world, providing a set of API skills give the LLMs\ngrounding to enable them take actions in the real world. The skill functions constrain completions to\nthe skill descriptions, enhancing the LLM\u2019s awareness of the robot\u2019s capabilities and ensuring that\nthe generated natural language actions are both feasible and contextually appropriate.\nIn contrast to the approach used in designing affordances in SayCan, this methodology deviates\nby omitting the utilisation of a reinforcement learning algorithm for acquiring language-conditioned\nvalue functions. Instead, the definition of robot affordances relies on the manual programming of\nmanipulation and motion algorithms.\nUtilizing the GPT-4 Vision Language Model [48] is employed to furnish scene descriptions ( S)\nfor the LLM prompt, thereby enabling the integration of object affordances into the LLM\u2019s plan\ngeneration.", "start_char_idx": 10860, "end_char_idx": 20815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7c1c087-905a-41a0-88d3-92cf9a603438": {"__data__": {"id_": "d7c1c087-905a-41a0-88d3-92cf9a603438", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, {"node_id": "b293d45b-6ae1-4bbc-b549-50431cbbddcf", "node_type": "1", "metadata": {}, "hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "class_name": "RelatedNodeInfo"}]}, "text": "Figure 2 : Continuous feedback process between human user and LLM to generate robot plan.\nThe human provides feedback instructions in natural language which the LLM uses to update the\ngenerated plan\n2.2 Perception with Foundation Vision Models\n. As part of the set of robot skills, perception actions are defined, incorporating foundational vision\nmodels for zero-shot object detection and scene understanding.The vision models are employed for\nobject identification within the scene, yielding bounding boxes for detected objects. Subsequently,\npixel-to-coordinate values are computed. The perception functions operate beneath the rule-based\nactions, albeit not directly presented as skills to the LLM. Full object generalisation is possible by\ncombining the SAM model for image segmentation and the CLiP model for classification of those\nsegments. In the case of the ViLD model, a pre-defined list of possible objects is provided to the\n6\n\nAlgorithm 1 LLM robot joint plan generation\nGiven : A high level instruction I, state S, and \u03a0 a set of skills and their language descriptions \u03a0 d\n1:I\u21d0Instruction\n2:\u03a0\u21d0Skills\n3:S\u21d0State\n4:MessageStream \u21d0Store previous messages and responses\n5:MaxFeedback \u21d0MaxFeedbackCount\n6:while I\u0338=stopdo\n7: S\u21d0SceneDescription (Camera )\n8: P\u21d0LLMPlanGenerator (I, S,\u03a0, MessageStream )\n9: MessageStream +I \u25b7 add instruction to message stream\n10: MessageSteam +P \u25b7 add llm generated plan to message steam\n11: NumFeedback \u21d00\n12: InstructionApproved \u21d0RequestApproval (P)\n13: while InstructionApproved \u0338=True &&NumFeedback < MaxFeedback do\n14: Feedback \u21d0RequestFeedback (P)\n15: P\u21d0LLMPlanGenerator (F, S, \u03a0, MessageStream )\n16: MessageSteam +F\n17: MessageSteam +P\n18: InstructionApproved \u21d0RequestApproval (P)\n19: end while\n20: RobotExecute (P)\u25b7execute robot instructions using low level APIs based on generated plan\n21:end while\nmodel before initiation. Notably, the model does not detect objects beyond the predetermined options\nin the list. Each item in the specified list is assigned a probability score, subsequently employed to\nidentify the presence of objects in the scene and ascertain their respective locations.\n2.3 Prompt Design\n. The GPT-3.5 model [49] is utilized for prompting through an accessible API. The prompt design for\nthe LLM encompasses instructions regarding the available robot skills \u03a0 dand illustrative examples\ndemonstrating the application of these skills in conjunction. This methodology aligns with the few-\nshot prompting technique.\nA prompt template is formulated by integrating the set of robot skills \u03a0, the language descriptions\nof these skills \u03a0 d, example plans with corresponding instructions Pe, a given instruction I, and a\ndescription of the current robot location, constituting the state S. These variables are then passed\ninto the LLM plan generation function which is used to generate a robot execution plan \u03c1as shown\nin Figure 3. The action scoring approach described in SayCan is not employed; instead, a few-shot\nprompting approach, as outlined in Instruct2Act, is utilized.\n2.4 The Message Stream\nThe incorporation of human interactivity in robot planning is facilitated through the utilisation of\nthe message persistence feature within the OpenAI API. This capability enables the utilisation of a\nthread of conversations within the prompt. New instructions from the user and previously generated\nplans by the LLM are appended to the message stream, as depicted in Figure 1. User messages are\ncategorized into instructions and feedback. The size of the message stream is constrained by removing\nprior context to avoid exceeding the context window limit.\nThe interactive planning and execution process begins with users providing high-level task descrip-\ntions and instructions in natural language, subsequently processed by the LLM. Plan generation\nbased on this initial input is the next step, where the LLM generates an initial task plan. However,\nthis plan may be incomplete or sub-optimal, which is why users are given the opportunity to provide\nfeedback and adjustment commands through natural language interactions. The iterative planning\nprocess continues as the system interprets and acts on user feedback, adjusting the plan until user\nsatisfaction is achieved. The final agreed-upon plan is executed by the robot in the real or simulated\nenvironment.\n7\n\nFigure 3 : Instructions, skills and their examples and scene descriptions are combined into a single\nprompt and sent to the LLM, the LLM then uses this information to generate the robot execution\nplan.\n2.5 Motion and Manipulation\nA set of rule-based actions is devised for the robot, encompassing operations such as pick-and-place,\npick, move-left, etc. These actions are meticulously crafted and fine-tuned for enhanced accuracy.\nThe functions are accessible through APIs and can be executed sequentially when the robot plan\nis generated. Termed as \u2019skills,\u2019 these actions represent a defined set of activities achievable by the\nrobot.\nThis methodology integrates the components of the research, emphasizing the comprehensive\napproach utilized to facilitate joint robot planning through human-robot interaction with the\nassistance of LLMs.\n3 Experimental Setup and Procedure\nTo empirically assess the effectiveness of the system, a series of experiments were conducted in\nboth simulated and physical environments, each involving a solitary human participant. A terminal\nenvironment was provided to facilitate the transmission of textual instructions and feedback to\nthe system. Throughout the experiment, the human participant interacted with the robot, issuing\nhigh-level natural language instructions, feedback, and adjustment commands. The robot\u2019s language\nunderstanding component, powered by an LLM, interpreted and responded to these instructions\nand commands. The planning and execution process remained dynamic, incorporating iterative plan\nadjustments in response to the singular user\u2019s feedback.\n3.1 Simulated Evaluation\nThe VIMABench is a simulation benchmark of multi-modal prompts for robotics [5]. It interleaves\ntextual and visual tokens. The benchmark contains thousands of procedural generated table top tasks\nwith multi-modal prompts with systematic evaluation protocols for generalisation.\nSeveral representative meta tasks were chosen from VIMABench, amounting to a total of three\ntasks. Spanning from simple object manipulation to visual reasoning, these tasks were selected to\nassess the proposed methods within the tabletop manipulation domain, as illustrated in Figure 4.\nThe evaluation encompasses the following tasks:\n1.Task 1 Scene Rearrangement : The robot is positioned within a tabletop environment containing\nobjects and receptors. A target scene image is supplied, prompting the system to reconfigure the\nenvironment to align with the specified target scene.\n2.Task 2 Visual Manipulation : The robot is situated within a tabletop environment, furnished\nwith objects and receptors, and directed to manipulate the objects to the receptors in a specified\nsequence.\n8\n\n3.Task 3 Object Rotation : The robot is situated in a tabletop environment, featuring an object,\nand is subjected to a prompt specifying a distinct target angular rotation for the object.\nFigure 4 : The simulated tasks used for evaluation are Task 1: Rearrange Scene, Task 2: Visual\nManipulation, Task 3: Rotate. The image shows an example scene setup and instruction for each task.\nVIMABench categorizes tasks into partitions, including combinatorial generalization ,place-\nment generalization , and novel object generalization . The benchmark offers two prompt\nconfiguration modes: single modality, involving textual information only, and multi-modality, incor-\nporating both texts and images. Experiments are conducted 10 times for each partition, modality,\nand task, resulting in a total of 60 runs per task.\nIn the simulated environment, the exclusive utilisation of the SAM-CLIP vision system is exam-\nined. The system is evaluated under two conditions: one without feedback to the LLM post-plan\ngeneration, and another allowing users to provide feedback, which is then incorporated into the\nmessage stream, serving as contextual input for subsequent plan generation by the LLM.\n3.2 Physical Evaluation\nIn the physical experiment, a robotic system is utilized. The experimental setup closely corresponds\nto the simulated environment, as depicted in Figure 5, incorporating a top-view camera for vision\nskills and employing analogous objects and receptors.\nRobot Arm . The Niryo Ned robot is a collaborative robot arm which can be used in both\nresearch and industrial applications [50], this is used as the robot arm for the experiment.The robot\nconnects to a shared network with an Apple M1 MacBook serving as the controller device, equipped\nwith up to 10 CPU cores, a high-performance GPU boasting 16 cores, a Neural Engine, and a Unified\nMemory Architecture with 16GB of RAM, facilitating instruction transmission to the robot arm over\nthe network.\nImage and Depth Camera A testbed setup is established for the experiment, incorporating\nthe installation of the Intel RealSense D435i image and depth camera. The robot is positioned in\na stable configuration beneath the camera setup to ensure accuracy in pixel-to-coordinate value\ntransformations, as depicted in Figure 5 [51].\nFor the physical evaluation, two tabletop manipulation tasks closely resembling the visual manipu-\nlation task outlined in VIMABench are selected, as depicted in Figure 6. The evaluation encompasses\nthe following tasks:\n1.Task A Pick and Place Single Object : In this task, the robot is positioned within a scene featuring\na solitary Lego block positioned between two receptors\u2014a green box and a blue box.", "start_char_idx": 20816, "end_char_idx": 30533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16196373-14c0-4390-a0db-8ac5b663447f": {"__data__": {"id_": "16196373-14c0-4390-a0db-8ac5b663447f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, {"node_id": "cf72c77f-d210-49e0-9fbc-868444937960", "node_type": "1", "metadata": {}, "hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "class_name": "RelatedNodeInfo"}]}, "text": "The system\nis directed to position the block in a designated receptor, for example, \u2019 place the blue block in the\ngreen bowl \u2019.\n2.Task B Pick and Place Multiple Objects :In this task, the robot is situated within a scene compris-\ning multiple Lego blocks and two receptors\u2014a green box and a blue box. The system is directed to\narrange multiple items into distinct receptors in a predetermined order, for instance, placing the\nred block in the blue bowl, followed by positioning the green block in the correspondingly colored\nbowl. i.e \u2019 put the red block in the blue bowl, then the green block in the matching colored bowl \u2019.\n9\n\nOur system is evaluated on 3 variations: plan generation with feedback, plan generation without\nfeedback and plan generation without scene description.\nFigure 5 : The experimental setup comprises a Niryo Ned robot arm, an Intel RealSense D435i\ndepth camera, and colored bowls (green and blue) serving as receptors, along with a collection of\nmulticolored Lego blocks.\nThe system\u2019s performance is assessed independently using SAM-CLiP and ViLD vision systems.\nEach task within every system variation is executed 10 times for each vision model, resulting in a\ntotal of 40 tests for each system configuration.\nThe quantitative metric employed for gauging the performance of the various approaches is the\ntask success rate.\nFigure 6 : The simulated environment features a top view camera which is used for detecting object\nsin the scene. The physical environment is configured to closely emulate the settings utilized in the\nsimulated evaluation, employing analogous objects and configurations.\n4 Results\nTable 2 demonstrates the system\u2019s performance in executing scene rearrangement tasks prompted\nthrough single and multi-modal configurations. Notably, the system exhibits an inability to success-\nfully execute any scene rearrangement tasks when prompted with a single modality. This limitation\narises from the absence of contextual information, specifically the target scene image, in single-modal\nprompts.\nAs highlighted in Table 2, the introduction of feedback provides a 18.5% increase in multi modality\ntasks, and a 2.5% performance drop over the baseline on single modality tasks. This improvement is\nattributed to the heightened complexity in generating plans for multi-modal prompts.\nIn the context of the visual manipulation task (Task 2), the base implementation surpasses the\nfeedback system. This observation raises the suspicion that discrepancies in task execution accuracy,\nrather than plan generation, may underlie the system\u2019s comparative performance. Negative results are\nobserved primarily in tasks prompted with a single modality, aligning with the inherent limitations\nassociated with insufficient contextual information.\nObservations during experiments reveal consistent plan generation errors by the model for specific\ninstructions, and even after feedback is provided for a particular mistake, the model tends to repeat\n10\n\nTable 2 : VIMABench simulated environment evaluation\nSingle modality1Multi modality2\nProject Task 1 Task 2 Task 3 Task 1 Task 2 Task 3\nInstruct2Act 0.0% 70% 90% 40% 63.3% 76.66%\nInstruct2Act w/Feedback 0.0% 66% 90% 46.66% 73.33% 93.3%\nNote: The success rate is calculated by finding the average success of task completion for each task across the 3\npartitions.\n1For single modality only textual data is provided to the context of the model.\n2For multi modality both texts and images are provided as contexts to the model.\nthese errors once the message stream is cleared. This recurrence is attributed to the absence of a\nmemory system that would retain feedback and context across experiments.\nThe language model demonstrates generalisation to new objects and employs novel approaches in\ncalling the APIs beyond the provided examples. Although infrequent, the model may respond with\nnatural language outputs outside the defined API, and providing feedback instructions corrects such\noccurrences.\nFigure 7 : The human user provides the instruction \u2019 place the green block in the matching colored\nbowl\u2019. The LLM uses this instruction to generate a plan which is then executed by the physical robot\nTable 3 presents the outcomes of the physical evaluation conducted on the system. The findings\nindicate that the incorporation of scene descriptions yields an 8.5% reduction to system success rate\nwhile adding an average of 7 seconds to the overall system execution time. For these reasons the\nscene description module may be considered dispensable in future iterations.\nFurthermore, discernible variations in success rates among zero-shot object detection models\nare observed. The ViLD model excels in accurately locating specified items within the scene but\nencounters challenges in distinguishing color differences between objects, as illustrated in Figure 9.\nConsequently, the model demonstrates inaccuracies, notably in predicting instances where multi-\nple objects of the same type but in different colors coexist, sometimes failing entirely to identify\nany instance of the object, as exemplified in query 2 of Figure 9. In contrast, Figure 10 illustrates\nthat the SAM-CLiP approach exhibits superior color identification capabilities. However, it is asso-\nciated with a significantly higher latency; during our experimental evaluations, the ViLD approach\ntook an average of 15 seconds for inference, while the SAM-CLiP approach took an average of 144\nseconds. Although the SAM-CLiP approach achieves heightened accuracy in detecting objects with\ndiverse colors, it occasionally groups disparate objects together if they share the same color\u2014such\nas categorizing a square green container as a green block, as demonstrated in query 1 in Figure 10.\nObservations indicate that zero-shot object detection models exhibit superior performance on\nsimulation images within the benchmark environment compared to real-world images tested on an\nactual robot setup. The accuracy of the detection models does not achieve commensurate levels when\n11\n\nFigure 8 : The human user has provided a prompt to place the green block in the blue bowl . The\nLLM uses the instruction provided and the affordance skills available to generate the plan which is\nthen executed by the robot\nFigure 9 : Zero-shot object recognition employing ViLD is presented in the diagram. The upper\nimage depicts the original scene, and the lower image exhibits a green bounding box drawn around\nthe identified target object. The diagram illustrates the algorithm\u2019s inability to identify query 2\nandquery 3 , accompanied by an erroneous prediction for query 4 .\nFigure 10 : Zero-shot object recognition using SAM-CLiP is depicted in the diagram. The upper\nimage represents the original scene, while the lower image displays a green bounding box drawn\naround the identified target object. The illustration demonstrates the model\u2019s accurate recognition\nofquery 2 ,query 3 andquery 4 . However, it exhibits a failure in identifying the object in query\n1, as it misidentifies the green box as the green block.\n12\n\nTable 3 : Physical robot environment evaluation\nViLD1SAM-CLiP2\nMethodology Task A Task B Task A Task B\nw/Feedback 60% 40% 60% 80%\nw/o Feedback 20% 40% 70% 80%\nw/o Scene Description 30% 40% 80% 70%\nNote: Each task is executed 10 times on the physical robot within the environment setup.\n1For the ViLD system the location of the receptors were hard-coded because the model was having difficulty identifying\nthe receptors in the scene.\n2The SAM-CLiP model used in the physical robot is the exact same as used in the VIMABench simulated environment.\nconfronted with real-world embodied images. This suggests a potential barrier to the practical appli-\ncation of this approach in real-world scenarios, necessitating the development of zero-shot learning\ndetection models tailored for real-world embodied images.\n5 Discussion\nThe presented research introduces a methodology to facilitate collaborative robot planning through\nhuman-robot interaction, harnessing LLMs. Subsequent evaluations were conducted in a simulated\nrobotics benchmark and on a physical robot arm setup. The ensuing discussion outlines significant\nfindings and insights gleaned from the experiments.\nThe proficient completion of object manipulation tasks by the robot arm, guided by human\nusers, underscores the efficacy of the approach. Leveraging language understanding and generation\ncomponents based on LLMs facilitated natural language communication, empowering users to issue\nhigh-level commands, deliver feedback, and optimize task plans dynamically during execution. This\nadaptability stands out as a pivotal strength of the methodology, indicative of the potential for\nheightened user agency in the realm of human-robot collaboration.\nThe system demonstrated its capacity to understand and execute a variety of user instructions,\nsuch as placing and relocating objects. While these set of manipulations are limited this system can\nbe extended to support more complex skills that can be composed together to achieve a larger set\nof tasks.\nLeveraging the VIMABench environment for bench-marking provided valuable insights into the\nsystem\u2019s performance, enabling an evaluation of its proficiency in comprehending and responding to\na diverse range of language instructions. The environment wrapper facilitated real-time feedback and\nadjustment, aligning the approach with principles of adaptability and continuous improvement.\nIn quantitative metrics, the system demonstrated 8.6% performance increase in simulated eval-\nuations and a 14% increase in physical evaluations. This shows that the method can be further\nexplored to enable collaborative task planning between humans and robots in physical and virtual\nenvironments.\nHowever, there are areas for improvement. As with many language models, the system\u2019s under-\nstanding and generation capabilities may occasionally exhibit limitations.", "start_char_idx": 30534, "end_char_idx": 40438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf090498-501a-4e10-b6fb-411592ea0e5a": {"__data__": {"id_": "cf090498-501a-4e10-b6fb-411592ea0e5a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, {"node_id": "535af1ae-4b4b-4082-9a2b-ecb9c394b99e", "node_type": "1", "metadata": {}, "hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "class_name": "RelatedNodeInfo"}]}, "text": "There is a need to address\nambiguities in user instructions and improve the system\u2019s capacity to ask clarifying questions when\nencountering unclear or contradictory language input.\nAdditionally, the study emphasizes the need for computational optimization, aligning with prior\nresearch on the computational constraints of language models, to enhance efficiency for deployment\nin real robotic systems.\n6 Conclusion\nThis research presents a system utilizing LLMs to facilitate collaborative robot planning through\nhuman-robot interaction. The amalgamation of language models with robot planning and execution\nintroduces compelling opportunities and challenges, paving the way for the evolution of intelligent\nand interactive robots. Previous studies have demonstrated LLMs\u2019 potential as generalist planners\nfor robot task execution in natural language. This research aims to contribute by transforming LLM-\ngenerated plans into an interactive process, fostering continuous interaction between the user and\nthe robot through the LLM.\n13\n\nProspective avenues for the project involve exploring the involvement of other AI language mod-\nels in evaluating plans generated by the LLM and providing iterative feedback until successful task\ncompletion. Alternatively, implementing a comprehensive memory system for the Robot LLM, akin\nto Generative Agents\u2019 [56] work, could enhance the LLM\u2019s ability to actively learn from these\ninteractions.\nReferences\n[1] Huang, S., Jiang, Z., Dong, H., Qiao, Y., Gao, P. and Li, H. (2023). Instruct2Act: Mapping Multi-\nmodality Instructions to Robotic Actions with Large Language Model. arXiv (Cornell University).\ndoi:https://doi.org/10.48550/arxiv.2305.11176.\n[2] Wu, J., Antonova, R., Kan, A., Lepert, M., Zeng, A., Song, S., Bohg, J., Rusinkiewicz, S. and\nFunkhouser, T., 2023. Tidybot: Personalized robot assistance with large language models. arXiv\npreprint arXiv:2305.05658.\n[3] A. I. K\u00b4 aroly, P. Galambos, J. Kuti and I. J. Rudas, \u201dDeep Learning in Robotics: Survey on Model\nStructures and Training Strategies,\u201d in IEEE Transactions on Systems, Man, and Cybernetics:\nSystems, vol. 51, no. 1, pp. 266-279, Jan. 2021, doi: 10.1109/TSMC.2020.3018325.\n[4] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson,\nJ., Vuong, Q., Yu, T. and Huang, W., 2023. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378.\n[5] Kovalev, A.K., Panov, A.I. Application of Pretrained Large Language Models\nin Embodied Artificial Intelligence. Dokl. Math. 106 (Suppl 1), S85\u2013S90 (2022).\nhttps://doi.org/10.1134/S1064562422060138\n[6] Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J. and\nGarg, A., 2023, May. Progprompt: Generating situated robot task plans using large language\nmodels. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 11523-\n11530). IEEE.\n[7] Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P. and Zeng, A.,\n2023, May. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) (pp. 9493-9500). IEEE.\n[8] Huang, W., Abbeel, P., Pathak, D. and Mordatch, I., 2022, June. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning (pp. 9118-9147). PMLR.\n[9] Zhao, X., Li, M., Weber, C., Hafez, M.B. and Wermter, S., 2023. Chat with the environment:\nInteractive multimodal perception using large language models. arXiv preprint arXiv:2303.08268.\n[10] Zhang, B. and Soh, H., 2023. Large language models as zero-shot human models for human-robot\ninteraction. arXiv preprint arXiv:2303.03548.\n[11] Xie, Y., Yu, C., Zhu, T., Bai, J., Gong, Z. and Soh, H., 2023. Translating natural language to\nplanning goals with large-language models. arXiv preprint arXiv:2302.05128.\n[12] Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J. and Stone, P., 2023. Llm+ p: Empow-\nering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.\n[13] Wake, N., Kanehara, A., Sasabuchi, K., Takamatsu, J. and Ikeuchi, K., 2023. Chatgpt\nempowered long-step robot control in various environments: A case application. arXiv preprint\narXiv:2304.03893\n[14] Ye, F., Zhang, S., Wang, P. and Chan, C.-Y. (2021). A Survey of Deep Reinforce-\nment Learning Algorithms for Motion Planning and Control of Autonomous Vehicles.\ndoi:https://doi.org/10.1109/iv48863.2021.9575880.\n14\n\n[15] Hadi, Muhammad Usman; tashi, qasem al; Qureshi, Rizwan; Shah, Abbas; muneer,\namgad; Irfan, Muhammad; et al. (2023). Large Language Models: A Comprehensive Sur-\nvey of its Applications, Challenges, Limitations, and Future Prospects. TechRxiv. Preprint.\nhttps://doi.org/10.36227/techrxiv.23589741.v3\n[16] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. and Cao, Y., 2022. React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\n[17] Lykov, A. and Tsetserukou, D., 2023. LLM-BRAIn: AI-driven Fast Generation of Robot\nBehaviour Tree based on Large Language Model. arXiv preprint arXiv:2305.19352.\n[18] Ding, Y., Zhang, X., Amiri, S., Cao, N., Yang, H., Esselink, C. and Zhang, S., 2022. Robot task\nplanning and situation handling in open worlds. arXiv preprint arXiv:2210.01287.\n[19] Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.H., Arenas, M.G., Chiang, H.T.L., Erez,\nT., Hasenclever, L., Humplik, J. and Ichter, B., 2023. Language to Rewards for Robotic Skill\nSynthesis. arXiv preprint arXiv:2306.08647.\n[20] Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A.,\nZhu, Y. and Fan, L., 2022. Vima: General robot manipulation with multimodal prompts. arXiv\npreprint arXiv:2210.03094.\n[21] Ding, Y., Zhang, X., Paxton, C. and Zhang, S., 2023. Task and motion planning with large\nlanguage models for object rearrangement. arXiv preprint arXiv:2303.06247.\n[22] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n[23] Dahiya, Abhinav, Alexander M. Aroyo, Kerstin Dautenhahn, and Stephen L. Smith.", "start_char_idx": 40439, "end_char_idx": 46776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0": {"__data__": {"id_": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d208a291-bfa1-4867-9fa5-3c4617ebd24f", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, {"node_id": "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7", "node_type": "1", "metadata": {}, "hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "class_name": "RelatedNodeInfo"}]}, "text": "\u201dA survey of\nmulti-agent Human\u2013Robot Interaction systems.\u201d Robotics and Autonomous Systems 161 (2023):\n104335.\n[24] Sharkawy, A.N., 2021. Human-robot interaction: Applications. arXiv preprint arXiv:2102.00928.\n[25] Billard, A., Robins, B., Nadel, J. and Dautenhahn, K., 2007. Building robota, a mini-humanoid\nrobot for the rehabilitation of children with autism. Assistive Technology, 19(1), pp.37-49.\n[26] Clabaugh, C., Tsiakas, K. and Mataric, M., 2017, September. Predicting preschool mathematics\nperformance of children with a socially assistive robot tutor. In Proceedings of the Synergies\nbetween Learning and Interaction Workshop@ IROS, Vancouver, BC, Canada (pp. 24-28).\n[27] Knudsen, M. and Kaivo-Oja, J., 2020. Collaborative robots: Frontiers of current literature.\nJournal of Intelligent Systems: Theory and Applications, 3(2), pp.13-20.\n[28] Vicentini, F., 2021. Collaborative robotics: a survey. Journal of Mechanical Design, 143(4),\np.040802.\n[29] Javaid, M., Haleem, A., Singh, R.P., Rab, S. and Suman, R., 2022. Significant applications of\nCobots in the field of manufacturing. Cognitive Robotics, 2, pp.222-233.\n[30] Huang, C., Mees, O., Zeng, A. and Burgard, W., 2023, May. Visual language maps for robot\nnavigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA)(pp.\n10608-10615). IEEE.\n[31] Bergerman, M., Maeta, S.M., Zhang, J., Freitas, G.M., Hamner, B., Singh,\nS. and Kantor, G. (2015). Robot Farmers: Autonomous Orchard Vehicles Help\nTree Fruit Production. IEEE Robotics & Automation Magazine, 22(1), pp.54\u201363.\ndoi:https://doi.org/10.1109/mra.2014.2369292.\n15\n\n[32] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,  L. and\nPolosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems,\n30.\n[33] Lin, T., Wang, Y., Liu, X. and Qiu, X., 2022. A survey of transformers. AI Open.\n[34] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33, pp.1877-1901.\n[35] Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R. and McHardy, R., 2023. Challenges\nand applications of large language models. arXiv preprint arXiv:2307.10169.\n[36] Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam,\nM., Chaplot, D.S., Maksymets, O. and Gokaslan, A., 2021. Habitat 2.0: Training home assistants\nto rearrange their habitat. Advances in Neural Information Processing Systems, 34, pp.251-266.\n[37] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess,\nD., Dubey, A., Finn, C. and Florence, P., 2023. Rt-2: Vision-language-action models transfer web\nknowledge to robotic control. arXiv preprint arXiv:2307.15818.\n[38] Tan, C., Xu, X. and Shen, F., 2021. A survey of zero shot detection: methods and applications.\nCognitive Robotics, 1, pp.159-167.\n[39] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S.,\nBerg, A.C., Lo, W.Y. and Doll\u00b4 ar, P., 2023. Segment anything. arXiv preprint arXiv:2304.02643.\n[40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,\nA., Mishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning (pp. 8748-8763).\nPMLR.\n[41] Gu, X., Lin, T.Y., Kuo, W. and Cui, Y., 2021. Open-vocabulary object detection via vision and\nlanguage knowledge distillation. arXiv preprint arXiv:2104.13921.\n[42] Zhu, P., Wang, H. and Saligrama, V., 2019. Zero shot detection. IEEE Transactions on Circuits\nand Systems for Video Technology, 30(4), pp.998-1010.\n[43] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakr-\nishnan, K., Hausman, K. and Herzog, A., 2022. Do as i can, not as i say: Grounding language in\nrobotic affordances. arXiv preprint arXiv:2204.01691.\n[44] Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker, S., Tombari, F., Purohit,\nA., Ryoo, M., Sindhwani, V. and Lee, J., 2022. Socratic models: Composing zero-shot multimodal\nreasoning with language. arXiv preprint arXiv:2204.00598.\n[45] Zhao, Z., Lee, W.S. and Hsu, D., 2023. Large Language Models as Commonsense Knowledge for\nLarge-Scale Task Planning. arXiv preprint arXiv:2305.14078.\n[46] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y. and Narasimhan, K., 2023.\nTree of thoughts: Deliberate problem solving with large language models. arXiv preprint\narXiv:2305.10601.\n[47] Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J.B., Shu, T. and Gan, C., 2023.\nBuilding cooperative embodied agents modularly with large language models. arXiv preprint\narXiv:2307.02485.\n[48] Chen, L., Zhang, Y., Ren, S., Zhao, H., Cai, Z., Wang, Y., Wang, P., Liu, T. and Chang,\nB., 2023. Towards end-to-end embodied decision making via multi-modal large language model:\nExplorations with gpt4-vision and beyond. arXiv preprint arXiv:2310.02071.\n16\n\n[49] Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y. and\nZhou, J., 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\n[50] Niryo. (n.d.). Ned2 Overview. [online] Available at: https://niryo.com/products-cobots/robot-\nned-2/.\n[51] Intel\u00ae RealSenseTM Depth and Tracking Cameras. (2019). Depth Camera\nD435i \u2013 Intel \u00aeRealSenseTM Depth and Tracking Cameras. [online] Available at:\nhttps://www.intelrealsense.com/depth-camera-d435i/.", "start_char_idx": 46777, "end_char_idx": 52470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d208a291-bfa1-4867-9fa5-3c4617ebd24f": {"__data__": {"id_": "d208a291-bfa1-4867-9fa5-3c4617ebd24f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad8f3299-76b9-4412-b26d-0c99273842f0", "node_type": "4", "metadata": {}, "hash": "322f9d22fa54e6ce3f8a65893d95254c9d251c01b0c31bbc6675fb9489899de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}]}, "text": "[52] W. A. Bainbridge, J. Hart, E. S. Kim and B. Scassellati, \u201dThe effect of presence on human-\nrobot interaction,\u201d RO-MAN 2008 - The 17th IEEE International Symposium on Robot and\nHuman Interactive Communication, Munich, Germany, 2008, pp. 701-706, doi: 10.1109/RO-\nMAN.2008.4600749.\n[53] A. Vick, D. Surdilovic and J. Kr\u00a8 uger, \u201dSafe physical human-robot interaction with industrial\ndual-arm robots,\u201d 9th International Workshop on Robot Motion and Control, Kuslin, Poland,\n2013, pp. 264-269, doi: 10.1109/RoMoCo.2013.6614619.\n[54] Bragan\u00b8 ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019. A brief overview of the use of\ncollaborative robots in industry 4.0: Human role and safety. Occupational and environmental\nsafety and health, pp.641-650.\n[55] Bragan\u00b8 ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019. A brief overview of the use of\ncollaborative robots in industry 4.0: Human role and safety. Occupational and environmental\nsafety and health, pp.641-650.\n[56] Park, J.S., O\u2019Brien, J., Cai, C.J., Morris, M.R., Liang, P. and Bernstein, M.S., 2023, October.\nGenerative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual\nACM Symposium on User Interface Software and Technology (pp. 1-22).\nAppendix A Section title of first appendix\nAn appendix contains supplementary information that is not an essential part of the text itself but\nwhich may be helpful in providing a more comprehensive understanding of the research problem or\nit is information that is too cumbersome to be included in the body of the paper.\n17", "start_char_idx": 52471, "end_char_idx": 54037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e283248-217b-4607-a828-330db0cb3c4b": {"__data__": {"id_": "8e283248-217b-4607-a828-330db0cb3c4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0d9f52d2-3071-4ee8-a412-ef19afe74560", "node_type": "1", "metadata": {}, "hash": "555d533be8d80d2d1ac5a4e5c0f9d743d94049d9de36614077629d25ccbe5783", "class_name": "RelatedNodeInfo"}, {"node_id": "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52", "node_type": "1", "metadata": {}, "hash": "81753b255890c1e9faaa2542760e9c0b56ae0263dfc788954b7f7b5ebddeaf9c", "class_name": "RelatedNodeInfo"}, {"node_id": "8fc2ed2a-4859-4c63-99a1-98da3c706bfb", "node_type": "1", "metadata": {}, "hash": "e65bdb33425c9550ff3557a545aff066af2b5b09ac516e7d5d6bb40653a67018", "class_name": "RelatedNodeInfo"}, {"node_id": "37fc32c5-6cd8-4dac-b4fe-f058d588625f", "node_type": "1", "metadata": {}, "hash": "7b2141d767cf0cf83153a7606ef946705fd486949811a308da4943f4e1b1ffa7", "class_name": "RelatedNodeInfo"}, {"node_id": "177dd3e4-5571-4ead-9052-bd8f23b37f89", "node_type": "1", "metadata": {}, "hash": "5ad1e2384df87657a290b1a7d59385f49000073fb75be3f077f97dd63efccd38", "class_name": "RelatedNodeInfo"}]}, "text": "Human-Robot interaction through joint robot planning\nwith Large Language Models\nKosi Asuzu1*\n1*Birmingham City University.\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot generalisation capa-\nbilities, expanding their utility beyond natural language processing into various applications.\nLeveraging extensive web knowledge, these models generate meaningful text data in response\nto user-defined prompts, introducing a novel mode of interaction with software applications.\nRecent investigations have extended the generalizability of LLMs into the domain of robotics,\naddressing challenges in existing robot learning techniques such as reinforcement learning and\nimitation learning. This paper explores the application of LLMs for robot planning as an alter-\nnative approach to generate high-level robot plans based on prompts provided to the language\nmodel. The proposed methodology facilitates continuous user interaction and adjustment of task\nexecution plans in real-time. A pre-trained LLM is utilized for collaborative human-robot plan-\nning using natural language, complemented by Vision Language Models (VLMs) responsible for\ngenerating scene descriptions incorporated into the prompt for contextual grounding.\nEvaluation of the system is conducted within the VIMABench benchmark simulated environ-\nment. Further practicality assessment involves experimentation with the system on a robotic arm\nengaged in tabletop manipulation activities. The observed results reveal that soliciting human\nfeedback for zero-shot plans generated by LLMs in robot manipulation yields an 8.6% perfor-\nmance increase in simulated evaluations and a 14% increase in physical evaluations compared to\nthe baseline, showcasing the efficacy of the proposed approach.\nKeywords: human-robot interaction, robotics, robot-arm, large language models, computer vision,\nnatural language processing\n1 Introduction\nThe field of robotics has experienced notable progress, primarily propelled by the integration of deep\nlearning techniques in perception, planning, and manipulation [3]. Recent advancements in Large\nLanguage Models (LLMs), exemplified by GPT and its successors, have positioned these models as\ntransformative tools across diverse robot learning applications [4][5]. With the impending realisation\nof General-Purpose Robots (GPRs), LLMs have garnered significant attention for their potential\nto revolutionize robot planning, a crucial aspect of autonomous systems [12]. The capacity to plan\nactions, make informed decisions based on language-defined instructions, and adapt to dynamic\nenvironments is paramount for robots operating in real-world scenarios.", "start_char_idx": 0, "end_char_idx": 2671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfc6a798-5b46-44dd-a887-491516a74f13": {"__data__": {"id_": "cfc6a798-5b46-44dd-a887-491516a74f13", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "aaa052f9-cbb5-41bf-aa39-144bb278d76f", "node_type": "1", "metadata": {}, "hash": "dd95b0e93f5cb8845523cdc34b5b1f970c2e3fc9bcd398f5936dc050acabdb60", "class_name": "RelatedNodeInfo"}, {"node_id": "3fd74188-bb64-49a6-b643-06b173e30da9", "node_type": "1", "metadata": {}, "hash": "9ee9bc9123c08777edb87d85d2f3880f2aee358156c16c23e6f82bc125110c21", "class_name": "RelatedNodeInfo"}, {"node_id": "892913dc-5691-4b8c-b89a-7cbd4b290020", "node_type": "1", "metadata": {}, "hash": "1bceddfe3ffcf273ebbfa06b93837bc5d69f8796c8b590711d0bbea9508503d1", "class_name": "RelatedNodeInfo"}, {"node_id": "a8420ba4-1923-44bc-ad6f-f039cb8dac2d", "node_type": "1", "metadata": {}, "hash": "8b01d54ef9a09f2ac8d5701b51c62bf346548f23c8f564aacfdaf281c9386833", "class_name": "RelatedNodeInfo"}, {"node_id": "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4", "node_type": "1", "metadata": {}, "hash": "790a7bf0b5173889d4b1c24df255ccf215069fa4026b62f0de1178babcff6e2c", "class_name": "RelatedNodeInfo"}, {"node_id": "b6bc5afb-53ff-400c-92b8-660acb67c15e", "node_type": "1", "metadata": {}, "hash": "d0c630e1f7dd315529172d79870f78a2edebead71c5acc5a9f34737766324fab", "class_name": "RelatedNodeInfo"}]}, "text": "This research explores the application of LLMs in the domain of robot planning, emphasizing\nthe incorporation of human feedback to enhance the robustness and accuracy of the generated task\nexecution plan. The proposed end-to-end solution leverages zero-shot learning vision models for\nobject detection, vision-language models for scene description, and the manipulation of a physical\nrobot arm using generated instructions. Traditionally, robot planning relied on carefully crafted\nalgorithms, heuristics, or learning-based techniques like reinforcement learning or imitation learning.\nWhile fruitful, these approaches often encounter challenges in unstructured environments, diverse\ntasks, and nuanced human interactions [14].\n1\n\nLLMs, with their proficiency in comprehending and generating open-ended, human-like text,\nintroduce new possibilities in natural language understanding, generation, and reasoning [15]. These\nmodels excel in processing and generating human-readable instructions, bridging the gap between\nhuman intent and robot actions. The proposed methodology utilizes GPT language models to gener-\nate high-level robot plans, facilitating the execution of robotic tasks. These plans consist of high-level\nAPI functions (referred to as \u2019skills\u2019[43]) employing rule-based motion and manipulation algorithms\nand deep learning models like Vision Transformers for zero-shot object detection and Vision Lan-\nguage Models (VLMs) for scene understanding [22]. This approach allows the language model to\ngenerate plans using these skills as interfaces for perception and manipulation.\nThis research endeavors to enrich the ongoing dialogue concerning the future trajectory of\nrobotics, where intelligent planning, natural language interaction, and the transformative influence\nof Large Language Models (LLMs) coalesce to redefine the manner in which robots perceive, navi-\ngate, and engage with their surroundings. Consequently, the delineated contributions of this paper\nare as follows:\n1.Human-interactive general purpose robotic system : This system extends LLM plan gener-\nation for robotics by incorporating a feedback loop and message stream, enabling users to provide\nfeedback on generated LLM plans.\n2.Scene understanding : Utilizing a VLM, this contribution involves incorporating a description of\nthe robot\u2019s scene. The description is then added to the prompt provided to the LLM for planning,\nserving as a grounding mechanism for the LLM.\nThis study employs LLMs to formulate robot plans for the execution of human instructions\nconveyed in natural language.", "start_char_idx": 2672, "end_char_idx": 5247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "843dae4f-9177-4be2-bea3-15b167c43370": {"__data__": {"id_": "843dae4f-9177-4be2-bea3-15b167c43370", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a4f63a6b-d521-4b74-bce9-8970fa75e32f", "node_type": "1", "metadata": {}, "hash": "edeb9f9f1ab9c2c2dbd7c036eb7467c9426a4721eef29b21a25eba66f1b0c6df", "class_name": "RelatedNodeInfo"}, {"node_id": "d560d311-94bc-47ae-bbbc-2a83a13270e2", "node_type": "1", "metadata": {}, "hash": "7c21e94fcd4343da4c5dd8348aea69671b28765ff39fc12f1c8b7308fda915ce", "class_name": "RelatedNodeInfo"}, {"node_id": "02531e66-29d7-4df8-89b7-184cb314c3f6", "node_type": "1", "metadata": {}, "hash": "6138f49b24f87e8e692cffb8eddd4f2170b05c5331eabc3fb451118ac7ad2898", "class_name": "RelatedNodeInfo"}, {"node_id": "3dfbe97c-0ec0-466d-95a7-85757c75dc0a", "node_type": "1", "metadata": {}, "hash": "d53393852cb46475ca82e9d69fc700fdbe2f9a2a3fc7ebc96dde0a92d8b94f78", "class_name": "RelatedNodeInfo"}, {"node_id": "a766a312-c240-47be-a6b9-6c8a0f30effe", "node_type": "1", "metadata": {}, "hash": "ec53f0895b0c8a11a281f1984e2817f33719d99ecec0c3f7476e2d1675b7667e", "class_name": "RelatedNodeInfo"}]}, "text": "It facilitates real-time interaction and allows plan adjustments by the\nhuman user, as illustrated in Figure 1. The feedback, instructions, and plans are stored as context,\nintegrated into the message stream to enhance the responsiveness of the LLM. Subsequent evaluations\nscrutinize the proposed approach within a simulated robot learning benchmark, complemented by\nassessments on an operational robotic platform tailored for specific tabletop manipulation tasks.\nFigure 1 : Human robot interaction through joint planning with LLM system diagram. The diagram\nemphasizes the intricate dynamics between the human user and the LLM, with the storage of inter-\naction context in the message stream and subsequent execution of the robot plan.\n1.1 Related Work\n1.1.1 Human-Robot Interaction\nHuman-robot interaction (HRI) is a multidisciplinary field that explores the design, development,\nand evaluation of systems where humans and robots interact in various settings. Over the years,\nHRI has witnessed significant advancements, leading to an array of research and applications [23].\nIn collaborative and assistive robotics, researchers explore scenarios where robots work alongside\nhumans to enhance productivity or provide assistance [24] [26]. Pioneering investigations in human-\nrobot interaction have explored various applications, such as robot-assisted therapy. Notable research\nin this domain includes efforts to leverage social robots for aiding low-functioning children with autism\n2\n\n[25]. The broader landscape of human-robot interaction encompasses a diverse spectrum of studies\nand advancements, covering aspects like social robotics, design principles, collaborative frameworks,\nethical considerations, and technological innovations[53][52]. In the context of this research project,\na dyadic interaction system is devised to facilitate human-robot interaction in the planning of robot\ntasks, involving collaboration between a single robot and a human partner.\n1.1.2 Collaborative Robot arms in Industry\nCollaborative robotics, also known as cobots, represent a transformative paradigm in the manufac-\nturing and industrial sectors [54]. These robots are designed to work alongside human operators,\nfacilitating safer, more efficient, and highly flexible production processes [27]. A convergence of\nresearch in safety, human-robot interaction, task allocation, programming interfaces, industry-specific\napplications, and connectivity has laid the foundation for the widespread adoption of collaborative\nrobots in a variety of industrial settings [28].", "start_char_idx": 5248, "end_char_idx": 7807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6944eebd-6e1a-49f1-82eb-910d01b2d95a": {"__data__": {"id_": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00df92de-8dba-43a8-af11-19ce18c1df5e", "node_type": "1", "metadata": {}, "hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c24ccb24-0ecf-4a0d-b736-aa3765f5e854", "node_type": "1", "metadata": {}, "hash": "9ab438c1bb38602e11a4223744801a09bf364180371e9cc2104ebed5d9396e6c", "class_name": "RelatedNodeInfo"}, {"node_id": "2aa7ecba-c61b-49b5-b220-1fca807df300", "node_type": "1", "metadata": {}, "hash": "ddd4af6f205afd4350802a4fc56bde65123e0db010883f40acdb16002984615d", "class_name": "RelatedNodeInfo"}, {"node_id": "4e140397-4561-4086-b5d8-5b4f8095eeb4", "node_type": "1", "metadata": {}, "hash": "5208ebac2c3a164f6f46957a83033eeba166d492a678adcd2608295f233e24f8", "class_name": "RelatedNodeInfo"}, {"node_id": "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c", "node_type": "1", "metadata": {}, "hash": "0c1b1c724b8ffd4a837485ab4eec43d46f65e0154f89b7fa693c1374ad073a08", "class_name": "RelatedNodeInfo"}, {"node_id": "178675e5-5624-4cba-86c4-41443ef806be", "node_type": "1", "metadata": {}, "hash": "fc7d1236a106213f3b335aa885a1d5fecf82aacdcc4aa6f6a759d2d86d7c6018", "class_name": "RelatedNodeInfo"}]}, "text": "Collaborative robotics has brought transformative changes to the industrial landscape, offer-\ning innovative solutions for improving efficiency, safety, and flexibility in manufacturing processes\n[29]. Research conducted by Marcel Bergerman and Silvio M. Maeta primarily concentrates on the\ndesign of collaborative robots and their seamless integration into established farming processes [31].\nInvestigations in this domain have delved into factors such as comprehending and optimizing the\ninteraction between human workers and robots to elevate productivity. Effectively allocating tasks\nbetween human and robot workers represents a crucial research challenge.\nAs the field continues to evolve, the development of even more intuitive and adaptive collabo-\nrative robots is expected, addressing increasingly complex challenges in modern manufacturing. A\nframework is proposed to enable joint real-time planning between humans and intelligent collabo-\nrative robots using natural language. This approach is intended for application in industries where\nhumans and robots collaborate in shared environments.\nPrior investigations have delved into the integration of collaborative robot arms within Indus-\ntry 4.0 manufacturing environments, demonstrating their capability to enhance overall production\nefficiency by aiding workers in both physical and cognitive tasks [55]. The study underscores the\ndynamic nature of human involvement in the manufacturing process and emphasizes the necessity\nfor human operators to adapt effectively to interact with these tools. The current research delves\ninto language as an interface for facilitating dynamic interaction within this context.\n1.1.3 Large Language Models\nThe Transformer architecture, introduced by Vaswani et al. [32], represents a foundational break-\nthrough in the development of LLMs. Transformers rely on self-attention mechanisms to capture\ncomplex dependencies between words in a sequence, enabling parallelism and scale. This architecture\nforms the basis of many subsequent language models, such as BERT, GPT, and XLNet [33].\nLLMs represent a revolutionary leap in natural language processing and understanding. The\ncombination of sophisticated architectures, massive data-sets, fine-tuning strategies and multilingual\ncapabilities has paved the way for their widespread adoption and applications across various domains\n[35].\nAs research in this field continues to evolve, the development of more sophisticated, context-aware,\nand ethically aligned language models is expected, offering exciting opportunities and challenges for\nthe future of AI-powered language understanding and generation. One of such future opportunities\ninclude enabling new forms of language inference for robotics application.", "start_char_idx": 7808, "end_char_idx": 10566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00df92de-8dba-43a8-af11-19ce18c1df5e": {"__data__": {"id_": "00df92de-8dba-43a8-af11-19ce18c1df5e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883", "node_type": "1", "metadata": {}, "hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b70a610f-59be-4ed3-9210-3fd4af7e34cb", "node_type": "1", "metadata": {}, "hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "class_name": "RelatedNodeInfo"}]}, "text": "One of such future opportunities\ninclude enabling new forms of language inference for robotics application.\n1.1.4 Zero-shot object detection\nZero-shot learning aims to identify objects without available labels during training, enabling classifiers\nto recognize unseen classes [38]. Zero-shot detection (ZSD) is designed to simultaneously localize\nand recognize objects from novel categories [38][42].", "start_char_idx": 10459, "end_char_idx": 10859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3de7bbe1-118a-45de-ab42-327e9f821b94": {"__data__": {"id_": "3de7bbe1-118a-45de-ab42-327e9f821b94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8c26192c-4bb1-4fe5-92a4-0cfd6afc672c", "node_type": "1", "metadata": {}, "hash": "460bcb7c0b5f667968b98861d24282042078edbea5d43fe84392f6ca9967b179", "class_name": "RelatedNodeInfo"}, {"node_id": "ccdc64e7-8578-402c-adfc-7f9bedebc2fa", "node_type": "1", "metadata": {}, "hash": "bf6da2e2d1f0cc03419c25db15eaf97b5350d817d5a1b0184cf50ec73e9e05d0", "class_name": "RelatedNodeInfo"}, {"node_id": "e748def4-5a55-4031-947f-9004d685688c", "node_type": "1", "metadata": {}, "hash": "bd0d69ebba0018a2cae92f34220d050df176edcfae5ad7ee016ac226178cb12c", "class_name": "RelatedNodeInfo"}, {"node_id": "7919a516-e2f9-4871-b726-7ef1660feebd", "node_type": "1", "metadata": {}, "hash": "12eca0246b68dca61368d35769141668b919226c4837cac3a4554e92d06603d6", "class_name": "RelatedNodeInfo"}, {"node_id": "70f7146f-e6e4-4d84-900e-aac0131533f9", "node_type": "1", "metadata": {}, "hash": "54c22477f765376339278c39e2303b60bb566b14ff94499aa0e136020d07a1c7", "class_name": "RelatedNodeInfo"}]}, "text": "ZSD algorithms fall into classifier-based and\ninstance-based methods. Classifier-based methods concentrate on amalgamating a traditional object\ndetection framework with a zero-shot learning classifier. In the case of instance-based methods,\ninspired by synthesis techniques in zero-shot learning, the typical approach involves initially training\na traditional object detection with seen objects and subsequently updating the confidence predictor\nusing synthesized images or visual features.\n3\n\nThis paper employs two classifier-based methods, namely SAM-CLiP[39][40] and ViLD[41], for\nidentifying objects in a scene based on a query.\nSAM-CLiP utilizes contrastive learning on both natural language and images to enable zero-shot\nobject classification. The Segment Anything Model (SAM) is a segmentation model facilitating data\nannotation and zero-shot transfer through prompt engineering. CLiP replaces the original classifier\nwith zero-shot learning classifiers [1].\nIn the second vision approach, ViLD leverages vision and language knowledge distillation. This\ninvolves distilling knowledge from a pre-trained open-vocabulary image classification model into a\ntwo-stage detector.\nThe selection of ViLD and SAM-CLiP as the zero-shot detection models in this study is predicated\non their accessibility and prior utilization in analogous projects such as SayCan and Instruct2Act.\nIt\u2019s essential to note that this research does not conduct a comprehensive assessment of all existing\nzero-shot models or determine the optimal ones; rather, the choice of models is contingent upon their\navailability.\nViLD comprises two components, learning with text embeddings and image embeddings derived\nfrom an open vocabulary image classification model. The ViLD model, also used in SayCan and\nSocratic Models, serves as the vision model in this implementation [43][44]. This literature review\nhighlights key approaches in the realm of zero-shot learning and detection, providing a foundation\nfor the current research.\n1.1.5 LLMs for Robot Planning\nThe integration of LLMs with robotics has opened new frontiers in robot planning and execution\n[17]. These models empower robots with natural language understanding, enabling them to interact\nwith humans and external systems through text or speech [10].\nOne of the foundational applications of LLMs in robotics is the understanding of natural lan-\nguage commands [4] [9]. Research in this area has explored methods for mapping text or spoken\nlanguage into executable robot commands [1].", "start_char_idx": 0, "end_char_idx": 2520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d18e9348-dfee-468f-85ad-d27e16ec8090": {"__data__": {"id_": "d18e9348-dfee-468f-85ad-d27e16ec8090", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "25f39df5-e777-427c-a9a0-a661f7ca48ab", "node_type": "1", "metadata": {}, "hash": "701b4d322c8220cb4525b80dfb544d9e0d5bb220b33935aac593b039bc297249", "class_name": "RelatedNodeInfo"}, {"node_id": "f4023064-a0ff-4c94-a537-214611f7c55a", "node_type": "1", "metadata": {}, "hash": "129965f24bfbb769058b4d3fd980d88037a8532e8c3a23f925e816b06b5d172e", "class_name": "RelatedNodeInfo"}, {"node_id": "78598da2-b279-46d4-aaf0-fefca676c30d", "node_type": "1", "metadata": {}, "hash": "84af333610b2ca463f614a413ca1625a4dd006b5b1eabfb18da9a66c0b22ab73", "class_name": "RelatedNodeInfo"}, {"node_id": "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8", "node_type": "1", "metadata": {}, "hash": "dbbbe91bdc83edd1df308d2fa8a0a72ea92a71d197e06c3f9f2f9f5df0d1745a", "class_name": "RelatedNodeInfo"}, {"node_id": "7159ecb2-ba9a-49a2-9294-ac6a6e52ccf7", "node_type": "1", "metadata": {}, "hash": "7bd07ca2b81e49b420b289efd8ef3dfd63a79a6b5b3d9f6292c8f5b8d4843012", "class_name": "RelatedNodeInfo"}]}, "text": "Seminal works by Brohan et al. and Jiang et al. have\ndemonstrated the feasibility of converting human instructions into robot actions. LLMs have been\nemployed to improve robotic task planning and execution [37] [20]. TidyBot show how LLMs can be\nused in the personalisation of robot policy [2].\nPrior studies have delved into the application of Large Language Models (LLMs) in planning\nwithin a communicative environment [47]. In their work, a specialized framework was formulated for\ncooperative agents operating within a multi-agent embodied environment. Showcasing the capabilities\nof the GPT-4 model, the investigation illustrated its capacity to outperform robust planning-based\nmethods, exemplifying emergent effective communication devoid of the necessity for fine-tuning.\nThe LLM-MCTS (Monte Carlo Tree Search) investigation contributed valuable insights by reveal-\ning that LLMs not only provide a policy for action but also offer a commonsense model of the world\n[45]. Monte Carlo Tree Search, a powerful search algorithm, was employed in this exploration. This\ninvolved integrating the world model into the policy for a search algorithm like MCTS. Concurrently,\nLLMs with Tree-of-Thought (LLM-ToT) approach adopted a tree-based strategy, enabling LLMs to\nengage in deliberate decision-making by assessing various reasoning paths and self-evaluating choices\nto determine the subsequent course of action [46].\nThe results of these studies highlight the effectiveness of search-based plan generation over policies\ninduced by LLMs. Nevertheless, given the straightforward nature of the tasks in these experiments,\nthe decision is made to refrain from employing the search-based approach. Instead, reliance is placed\non the LLM-induced policies in their original form.\nRecent research endeavors have prominently concentrated on formulating task plans grounded\nin high-level natural language descriptions, thereby endowing robots with the capability to execute\nintricate tasks with minimal human intervention [34] [21] [18]. Yu et al. advanced the field by utilizing\na language model to generate rewards applicable to robots for skill synthesis [19]. Another notable\napproach, denoted as LLMs with optimal Planning proficiency (LLM+P), leverages language models\nto produce planning directives that robots can subsequently employ for executing complex tasks\n[12].", "start_char_idx": 2521, "end_char_idx": 4888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "689d7d2f-c966-411d-9425-fc11d5ac733d": {"__data__": {"id_": "689d7d2f-c966-411d-9425-fc11d5ac733d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f051495e-b4cf-462e-b42a-67f9b5ebd4c0", "node_type": "1", "metadata": {}, "hash": "6065556a5f00d36a5b50590335aa6dbdcccaaffc81ced2cbb8a1474da8acd3cb", "class_name": "RelatedNodeInfo"}, {"node_id": "43d26dba-e6a1-48c4-ac6f-513f6d1ceace", "node_type": "1", "metadata": {}, "hash": "267c7fbf07b5d0bb58522393e8e32d301d090e8df80c8a47130bf8523929250b", "class_name": "RelatedNodeInfo"}, {"node_id": "1407828d-734e-4103-b05f-1c3429cf8784", "node_type": "1", "metadata": {}, "hash": "d3facc49994d9386495173f35ec304aac0bb4a2ef1d695076d72034dda496f7f", "class_name": "RelatedNodeInfo"}, {"node_id": "c17b65e8-1f37-4a18-beea-1450866636f4", "node_type": "1", "metadata": {}, "hash": "5310d24e111644c1c2707d0a1662cc413afef6eaf1196a3abae16ea9ab676bba", "class_name": "RelatedNodeInfo"}]}, "text": "LLM+P employs language models to address Planning Domain Definition Language (PDDL)-\nbased planning problems. Significantly, the researchers demonstrate that the language understanding\ncapabilities of LLMs empower them to apply common sense reasoning to terms within the PDDL\nproblem.\n4\n\nThe decision to omit the PDDL approach in this study is grounded in the intricate nature\nof parsing PDDL instructions into robot actions, as such intricacies fall beyond the scope of this\ninvestigation.\nLLM-Brain paper proposes a memory and action system for embodied control and configuration\nfor a nervous system with the LLM acting as the central brain[17]. Code-as-Policies uses language\nmodels to generate executable python code using API directives which can then be given to a robot\nas policies [7]. Methods such as Instruct2Act and ProgPrompt focus on generating instructions using\npredefined high level robot functions without any programming primitives [1] [6]. This approach\nstands as the prevailing method in contemporary literature and is the methodology employed in the\ndevelopment of the system in this research [8] [9] [11] [17] [18]. While ProgPrompt integrates program-\nming language structures, such as loops, lists, and function definitions, into its robot plan generation,\nthis approach diverges by refraining from the explicit utilisation of programming structures. Instead,\nmost implementation complexities are managed behind the API definition, akin to Instruct2Act.\nThis streamlined approach simplifies the generated LLM plans and the feedback process.\nThe synergy between LLMs and robotics has ushered in an era of more intuitive, versatile, and\nadaptive robotic systems [37]. As research in this field advances, the development of robots that under-\nstand and communicate with humans through natural language continues to unlock new possibilities\nin domains such as home automation, healthcare, education, and industry [2].", "start_char_idx": 4889, "end_char_idx": 6827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de": {"__data__": {"id_": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dac9c22d-586d-41fc-9efe-27c747d1c629", "node_type": "1", "metadata": {}, "hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2ae020f6-3531-47c0-acfb-5c2635f4d78f", "node_type": "1", "metadata": {}, "hash": "9353ff451cd12e3010e3138f92e760878685958101b286893477b9d9384ec63a", "class_name": "RelatedNodeInfo"}, {"node_id": "2b9a7b44-a020-4a1e-8471-3b9ca588f56d", "node_type": "1", "metadata": {}, "hash": "b1cc68771ccfea5f64075ce2e2c3038740627bd4ee7729ff067718d300853fa9", "class_name": "RelatedNodeInfo"}, {"node_id": "13103365-ee6e-468d-8679-47406168ad1e", "node_type": "1", "metadata": {}, "hash": "2cef2a32a39108ae84b7d9b1e8083a885385f33cd56b7bf2c146ceaa5592baa2", "class_name": "RelatedNodeInfo"}, {"node_id": "2970cdd8-7cc5-441e-9180-3f0d35aed72b", "node_type": "1", "metadata": {}, "hash": "4c9853b85b783cb771531a481132463bfdd8af64a380cf044efc97796273b404", "class_name": "RelatedNodeInfo"}, {"node_id": "ae2718cf-b324-43b2-80af-608864bbd060", "node_type": "1", "metadata": {}, "hash": "41191277b84541d7df93f9645c55c3dea9a038dc3a4860f9d7db93c451522625", "class_name": "RelatedNodeInfo"}]}, "text": "Table 1 : Physical robot environment evaluation\nRelated Projects\nProject name Methodology Evaluation tech-\nniqueReference\nInstruct2Act Using multimodal instructions pro-\nvided to pre-trained model to gen-\nerate highlevel robot plan based on\npython APIsSimulation [1]\nTinyBot Using language models to generate\nrobot plans based on user prefer-\nences from a set of examples via\nprevious interactionsSimulation &\nPhysical Robot[2]\nPaLM-E Using multi-modal transformer\nmodel trained end-to-end on\nvision, language and continuous\nactions for sequential robot manip-\nulation planning, visual question\nanswering and captioningPhysical Robot [4]\nCode-As-Policies Using code writing language mod-\nels to write python code which is\nthen used as a robot policySimulation [7]\nProgPrompt Using program like instructions\nas input to pre-trained language\nmodel to generate programmatic\nplan instructions for robot controlSimulation [6]\nLLM-BRAIn Using a fine-tuned LLM to gener-\nate robot behaviour tree to control\nrobotSimulation [17]\nLLM-P Using language models to generate\nplans in the planning domain def-\ninition language (PDDL) given a\nhigh level planning problemSimulation [12]\nRT-2 Trained transformer model using\ninternet scale vision and language\ndata as well as low level robot con-\ntrol instructions which is then used\nfor generating robot actions given\nvisual and language instructionsPhysical Robot [37]\nSayCan Using pre-trained language models\nto suggest robot affordances based\non provided instructionPhysical Robot [43]\n5\n\n2 Joint robot planning with Large Language Models\nOur system integrates LLMs to enable collaborative robot planning through interactions with human\nusers as shown in Figure 2. The primary objective is to establish a dynamic planning environment\nwhere humans can work in unison with robots by providing high-level task instructions, feedback,\nand plan adjustments.\nIn this paper few-shot prompting is used to guide the model to generate outputs from a defined\ndistribution of actions (\u03a0). The set of low level skills (\u03a0) and the language descriptions (\u03a0 d) are\nprovided along with the instruction ( I) and a description of the scene ( S) where the robot is located.\n2.1 Language Grounding\nLLMs are not naturally grounded in the real world, providing a set of API skills give the LLMs\ngrounding to enable them take actions in the real world.", "start_char_idx": 6828, "end_char_idx": 9192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dac9c22d-586d-41fc-9efe-27c747d1c629": {"__data__": {"id_": "dac9c22d-586d-41fc-9efe-27c747d1c629", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "node_type": "1", "metadata": {}, "hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3406b63b-8760-44bc-bad2-60f68ad85a22", "node_type": "1", "metadata": {}, "hash": "7fbace106ea6f4b25b6e38a3bf6a53a4196dc0947e0a21d89e22cb7ed4be64ac", "class_name": "RelatedNodeInfo"}, {"node_id": "01020a24-b2d0-43f9-9c09-4f28a0ff2557", "node_type": "1", "metadata": {}, "hash": "6269235c94a7bc3062c7ede7e97fd37080ac9da714262a6869652370674c96c1", "class_name": "RelatedNodeInfo"}]}, "text": "The skill functions constrain completions to\nthe skill descriptions, enhancing the LLM\u2019s awareness of the robot\u2019s capabilities and ensuring that\nthe generated natural language actions are both feasible and contextually appropriate.\nIn contrast to the approach used in designing affordances in SayCan, this methodology deviates\nby omitting the utilisation of a reinforcement learning algorithm for acquiring language-conditioned\nvalue functions. Instead, the definition of robot affordances relies on the manual programming of\nmanipulation and motion algorithms.\nUtilizing the GPT-4 Vision Language Model [48] is employed to furnish scene descriptions ( S)\nfor the LLM prompt, thereby enabling the integration of object affordances into the LLM\u2019s plan\ngeneration.", "start_char_idx": 9193, "end_char_idx": 9955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e7527b6-f6c8-4689-8d08-a92632a126ed": {"__data__": {"id_": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b86032ac-9442-4216-ba6b-9afa9612fbb9", "node_type": "1", "metadata": {}, "hash": "e2be555d9672279a599f6c588df01119d17a574b8e09da5c03b0e605ff876723", "class_name": "RelatedNodeInfo"}, {"node_id": "123060e0-ef82-4211-9ac2-8b7958ed9ffc", "node_type": "1", "metadata": {}, "hash": "95414fca3517dc99c4ff51a8cc9e59023a318f416a9a2ff8d7713847633af75a", "class_name": "RelatedNodeInfo"}, {"node_id": "69cf0473-e62a-4861-a3ed-572985701870", "node_type": "1", "metadata": {}, "hash": "18d4eca43dea4efeefa8283975dd857a1fd63576753abdb9984ff10350712d31", "class_name": "RelatedNodeInfo"}, {"node_id": "4c056a44-4ed4-4f29-af60-bc75de435d21", "node_type": "1", "metadata": {}, "hash": "09f50c0914d40b0677f0cd60cc8d2c95d9539728e47158bc6f06161a876ab8fb", "class_name": "RelatedNodeInfo"}, {"node_id": "d8b7d0cb-c03d-4972-abaa-0708bbb9beb0", "node_type": "1", "metadata": {}, "hash": "2cff1b743defb5186f9604f7853541fa11c6ecb594631d5bbf1c7539ff153019", "class_name": "RelatedNodeInfo"}]}, "text": "Figure 2 : Continuous feedback process between human user and LLM to generate robot plan.\nThe human provides feedback instructions in natural language which the LLM uses to update the\ngenerated plan\n2.2 Perception with Foundation Vision Models\n. As part of the set of robot skills, perception actions are defined, incorporating foundational vision\nmodels for zero-shot object detection and scene understanding.The vision models are employed for\nobject identification within the scene, yielding bounding boxes for detected objects. Subsequently,\npixel-to-coordinate values are computed. The perception functions operate beneath the rule-based\nactions, albeit not directly presented as skills to the LLM. Full object generalisation is possible by\ncombining the SAM model for image segmentation and the CLiP model for classification of those\nsegments. In the case of the ViLD model, a pre-defined list of possible objects is provided to the\n6\n\nAlgorithm 1 LLM robot joint plan generation\nGiven : A high level instruction I, state S, and \u03a0 a set of skills and their language descriptions \u03a0 d\n1:I\u21d0Instruction\n2:\u03a0\u21d0Skills\n3:S\u21d0State\n4:MessageStream \u21d0Store previous messages and responses\n5:MaxFeedback \u21d0MaxFeedbackCount\n6:while I\u0338=stopdo\n7: S\u21d0SceneDescription (Camera )\n8: P\u21d0LLMPlanGenerator (I, S,\u03a0, MessageStream )\n9: MessageStream +I \u25b7 add instruction to message stream\n10: MessageSteam +P \u25b7 add llm generated plan to message steam\n11: NumFeedback \u21d00\n12: InstructionApproved \u21d0RequestApproval (P)\n13: while InstructionApproved \u0338=True &&NumFeedback < MaxFeedback do\n14: Feedback \u21d0RequestFeedback (P)\n15: P\u21d0LLMPlanGenerator (F, S, \u03a0, MessageStream )\n16: MessageSteam +F\n17: MessageSteam +P\n18: InstructionApproved \u21d0RequestApproval (P)\n19: end while\n20: RobotExecute (P)\u25b7execute robot instructions using low level APIs based on generated plan\n21:end while\nmodel before initiation. Notably, the model does not detect objects beyond the predetermined options\nin the list.", "start_char_idx": 0, "end_char_idx": 1960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73dd9204-cda7-4bbd-b64c-daf82a689ddd": {"__data__": {"id_": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e10709d3-92ff-47c2-8da9-27e20d05e99a", "node_type": "1", "metadata": {}, "hash": "8426468f595b3ee5efbb038992da0dc251fcbb2b82fb90372edbac528d5996bd", "class_name": "RelatedNodeInfo"}, {"node_id": "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8", "node_type": "1", "metadata": {}, "hash": "03b1f2896e55ef3b8f6fbdae3ba07a7ce31759d72afa364cc9a42db2f38504e5", "class_name": "RelatedNodeInfo"}, {"node_id": "43987c9d-fcbe-4f29-bc90-36429a295adb", "node_type": "1", "metadata": {}, "hash": "cedf9092f16b2f6c47c56ededc38c07f479c8dbbc066c878269739ef9c9f0f27", "class_name": "RelatedNodeInfo"}, {"node_id": "852d47eb-cef2-4b84-9efd-e212ab46cff6", "node_type": "1", "metadata": {}, "hash": "67ed8dd45ee9d1fdaa95ebb04e711e8c2010b803ed23d75a2df419d00a00d41f", "class_name": "RelatedNodeInfo"}, {"node_id": "1400cf08-5d42-45ab-a44d-1e465d29fd06", "node_type": "1", "metadata": {}, "hash": "b9ca4fb4756075afcad20038f20b28a9139b117a7d0eb06abcc4179b514986dc", "class_name": "RelatedNodeInfo"}]}, "text": "Notably, the model does not detect objects beyond the predetermined options\nin the list. Each item in the specified list is assigned a probability score, subsequently employed to\nidentify the presence of objects in the scene and ascertain their respective locations.\n2.3 Prompt Design\n. The GPT-3.5 model [49] is utilized for prompting through an accessible API. The prompt design for\nthe LLM encompasses instructions regarding the available robot skills \u03a0 dand illustrative examples\ndemonstrating the application of these skills in conjunction. This methodology aligns with the few-\nshot prompting technique.\nA prompt template is formulated by integrating the set of robot skills \u03a0, the language descriptions\nof these skills \u03a0 d, example plans with corresponding instructions Pe, a given instruction I, and a\ndescription of the current robot location, constituting the state S. These variables are then passed\ninto the LLM plan generation function which is used to generate a robot execution plan \u03c1as shown\nin Figure 3. The action scoring approach described in SayCan is not employed; instead, a few-shot\nprompting approach, as outlined in Instruct2Act, is utilized.\n2.4 The Message Stream\nThe incorporation of human interactivity in robot planning is facilitated through the utilisation of\nthe message persistence feature within the OpenAI API. This capability enables the utilisation of a\nthread of conversations within the prompt. New instructions from the user and previously generated\nplans by the LLM are appended to the message stream, as depicted in Figure 1. User messages are\ncategorized into instructions and feedback. The size of the message stream is constrained by removing\nprior context to avoid exceeding the context window limit.\nThe interactive planning and execution process begins with users providing high-level task descrip-\ntions and instructions in natural language, subsequently processed by the LLM. Plan generation\nbased on this initial input is the next step, where the LLM generates an initial task plan. However,\nthis plan may be incomplete or sub-optimal, which is why users are given the opportunity to provide\nfeedback and adjustment commands through natural language interactions. The iterative planning\nprocess continues as the system interprets and acts on user feedback, adjusting the plan until user\nsatisfaction is achieved. The final agreed-upon plan is executed by the robot in the real or simulated\nenvironment.", "start_char_idx": 1872, "end_char_idx": 4326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adfb9d0c-8f3c-4922-be13-e6b17a2cb838": {"__data__": {"id_": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "97a9637e-637b-43a3-9842-34beced18813", "node_type": "1", "metadata": {}, "hash": "48614cd16bec97836dad5b95c06197d80f37579c1129ff5551b7e22146561300", "class_name": "RelatedNodeInfo"}, {"node_id": "e16e2fef-45e6-4638-b561-ddd00e0ebee7", "node_type": "1", "metadata": {}, "hash": "d7d7854eef3fd161be99b1c15ecd44115e00458723b60f0f40a88f46926ea5f0", "class_name": "RelatedNodeInfo"}, {"node_id": "2631d6e0-e208-4389-a568-5d229055d8b6", "node_type": "1", "metadata": {}, "hash": "0aa3fedd69ffa975dc8d4964c40f90cd75cee65629da79f28f4e82b62112f4d2", "class_name": "RelatedNodeInfo"}, {"node_id": "7de4be5c-f717-43bd-b932-844e2b16d00b", "node_type": "1", "metadata": {}, "hash": "3ae53c81581c77be64d487bb916b2b7d6ff393711bd7a5e1a69286ad9be8dd1e", "class_name": "RelatedNodeInfo"}, {"node_id": "ba291a7d-f31c-411c-93a0-8fc765f10b12", "node_type": "1", "metadata": {}, "hash": "99bd8b1f09aab1adca233c6511d6054908f60b57d3f1c95a38f047132da27015", "class_name": "RelatedNodeInfo"}]}, "text": "The final agreed-upon plan is executed by the robot in the real or simulated\nenvironment.\n7\n\nFigure 3 : Instructions, skills and their examples and scene descriptions are combined into a single\nprompt and sent to the LLM, the LLM then uses this information to generate the robot execution\nplan.\n2.5 Motion and Manipulation\nA set of rule-based actions is devised for the robot, encompassing operations such as pick-and-place,\npick, move-left, etc. These actions are meticulously crafted and fine-tuned for enhanced accuracy.\nThe functions are accessible through APIs and can be executed sequentially when the robot plan\nis generated. Termed as \u2019skills,\u2019 these actions represent a defined set of activities achievable by the\nrobot.\nThis methodology integrates the components of the research, emphasizing the comprehensive\napproach utilized to facilitate joint robot planning through human-robot interaction with the\nassistance of LLMs.\n3 Experimental Setup and Procedure\nTo empirically assess the effectiveness of the system, a series of experiments were conducted in\nboth simulated and physical environments, each involving a solitary human participant. A terminal\nenvironment was provided to facilitate the transmission of textual instructions and feedback to\nthe system. Throughout the experiment, the human participant interacted with the robot, issuing\nhigh-level natural language instructions, feedback, and adjustment commands. The robot\u2019s language\nunderstanding component, powered by an LLM, interpreted and responded to these instructions\nand commands. The planning and execution process remained dynamic, incorporating iterative plan\nadjustments in response to the singular user\u2019s feedback.\n3.1 Simulated Evaluation\nThe VIMABench is a simulation benchmark of multi-modal prompts for robotics [5]. It interleaves\ntextual and visual tokens. The benchmark contains thousands of procedural generated table top tasks\nwith multi-modal prompts with systematic evaluation protocols for generalisation.\nSeveral representative meta tasks were chosen from VIMABench, amounting to a total of three\ntasks. Spanning from simple object manipulation to visual reasoning, these tasks were selected to\nassess the proposed methods within the tabletop manipulation domain, as illustrated in Figure 4.\nThe evaluation encompasses the following tasks:\n1.Task 1 Scene Rearrangement : The robot is positioned within a tabletop environment containing\nobjects and receptors. A target scene image is supplied, prompting the system to reconfigure the\nenvironment to align with the specified target scene.", "start_char_idx": 4237, "end_char_idx": 6820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4": {"__data__": {"id_": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b293d45b-6ae1-4bbc-b549-50431cbbddcf", "node_type": "1", "metadata": {}, "hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "08fa381b-8c67-4078-b3ca-6d13b5c81bf5", "node_type": "1", "metadata": {}, "hash": "c28d50d68db0b6a5103bff6a3139c74bba43ba788f89d7cad7c231a2e22ca245", "class_name": "RelatedNodeInfo"}, {"node_id": "364c343f-141b-46ce-aae0-fb2eb01030c0", "node_type": "1", "metadata": {}, "hash": "feeeb8d26418ed9d6394bd2ac86c43f1f2e3b97bc98a0e05f5f79ab75c854512", "class_name": "RelatedNodeInfo"}, {"node_id": "c9b8f352-4f3f-441a-addd-f009db814b6a", "node_type": "1", "metadata": {}, "hash": "b1810a20499ef513027b8a2a8ab4183faaa074b2c5fa4c673d7c80bb25dc8b37", "class_name": "RelatedNodeInfo"}, {"node_id": "dfe44657-e45d-4d05-850e-65108df04c44", "node_type": "1", "metadata": {}, "hash": "353b41a426b7cf03b3d993da237f27c01f92cc479cf855becfc25666dc9ed5d9", "class_name": "RelatedNodeInfo"}, {"node_id": "5dc9fdbf-24a7-4bd6-9bc9-1e2121ffeccd", "node_type": "1", "metadata": {}, "hash": "1d2f644acc70be51507da3b8d99113407e8954150b915c0b22175388c871f516", "class_name": "RelatedNodeInfo"}]}, "text": "2.Task 2 Visual Manipulation : The robot is situated within a tabletop environment, furnished\nwith objects and receptors, and directed to manipulate the objects to the receptors in a specified\nsequence.\n8\n\n3.Task 3 Object Rotation : The robot is situated in a tabletop environment, featuring an object,\nand is subjected to a prompt specifying a distinct target angular rotation for the object.\nFigure 4 : The simulated tasks used for evaluation are Task 1: Rearrange Scene, Task 2: Visual\nManipulation, Task 3: Rotate. The image shows an example scene setup and instruction for each task.\nVIMABench categorizes tasks into partitions, including combinatorial generalization ,place-\nment generalization , and novel object generalization . The benchmark offers two prompt\nconfiguration modes: single modality, involving textual information only, and multi-modality, incor-\nporating both texts and images. Experiments are conducted 10 times for each partition, modality,\nand task, resulting in a total of 60 runs per task.\nIn the simulated environment, the exclusive utilisation of the SAM-CLIP vision system is exam-\nined. The system is evaluated under two conditions: one without feedback to the LLM post-plan\ngeneration, and another allowing users to provide feedback, which is then incorporated into the\nmessage stream, serving as contextual input for subsequent plan generation by the LLM.\n3.2 Physical Evaluation\nIn the physical experiment, a robotic system is utilized. The experimental setup closely corresponds\nto the simulated environment, as depicted in Figure 5, incorporating a top-view camera for vision\nskills and employing analogous objects and receptors.\nRobot Arm . The Niryo Ned robot is a collaborative robot arm which can be used in both\nresearch and industrial applications [50], this is used as the robot arm for the experiment.The robot\nconnects to a shared network with an Apple M1 MacBook serving as the controller device, equipped\nwith up to 10 CPU cores, a high-performance GPU boasting 16 cores, a Neural Engine, and a Unified\nMemory Architecture with 16GB of RAM, facilitating instruction transmission to the robot arm over\nthe network.\nImage and Depth Camera A testbed setup is established for the experiment, incorporating\nthe installation of the Intel RealSense D435i image and depth camera.", "start_char_idx": 6821, "end_char_idx": 9141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b293d45b-6ae1-4bbc-b549-50431cbbddcf": {"__data__": {"id_": "b293d45b-6ae1-4bbc-b549-50431cbbddcf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7c1c087-905a-41a0-88d3-92cf9a603438", "node_type": "1", "metadata": {}, "hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7a8b215a-9e7a-4085-9a9f-9ca9650f5bde", "node_type": "1", "metadata": {}, "hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "class_name": "RelatedNodeInfo"}]}, "text": "The robot is positioned in\na stable configuration beneath the camera setup to ensure accuracy in pixel-to-coordinate value\ntransformations, as depicted in Figure 5 [51].\nFor the physical evaluation, two tabletop manipulation tasks closely resembling the visual manipu-\nlation task outlined in VIMABench are selected, as depicted in Figure 6. The evaluation encompasses\nthe following tasks:\n1.Task A Pick and Place Single Object : In this task, the robot is positioned within a scene featuring\na solitary Lego block positioned between two receptors\u2014a green box and a blue box.", "start_char_idx": 9142, "end_char_idx": 9717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00821df1-46e5-4f24-9733-627c63fa7193": {"__data__": {"id_": "00821df1-46e5-4f24-9733-627c63fa7193", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "569fcf26-224b-459a-9c7a-28d69b951b76", "node_type": "1", "metadata": {}, "hash": "7e87d9593d814155c7d48f91f85a9bbb2cbba63fedf84a142e0360962dede27d", "class_name": "RelatedNodeInfo"}, {"node_id": "6d1dc7fb-4980-4e57-9586-804dba9f54f0", "node_type": "1", "metadata": {}, "hash": "4b678d8932d9be538acf4c3613dad8bc821424ab755fbe1b390d0057e9fe55e7", "class_name": "RelatedNodeInfo"}, {"node_id": "2808af67-c8e9-418e-90f5-e2906405b7f2", "node_type": "1", "metadata": {}, "hash": "6431444ec23e4c4b7d1520afe373af05ba2aeb92af886656d63c8351fe26c70b", "class_name": "RelatedNodeInfo"}, {"node_id": "299d1b08-d852-4fe2-9474-826673130b7a", "node_type": "1", "metadata": {}, "hash": "13c85c0f682d630555b16d79c23b531d82b8b99006700cbcc636ea1fa81a432e", "class_name": "RelatedNodeInfo"}, {"node_id": "91a84ae8-5bd9-40b0-96f7-ec19ecba5e7f", "node_type": "1", "metadata": {}, "hash": "b430f2a9c249f8dc3c94d777b6dd8230f8eb62d536c24635ab6b5a5edad3bbd2", "class_name": "RelatedNodeInfo"}]}, "text": "The system\nis directed to position the block in a designated receptor, for example, \u2019 place the blue block in the\ngreen bowl \u2019.\n2.Task B Pick and Place Multiple Objects :In this task, the robot is situated within a scene compris-\ning multiple Lego blocks and two receptors\u2014a green box and a blue box. The system is directed to\narrange multiple items into distinct receptors in a predetermined order, for instance, placing the\nred block in the blue bowl, followed by positioning the green block in the correspondingly colored\nbowl. i.e \u2019 put the red block in the blue bowl, then the green block in the matching colored bowl \u2019.\n9\n\nOur system is evaluated on 3 variations: plan generation with feedback, plan generation without\nfeedback and plan generation without scene description.\nFigure 5 : The experimental setup comprises a Niryo Ned robot arm, an Intel RealSense D435i\ndepth camera, and colored bowls (green and blue) serving as receptors, along with a collection of\nmulticolored Lego blocks.\nThe system\u2019s performance is assessed independently using SAM-CLiP and ViLD vision systems.\nEach task within every system variation is executed 10 times for each vision model, resulting in a\ntotal of 40 tests for each system configuration.\nThe quantitative metric employed for gauging the performance of the various approaches is the\ntask success rate.\nFigure 6 : The simulated environment features a top view camera which is used for detecting object\nsin the scene. The physical environment is configured to closely emulate the settings utilized in the\nsimulated evaluation, employing analogous objects and configurations.\n4 Results\nTable 2 demonstrates the system\u2019s performance in executing scene rearrangement tasks prompted\nthrough single and multi-modal configurations. Notably, the system exhibits an inability to success-\nfully execute any scene rearrangement tasks when prompted with a single modality. This limitation\narises from the absence of contextual information, specifically the target scene image, in single-modal\nprompts.\nAs highlighted in Table 2, the introduction of feedback provides a 18.5% increase in multi modality\ntasks, and a 2.5% performance drop over the baseline on single modality tasks. This improvement is\nattributed to the heightened complexity in generating plans for multi-modal prompts.\nIn the context of the visual manipulation task (Task 2), the base implementation surpasses the\nfeedback system.", "start_char_idx": 0, "end_char_idx": 2431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b7fa9d2-6028-4406-b856-ccb1daad3a58": {"__data__": {"id_": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "97a1dcc6-a59a-4661-8ffc-9b545bf9dd76", "node_type": "1", "metadata": {}, "hash": "9e0ac55c08a531f2659f3b5c401a2c1ab6927fc3a15cfc88eb91ab0a3f08b98d", "class_name": "RelatedNodeInfo"}, {"node_id": "f71aadfd-9c6b-45bd-a77b-94f1df020654", "node_type": "1", "metadata": {}, "hash": "e0d4061533952de6eddedc02c9fe4b3218fb1c3d8c702762fc97373d4ccfcc5b", "class_name": "RelatedNodeInfo"}, {"node_id": "0d78b9fc-b73f-438e-87b2-a96cc536ae59", "node_type": "1", "metadata": {}, "hash": "4c319aa84e073dbc4c6849db4ca8ec39e60134a9f9cf9cb32f53f1e7871057c7", "class_name": "RelatedNodeInfo"}, {"node_id": "327e5dac-8077-4076-bb6c-a583b893c8a3", "node_type": "1", "metadata": {}, "hash": "cb7afd3cdaf26a7eb75fcad8b4d6b54fdc43a20e0d572a0d3829fa0ad2ca9a1a", "class_name": "RelatedNodeInfo"}, {"node_id": "dfb015cc-9ce0-4787-b95f-c36bc0299591", "node_type": "1", "metadata": {}, "hash": "94175c9d7b2ee891b0a49c0da989de3fc6c4fc46704d187710aadd2728bd1188", "class_name": "RelatedNodeInfo"}]}, "text": "This observation raises the suspicion that discrepancies in task execution accuracy,\nrather than plan generation, may underlie the system\u2019s comparative performance. Negative results are\nobserved primarily in tasks prompted with a single modality, aligning with the inherent limitations\nassociated with insufficient contextual information.\nObservations during experiments reveal consistent plan generation errors by the model for specific\ninstructions, and even after feedback is provided for a particular mistake, the model tends to repeat\n10\n\nTable 2 : VIMABench simulated environment evaluation\nSingle modality1Multi modality2\nProject Task 1 Task 2 Task 3 Task 1 Task 2 Task 3\nInstruct2Act 0.0% 70% 90% 40% 63.3% 76.66%\nInstruct2Act w/Feedback 0.0% 66% 90% 46.66% 73.33% 93.3%\nNote: The success rate is calculated by finding the average success of task completion for each task across the 3\npartitions.\n1For single modality only textual data is provided to the context of the model.\n2For multi modality both texts and images are provided as contexts to the model.\nthese errors once the message stream is cleared. This recurrence is attributed to the absence of a\nmemory system that would retain feedback and context across experiments.\nThe language model demonstrates generalisation to new objects and employs novel approaches in\ncalling the APIs beyond the provided examples. Although infrequent, the model may respond with\nnatural language outputs outside the defined API, and providing feedback instructions corrects such\noccurrences.\nFigure 7 : The human user provides the instruction \u2019 place the green block in the matching colored\nbowl\u2019. The LLM uses this instruction to generate a plan which is then executed by the physical robot\nTable 3 presents the outcomes of the physical evaluation conducted on the system. The findings\nindicate that the incorporation of scene descriptions yields an 8.5% reduction to system success rate\nwhile adding an average of 7 seconds to the overall system execution time. For these reasons the\nscene description module may be considered dispensable in future iterations.\nFurthermore, discernible variations in success rates among zero-shot object detection models\nare observed. The ViLD model excels in accurately locating specified items within the scene but\nencounters challenges in distinguishing color differences between objects, as illustrated in Figure 9.", "start_char_idx": 2432, "end_char_idx": 4834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "938608bd-cfa6-4c4a-a770-fa7ac060dc4c": {"__data__": {"id_": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "44c18575-34fe-4375-9c77-1ef9114c4c7b", "node_type": "1", "metadata": {}, "hash": "ce7c00aa9f6fa4986f628d9164cd7d1dca495272067ad3f72c40696b07edd459", "class_name": "RelatedNodeInfo"}, {"node_id": "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f", "node_type": "1", "metadata": {}, "hash": "bcb1d7eb7f29940126ede54e55cc7cdbc87bd4e1e3123b988201dcd5519d4476", "class_name": "RelatedNodeInfo"}, {"node_id": "03f0766f-3c01-439f-9584-20eb51f4c2d8", "node_type": "1", "metadata": {}, "hash": "0b9dcaa8ae594c5dbd668b55433c58ceae46a5b2cf0adbace3250252fbd4e28a", "class_name": "RelatedNodeInfo"}, {"node_id": "c15d4371-261d-44c7-91d9-b2f2cff903f1", "node_type": "1", "metadata": {}, "hash": "9a486ed878a6ba424e35b3f0874b45e56e14635cfd8f81106354241826d2fd4f", "class_name": "RelatedNodeInfo"}, {"node_id": "98760a13-7772-4e2d-937c-4aa36a6342d9", "node_type": "1", "metadata": {}, "hash": "f54816904dcd3562a9dd50465272063930fc5fa2bd8e3fdd4a2fe55d8f0ede55", "class_name": "RelatedNodeInfo"}]}, "text": "Consequently, the model demonstrates inaccuracies, notably in predicting instances where multi-\nple objects of the same type but in different colors coexist, sometimes failing entirely to identify\nany instance of the object, as exemplified in query 2 of Figure 9. In contrast, Figure 10 illustrates\nthat the SAM-CLiP approach exhibits superior color identification capabilities. However, it is asso-\nciated with a significantly higher latency; during our experimental evaluations, the ViLD approach\ntook an average of 15 seconds for inference, while the SAM-CLiP approach took an average of 144\nseconds. Although the SAM-CLiP approach achieves heightened accuracy in detecting objects with\ndiverse colors, it occasionally groups disparate objects together if they share the same color\u2014such\nas categorizing a square green container as a green block, as demonstrated in query 1 in Figure 10.\nObservations indicate that zero-shot object detection models exhibit superior performance on\nsimulation images within the benchmark environment compared to real-world images tested on an\nactual robot setup. The accuracy of the detection models does not achieve commensurate levels when\n11\n\nFigure 8 : The human user has provided a prompt to place the green block in the blue bowl . The\nLLM uses the instruction provided and the affordance skills available to generate the plan which is\nthen executed by the robot\nFigure 9 : Zero-shot object recognition employing ViLD is presented in the diagram. The upper\nimage depicts the original scene, and the lower image exhibits a green bounding box drawn around\nthe identified target object. The diagram illustrates the algorithm\u2019s inability to identify query 2\nandquery 3 , accompanied by an erroneous prediction for query 4 .\nFigure 10 : Zero-shot object recognition using SAM-CLiP is depicted in the diagram. The upper\nimage represents the original scene, while the lower image displays a green bounding box drawn\naround the identified target object. The illustration demonstrates the model\u2019s accurate recognition\nofquery 2 ,query 3 andquery 4 . However, it exhibits a failure in identifying the object in query\n1, as it misidentifies the green box as the green block.", "start_char_idx": 4835, "end_char_idx": 7038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa0483c5-b5ed-490a-b7d3-c6972582bc61": {"__data__": {"id_": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf72c77f-d210-49e0-9fbc-868444937960", "node_type": "1", "metadata": {}, "hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "eb04e912-9cbc-4e1c-86ac-d85de4ccff08", "node_type": "1", "metadata": {}, "hash": "d1312d5238dc41d2cbccad134c6baa007e6662a8fe4daf00fb72cb9878f183d4", "class_name": "RelatedNodeInfo"}, {"node_id": "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954", "node_type": "1", "metadata": {}, "hash": "86cbff7ce78b43a24c4db9b3f7375685af462134cfe22ec2e608d287f08fb4a9", "class_name": "RelatedNodeInfo"}, {"node_id": "04651f94-3b5b-4c58-b540-df3a6f9bc4d1", "node_type": "1", "metadata": {}, "hash": "b9b79ab6674569f0567521af59d5a561b06c0671f5b552d130147cca7d87429f", "class_name": "RelatedNodeInfo"}, {"node_id": "3a2c72d9-1e14-4918-bfa0-042c67e7dfee", "node_type": "1", "metadata": {}, "hash": "7233a467228b714f73d823314517c2ca65cfcaa6864be66d41ab6c1349f546a4", "class_name": "RelatedNodeInfo"}]}, "text": "12\n\nTable 3 : Physical robot environment evaluation\nViLD1SAM-CLiP2\nMethodology Task A Task B Task A Task B\nw/Feedback 60% 40% 60% 80%\nw/o Feedback 20% 40% 70% 80%\nw/o Scene Description 30% 40% 80% 70%\nNote: Each task is executed 10 times on the physical robot within the environment setup.\n1For the ViLD system the location of the receptors were hard-coded because the model was having difficulty identifying\nthe receptors in the scene.\n2The SAM-CLiP model used in the physical robot is the exact same as used in the VIMABench simulated environment.\nconfronted with real-world embodied images. This suggests a potential barrier to the practical appli-\ncation of this approach in real-world scenarios, necessitating the development of zero-shot learning\ndetection models tailored for real-world embodied images.\n5 Discussion\nThe presented research introduces a methodology to facilitate collaborative robot planning through\nhuman-robot interaction, harnessing LLMs. Subsequent evaluations were conducted in a simulated\nrobotics benchmark and on a physical robot arm setup. The ensuing discussion outlines significant\nfindings and insights gleaned from the experiments.\nThe proficient completion of object manipulation tasks by the robot arm, guided by human\nusers, underscores the efficacy of the approach. Leveraging language understanding and generation\ncomponents based on LLMs facilitated natural language communication, empowering users to issue\nhigh-level commands, deliver feedback, and optimize task plans dynamically during execution. This\nadaptability stands out as a pivotal strength of the methodology, indicative of the potential for\nheightened user agency in the realm of human-robot collaboration.\nThe system demonstrated its capacity to understand and execute a variety of user instructions,\nsuch as placing and relocating objects. While these set of manipulations are limited this system can\nbe extended to support more complex skills that can be composed together to achieve a larger set\nof tasks.\nLeveraging the VIMABench environment for bench-marking provided valuable insights into the\nsystem\u2019s performance, enabling an evaluation of its proficiency in comprehending and responding to\na diverse range of language instructions. The environment wrapper facilitated real-time feedback and\nadjustment, aligning the approach with principles of adaptability and continuous improvement.", "start_char_idx": 7039, "end_char_idx": 9438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf72c77f-d210-49e0-9fbc-868444937960": {"__data__": {"id_": "cf72c77f-d210-49e0-9fbc-868444937960", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16196373-14c0-4390-a0db-8ac5b663447f", "node_type": "1", "metadata": {}, "hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "194e47d5-07a0-403c-be14-d68931a214fd", "node_type": "1", "metadata": {}, "hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "class_name": "RelatedNodeInfo"}]}, "text": "In quantitative metrics, the system demonstrated 8.6% performance increase in simulated eval-\nuations and a 14% increase in physical evaluations. This shows that the method can be further\nexplored to enable collaborative task planning between humans and robots in physical and virtual\nenvironments.\nHowever, there are areas for improvement. As with many language models, the system\u2019s under-\nstanding and generation capabilities may occasionally exhibit limitations.", "start_char_idx": 9439, "end_char_idx": 9904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcd36521-18d4-4ede-889d-47fb5fc3bf31": {"__data__": {"id_": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "865e3761-95fb-4411-91d9-ac4d0dfcd283", "node_type": "1", "metadata": {}, "hash": "249e92979f223a88197eb309e23eba1fd65a0a63e8300f295dfbe1cde2df86ea", "class_name": "RelatedNodeInfo"}, {"node_id": "35ef8e42-ebd3-4e50-a73a-7713e2e392bf", "node_type": "1", "metadata": {}, "hash": "6e827ed0b282c86f15af3feab69830d6a9dd1c0dbc1f34fd66835e5cdc2f9590", "class_name": "RelatedNodeInfo"}, {"node_id": "9ed73b61-29d1-4d79-a72f-31b0ba4720db", "node_type": "1", "metadata": {}, "hash": "0ae7eedeb692c45a07c9cde61cee9d91230660fb218cb8eaa3ee913ade2cb584", "class_name": "RelatedNodeInfo"}, {"node_id": "93be767d-58e9-4eb5-b5a2-ad7da310f65a", "node_type": "1", "metadata": {}, "hash": "9764ff814aa500b14c71356f01880c6f2af4a672e1eba723a4507abd8d97164c", "class_name": "RelatedNodeInfo"}, {"node_id": "47d6cb48-779a-494b-975f-025463507e59", "node_type": "1", "metadata": {}, "hash": "210a83a2134c257f1ce783ed19e8e3eac631dfd168c568a856212687b72e360b", "class_name": "RelatedNodeInfo"}]}, "text": "There is a need to address\nambiguities in user instructions and improve the system\u2019s capacity to ask clarifying questions when\nencountering unclear or contradictory language input.\nAdditionally, the study emphasizes the need for computational optimization, aligning with prior\nresearch on the computational constraints of language models, to enhance efficiency for deployment\nin real robotic systems.\n6 Conclusion\nThis research presents a system utilizing LLMs to facilitate collaborative robot planning through\nhuman-robot interaction. The amalgamation of language models with robot planning and execution\nintroduces compelling opportunities and challenges, paving the way for the evolution of intelligent\nand interactive robots. Previous studies have demonstrated LLMs\u2019 potential as generalist planners\nfor robot task execution in natural language. This research aims to contribute by transforming LLM-\ngenerated plans into an interactive process, fostering continuous interaction between the user and\nthe robot through the LLM.\n13\n\nProspective avenues for the project involve exploring the involvement of other AI language mod-\nels in evaluating plans generated by the LLM and providing iterative feedback until successful task\ncompletion. Alternatively, implementing a comprehensive memory system for the Robot LLM, akin\nto Generative Agents\u2019 [56] work, could enhance the LLM\u2019s ability to actively learn from these\ninteractions.\nReferences\n[1] Huang, S., Jiang, Z., Dong, H., Qiao, Y., Gao, P. and Li, H. (2023). Instruct2Act: Mapping Multi-\nmodality Instructions to Robotic Actions with Large Language Model. arXiv (Cornell University).\ndoi:https://doi.org/10.48550/arxiv.2305.11176.\n[2] Wu, J., Antonova, R., Kan, A., Lepert, M., Zeng, A., Song, S., Bohg, J., Rusinkiewicz, S. and\nFunkhouser, T., 2023. Tidybot: Personalized robot assistance with large language models. arXiv\npreprint arXiv:2305.05658.\n[3] A. I. K\u00b4 aroly, P. Galambos, J. Kuti and I. J. Rudas, \u201dDeep Learning in Robotics: Survey on Model\nStructures and Training Strategies,\u201d in IEEE Transactions on Systems, Man, and Cybernetics:\nSystems, vol. 51, no.", "start_char_idx": 0, "end_char_idx": 2124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88403054-34c4-4fdf-b50c-5918c1f3f7e1": {"__data__": {"id_": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "030bf9d4-f9b9-4807-a491-0d4610cefc62", "node_type": "1", "metadata": {}, "hash": "5d1bc46103b700e7ccf868c592a49bc69875078abd29799b2238815e343fe9c4", "class_name": "RelatedNodeInfo"}, {"node_id": "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760", "node_type": "1", "metadata": {}, "hash": "c00d28ffe836ebd6e04fce57a6a04c55006e7fa9ddb04017304f1a79018364c4", "class_name": "RelatedNodeInfo"}, {"node_id": "372512df-6ff4-4419-99e5-ad456f68de6b", "node_type": "1", "metadata": {}, "hash": "1aa6f5d6cac54932996d81e12eb65a20de89b2c694df5d192ad4b54ae2d36a7a", "class_name": "RelatedNodeInfo"}, {"node_id": "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977", "node_type": "1", "metadata": {}, "hash": "4e5528fee7d90b6129d8e3fcdd26cae5576a02fd5d78722bf5631e35307bec2c", "class_name": "RelatedNodeInfo"}, {"node_id": "646b8540-05fd-4678-aff4-24dbf948a091", "node_type": "1", "metadata": {}, "hash": "e98b70145d33dbe10dee7d7de26e8eb394f414e3e4805701777d28882882451f", "class_name": "RelatedNodeInfo"}, {"node_id": "8ffe4f91-6837-4828-b359-5f577da1ff25", "node_type": "1", "metadata": {}, "hash": "a83d3cab9bb95b9b881b6c56908f1d1499c1cbecf192617b2b8bb43200034206", "class_name": "RelatedNodeInfo"}]}, "text": "51, no. 1, pp. 266-279, Jan. 2021, doi: 10.1109/TSMC.2020.3018325.\n[4] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson,\nJ., Vuong, Q., Yu, T. and Huang, W., 2023. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378.\n[5] Kovalev, A.K., Panov, A.I. Application of Pretrained Large Language Models\nin Embodied Artificial Intelligence. Dokl. Math. 106 (Suppl 1), S85\u2013S90 (2022).\nhttps://doi.org/10.1134/S1064562422060138\n[6] Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J. and\nGarg, A., 2023, May. Progprompt: Generating situated robot task plans using large language\nmodels. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 11523-\n11530). IEEE.\n[7] Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P. and Zeng, A.,\n2023, May. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) (pp. 9493-9500). IEEE.\n[8] Huang, W., Abbeel, P., Pathak, D. and Mordatch, I., 2022, June. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning (pp. 9118-9147). PMLR.\n[9] Zhao, X., Li, M., Weber, C., Hafez, M.B. and Wermter, S., 2023. Chat with the environment:\nInteractive multimodal perception using large language models.", "start_char_idx": 2117, "end_char_idx": 3557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7643626-84dd-4b5c-9902-528a44aea517": {"__data__": {"id_": "e7643626-84dd-4b5c-9902-528a44aea517", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f4506df3-328d-4f3b-9c4e-abfe21c4b0e3", "node_type": "1", "metadata": {}, "hash": "8d24655c3c289158f664a09c2c8ffd76d1cd10e78accf706b5b9d49283566200", "class_name": "RelatedNodeInfo"}, {"node_id": "77a457c2-7d13-4288-abbc-dcf01a46b622", "node_type": "1", "metadata": {}, "hash": "5e120b1f46ebc87188435070c95b0b660fa1395e8a0d26d456b0522955bd7cfe", "class_name": "RelatedNodeInfo"}, {"node_id": "d14b14ed-9d86-4380-afb6-492ca0707b9e", "node_type": "1", "metadata": {}, "hash": "2b09bca843911747300ed1c764bf8d0abcf7e9583f0e82bbe10fe6f99c0fa1ac", "class_name": "RelatedNodeInfo"}, {"node_id": "762eeb22-f099-43ce-97a4-4489a833421f", "node_type": "1", "metadata": {}, "hash": "dc5ca7518521c3832b86bcc99698c947c581f4a3028333bc9f0d6d883fe76698", "class_name": "RelatedNodeInfo"}, {"node_id": "83c13dfa-f71b-4b13-948a-39bb24be72c7", "node_type": "1", "metadata": {}, "hash": "b16b63256fa2713297a89e4bdfac015ffc59305a2d1d2be13339825b5432df4d", "class_name": "RelatedNodeInfo"}]}, "text": "Chat with the environment:\nInteractive multimodal perception using large language models. arXiv preprint arXiv:2303.08268.\n[10] Zhang, B. and Soh, H., 2023. Large language models as zero-shot human models for human-robot\ninteraction. arXiv preprint arXiv:2303.03548.\n[11] Xie, Y., Yu, C., Zhu, T., Bai, J., Gong, Z. and Soh, H., 2023. Translating natural language to\nplanning goals with large-language models. arXiv preprint arXiv:2302.05128.\n[12] Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J. and Stone, P., 2023. Llm+ p: Empow-\nering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.\n[13] Wake, N., Kanehara, A., Sasabuchi, K., Takamatsu, J. and Ikeuchi, K., 2023. Chatgpt\nempowered long-step robot control in various environments: A case application. arXiv preprint\narXiv:2304.03893\n[14] Ye, F., Zhang, S., Wang, P. and Chan, C.-Y. (2021). A Survey of Deep Reinforce-\nment Learning Algorithms for Motion Planning and Control of Autonomous Vehicles.\ndoi:https://doi.org/10.1109/iv48863.2021.9575880.\n14\n\n[15] Hadi, Muhammad Usman; tashi, qasem al; Qureshi, Rizwan; Shah, Abbas; muneer,\namgad; Irfan, Muhammad; et al. (2023). Large Language Models: A Comprehensive Sur-\nvey of its Applications, Challenges, Limitations, and Future Prospects. TechRxiv. Preprint.\nhttps://doi.org/10.36227/techrxiv.23589741.v3\n[16] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. and Cao, Y., 2022.", "start_char_idx": 3468, "end_char_idx": 4919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "013731c3-85e2-48ed-9aff-b7ec67c9c047": {"__data__": {"id_": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "535af1ae-4b4b-4082-9a2b-ecb9c394b99e", "node_type": "1", "metadata": {}, "hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "96132b0f-5d0c-4595-846a-dfaa5aa4d96c", "node_type": "1", "metadata": {}, "hash": "0d1fba54a9b9ef2cdcb85a879c0ae852119619ce73615052558363bb156722b7", "class_name": "RelatedNodeInfo"}, {"node_id": "099d8859-ce61-4540-ab3f-9498a5174a42", "node_type": "1", "metadata": {}, "hash": "3ff518db952502edde0416aee8616833cfa722f454b3cc8b4263372b1eddcc7a", "class_name": "RelatedNodeInfo"}, {"node_id": "ce79aab2-672c-4686-9725-b0a6eb169b08", "node_type": "1", "metadata": {}, "hash": "d5ffcb4343736012565e71ba7c6c0a63eb17c35a7dcaa8285cdd9dd2bcce0f6d", "class_name": "RelatedNodeInfo"}, {"node_id": "88de296f-fdfe-4280-a61d-295f376a5dfa", "node_type": "1", "metadata": {}, "hash": "ee6d58168312ef6393bac703e4c688ebf6eff5af80b5c5ffd731465e8c9346ec", "class_name": "RelatedNodeInfo"}, {"node_id": "56bf4afc-039f-40c3-8a55-23add43f8a96", "node_type": "1", "metadata": {}, "hash": "1920f198d01d71637da2af45d2633e07ed5a24d432f04e2970498bf4954aaeff", "class_name": "RelatedNodeInfo"}]}, "text": "React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\n[17] Lykov, A. and Tsetserukou, D., 2023. LLM-BRAIn: AI-driven Fast Generation of Robot\nBehaviour Tree based on Large Language Model. arXiv preprint arXiv:2305.19352.\n[18] Ding, Y., Zhang, X., Amiri, S., Cao, N., Yang, H., Esselink, C. and Zhang, S., 2022. Robot task\nplanning and situation handling in open worlds. arXiv preprint arXiv:2210.01287.\n[19] Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.H., Arenas, M.G., Chiang, H.T.L., Erez,\nT., Hasenclever, L., Humplik, J. and Ichter, B., 2023. Language to Rewards for Robotic Skill\nSynthesis. arXiv preprint arXiv:2306.08647.\n[20] Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A.,\nZhu, Y. and Fan, L., 2022. Vima: General robot manipulation with multimodal prompts. arXiv\npreprint arXiv:2210.03094.\n[21] Ding, Y., Zhang, X., Paxton, C. and Zhang, S., 2023. Task and motion planning with large\nlanguage models for object rearrangement. arXiv preprint arXiv:2303.06247.\n[22] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929.", "start_char_idx": 4920, "end_char_idx": 6253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "535af1ae-4b4b-4082-9a2b-ecb9c394b99e": {"__data__": {"id_": "535af1ae-4b4b-4082-9a2b-ecb9c394b99e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf090498-501a-4e10-b6fb-411592ea0e5a", "node_type": "1", "metadata": {}, "hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9eea138e-4fc5-42bf-a900-cdd43e150767", "node_type": "1", "metadata": {}, "hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "class_name": "RelatedNodeInfo"}]}, "text": "arXiv preprint arXiv:2010.11929.\n[23] Dahiya, Abhinav, Alexander M. Aroyo, Kerstin Dautenhahn, and Stephen L. Smith.", "start_char_idx": 6221, "end_char_idx": 6337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e6766d-037f-44f7-b12e-ef6e22966bfc": {"__data__": {"id_": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8e8db431-5af6-4297-90bd-cb0af84599ce", "node_type": "1", "metadata": {}, "hash": "210e449fb73d7fa079fff6e98a296274ac20848b1310156fac3b433ccd0fc1db", "class_name": "RelatedNodeInfo"}, {"node_id": "40f356cb-9551-4638-ace0-da392e7add13", "node_type": "1", "metadata": {}, "hash": "63c3a5a9c49467cbb8e3e78e2d8ae02f47e2fe0ff8bb6845e31a15871f307292", "class_name": "RelatedNodeInfo"}, {"node_id": "6eff7521-1348-4f47-b7b6-1c821b83f958", "node_type": "1", "metadata": {}, "hash": "f006a46552b1347492357f5f52717aba3d77a541a7c75eb13d1ba56d77be7d3d", "class_name": "RelatedNodeInfo"}, {"node_id": "1a1dbca4-bbe4-45ad-bc9c-9698765ed087", "node_type": "1", "metadata": {}, "hash": "b1725dd95cfe0f8baa6b86b0cfed26a1ec8416a84f1f920c68e85d0815667e62", "class_name": "RelatedNodeInfo"}, {"node_id": "cf283e40-2254-4948-9e2a-828766fc97db", "node_type": "1", "metadata": {}, "hash": "7bc9b9f1e7a4ad2c5ce1162899103974b4c1d556195b0faa3ee33946edce9e4e", "class_name": "RelatedNodeInfo"}]}, "text": "\u201dA survey of\nmulti-agent Human\u2013Robot Interaction systems.\u201d Robotics and Autonomous Systems 161 (2023):\n104335.\n[24] Sharkawy, A.N., 2021. Human-robot interaction: Applications. arXiv preprint arXiv:2102.00928.\n[25] Billard, A., Robins, B., Nadel, J. and Dautenhahn, K., 2007. Building robota, a mini-humanoid\nrobot for the rehabilitation of children with autism. Assistive Technology, 19(1), pp.37-49.\n[26] Clabaugh, C., Tsiakas, K. and Mataric, M., 2017, September. Predicting preschool mathematics\nperformance of children with a socially assistive robot tutor. In Proceedings of the Synergies\nbetween Learning and Interaction Workshop@ IROS, Vancouver, BC, Canada (pp. 24-28).\n[27] Knudsen, M. and Kaivo-Oja, J., 2020. Collaborative robots: Frontiers of current literature.\nJournal of Intelligent Systems: Theory and Applications, 3(2), pp.13-20.\n[28] Vicentini, F., 2021. Collaborative robotics: a survey. Journal of Mechanical Design, 143(4),\np.040802.\n[29] Javaid, M., Haleem, A., Singh, R.P., Rab, S. and Suman, R., 2022. Significant applications of\nCobots in the field of manufacturing. Cognitive Robotics, 2, pp.222-233.\n[30] Huang, C., Mees, O., Zeng, A. and Burgard, W., 2023, May. Visual language maps for robot\nnavigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA)(pp.\n10608-10615). IEEE.\n[31] Bergerman, M., Maeta, S.M., Zhang, J., Freitas, G.M., Hamner, B., Singh,\nS. and Kantor, G. (2015). Robot Farmers: Autonomous Orchard Vehicles Help\nTree Fruit Production. IEEE Robotics & Automation Magazine, 22(1), pp.54\u201363.", "start_char_idx": 0, "end_char_idx": 1559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b70d562-31c4-4c4f-9f8b-c14d239ea067": {"__data__": {"id_": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "459eccb1-3c03-4448-996f-fe0ba067dcbe", "node_type": "1", "metadata": {}, "hash": "ec4b7ace06fd0da472959fad6fc51f964d5f9fe12244da33f1f3b91ca77e63c4", "class_name": "RelatedNodeInfo"}, {"node_id": "cf391a4a-f8a5-414f-8052-a04fc17a5661", "node_type": "1", "metadata": {}, "hash": "b78f6ca49abe05da07584314db3b86d24d8de540e8fc66eec55fe74e1d8e8366", "class_name": "RelatedNodeInfo"}, {"node_id": "5a4f3de1-9319-414a-9290-a52ca78149cb", "node_type": "1", "metadata": {}, "hash": "f87eec113670817d6be03361dc961bdf33b6483af3667311d611389f8d9dd6cd", "class_name": "RelatedNodeInfo"}, {"node_id": "2fb55683-a046-4f97-8148-64028440f7f5", "node_type": "1", "metadata": {}, "hash": "bb2193dd3ed3261d6d10371bf70db1e5912286b439bfe9f4182d09a23defcf69", "class_name": "RelatedNodeInfo"}, {"node_id": "9fbe7842-cbe3-4e3f-b84c-fbed358d25b8", "node_type": "1", "metadata": {}, "hash": "389babc3bf60ecff5f785e09b0d9f380ed675ff1f2f1307c5eb895c394e803a7", "class_name": "RelatedNodeInfo"}]}, "text": "IEEE Robotics & Automation Magazine, 22(1), pp.54\u201363.\ndoi:https://doi.org/10.1109/mra.2014.2369292.\n15\n\n[32] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,  L. and\nPolosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems,\n30.\n[33] Lin, T., Wang, Y., Liu, X. and Qiu, X., 2022. A survey of transformers. AI Open.\n[34] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33, pp.1877-1901.\n[35] Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R. and McHardy, R., 2023. Challenges\nand applications of large language models. arXiv preprint arXiv:2307.10169.\n[36] Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam,\nM., Chaplot, D.S., Maksymets, O. and Gokaslan, A., 2021. Habitat 2.0: Training home assistants\nto rearrange their habitat. Advances in Neural Information Processing Systems, 34, pp.251-266.\n[37] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess,\nD., Dubey, A., Finn, C. and Florence, P., 2023. Rt-2: Vision-language-action models transfer web\nknowledge to robotic control. arXiv preprint arXiv:2307.15818.", "start_char_idx": 1506, "end_char_idx": 2885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47cda882-478b-46e5-a57e-2c079162b9bc": {"__data__": {"id_": "47cda882-478b-46e5-a57e-2c079162b9bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "50d2cb4e-539a-439e-8c3b-15d446c31e28", "node_type": "1", "metadata": {}, "hash": "b3fc4a1ee231ced40ee8a071ba235203ed24add774e1511f04377023ba519cc1", "class_name": "RelatedNodeInfo"}, {"node_id": "e099c449-ef0c-4987-942d-5ec5644ff227", "node_type": "1", "metadata": {}, "hash": "d1696c29466420958ddec1b4435573f7a712be07a83771973ce3ef7738c5afb4", "class_name": "RelatedNodeInfo"}, {"node_id": "388d4767-5b8d-46de-b7f4-4a2caf2522a2", "node_type": "1", "metadata": {}, "hash": "9d8655c2cb390d20dc316ea78766dc2b87623f7b3fd97ccaa94d7ae3896a5572", "class_name": "RelatedNodeInfo"}, {"node_id": "06502e61-6ba0-4ef4-a63c-68d5451b5616", "node_type": "1", "metadata": {}, "hash": "97ea51abc01b4199f80b2a8e1e9d55f3eea5e90c474f8594a919edfee3982040", "class_name": "RelatedNodeInfo"}, {"node_id": "b346816f-0a6e-466f-b893-f155bc546628", "node_type": "1", "metadata": {}, "hash": "9f4287785f0215816114c8b2c2ec0700b99f75295f09ac2d7d8ccca1ce4b7240", "class_name": "RelatedNodeInfo"}]}, "text": "arXiv preprint arXiv:2307.15818.\n[38] Tan, C., Xu, X. and Shen, F., 2021. A survey of zero shot detection: methods and applications.\nCognitive Robotics, 1, pp.159-167.\n[39] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S.,\nBerg, A.C., Lo, W.Y. and Doll\u00b4 ar, P., 2023. Segment anything. arXiv preprint arXiv:2304.02643.\n[40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,\nA., Mishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning (pp. 8748-8763).\nPMLR.\n[41] Gu, X., Lin, T.Y., Kuo, W. and Cui, Y., 2021. Open-vocabulary object detection via vision and\nlanguage knowledge distillation. arXiv preprint arXiv:2104.13921.\n[42] Zhu, P., Wang, H. and Saligrama, V., 2019. Zero shot detection. IEEE Transactions on Circuits\nand Systems for Video Technology, 30(4), pp.998-1010.\n[43] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakr-\nishnan, K., Hausman, K. and Herzog, A., 2022. Do as i can, not as i say: Grounding language in\nrobotic affordances. arXiv preprint arXiv:2204.01691.", "start_char_idx": 2853, "end_char_idx": 4082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bef3ce88-7a33-4020-8720-0336a725ec9d": {"__data__": {"id_": "bef3ce88-7a33-4020-8720-0336a725ec9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7", "node_type": "1", "metadata": {}, "hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e62f2851-38fd-44c0-b6bc-7eb4b8759ed3", "node_type": "1", "metadata": {}, "hash": "df6d8a8b6a568dd41f46abc88a09e47fcf4c43cd3e14c92c377ca8bf08b43adc", "class_name": "RelatedNodeInfo"}, {"node_id": "5660390a-65cd-4b17-a5eb-c4b9096f2ef7", "node_type": "1", "metadata": {}, "hash": "f317894c2cdc0c7569aca7fcf09efe0062b377a718dcb69f51f9ca3498e33913", "class_name": "RelatedNodeInfo"}, {"node_id": "0432442a-9644-4a91-ad45-7f1da1102faa", "node_type": "1", "metadata": {}, "hash": "c462c0f6c7c92c84256c8a3034916f1d902527910dacb8c15ce819537e208112", "class_name": "RelatedNodeInfo"}, {"node_id": "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9", "node_type": "1", "metadata": {}, "hash": "d311716123fef415e3973110269cfd4c893341810de7132a15bd2ff2a1c19987", "class_name": "RelatedNodeInfo"}, {"node_id": "a74b2f86-6ba6-4db3-a1d8-b14ef775ea9b", "node_type": "1", "metadata": {}, "hash": "e09758e7807d5f507011f6361fb9b4128426bdb1521f79f92cc9396ed88d00ed", "class_name": "RelatedNodeInfo"}]}, "text": "arXiv preprint arXiv:2204.01691.\n[44] Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker, S., Tombari, F., Purohit,\nA., Ryoo, M., Sindhwani, V. and Lee, J., 2022. Socratic models: Composing zero-shot multimodal\nreasoning with language. arXiv preprint arXiv:2204.00598.\n[45] Zhao, Z., Lee, W.S. and Hsu, D., 2023. Large Language Models as Commonsense Knowledge for\nLarge-Scale Task Planning. arXiv preprint arXiv:2305.14078.\n[46] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y. and Narasimhan, K., 2023.\nTree of thoughts: Deliberate problem solving with large language models. arXiv preprint\narXiv:2305.10601.\n[47] Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J.B., Shu, T. and Gan, C., 2023.\nBuilding cooperative embodied agents modularly with large language models. arXiv preprint\narXiv:2307.02485.\n[48] Chen, L., Zhang, Y., Ren, S., Zhao, H., Cai, Z., Wang, Y., Wang, P., Liu, T. and Chang,\nB., 2023. Towards end-to-end embodied decision making via multi-modal large language model:\nExplorations with gpt4-vision and beyond. arXiv preprint arXiv:2310.02071.\n16\n\n[49] Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y. and\nZhou, J., 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\n[50] Niryo. (n.d.).", "start_char_idx": 4050, "end_char_idx": 5403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7": {"__data__": {"id_": "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "node_type": "1", "metadata": {}, "hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b282e0c1-33b9-4da3-93ab-7f38fb16b3df", "node_type": "1", "metadata": {}, "hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "class_name": "RelatedNodeInfo"}]}, "text": "[50] Niryo. (n.d.). Ned2 Overview. [online] Available at: https://niryo.com/products-cobots/robot-\nned-2/.\n[51] Intel\u00ae RealSenseTM Depth and Tracking Cameras. (2019). Depth Camera\nD435i \u2013 Intel \u00aeRealSenseTM Depth and Tracking Cameras. [online] Available at:\nhttps://www.intelrealsense.com/depth-camera-d435i/.", "start_char_idx": 5384, "end_char_idx": 5693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b44ef94-6ea4-4c20-b24f-b48496a28def": {"__data__": {"id_": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d208a291-bfa1-4867-9fa5-3c4617ebd24f", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d208a291-bfa1-4867-9fa5-3c4617ebd24f", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cf7170d0-b23f-4eb2-8665-b82cffcd222d", "node_type": "1", "metadata": {}, "hash": "a8a0154637fe7aba3ecc034ce903888d886ae9c1e74f5b6def9cb0f97008539a", "class_name": "RelatedNodeInfo"}, {"node_id": "d524de96-acb3-4502-97c4-cd3cbec8f6cc", "node_type": "1", "metadata": {}, "hash": "f7034d6d82e4337886853ca637966fb14d54083b4f2150fe077f586cfd05ec3c", "class_name": "RelatedNodeInfo"}, {"node_id": "1335020b-6427-4796-ad05-853b7bd61c16", "node_type": "1", "metadata": {}, "hash": "8dc44cc4865e3e6ca895744beaf580fd46dfa84ba9bf92e09a7e68287c33584b", "class_name": "RelatedNodeInfo"}, {"node_id": "ae83b356-b35c-441a-bc88-3cab957b97bb", "node_type": "1", "metadata": {}, "hash": "4f9c32aaae38899b6270ff3cfd00bd9223a3922ca7d9a6791e5dfec0311712e6", "class_name": "RelatedNodeInfo"}, {"node_id": "98e06e1f-432e-4d60-9c09-f7ce436cf92c", "node_type": "1", "metadata": {}, "hash": "328e1f0333a6bba7cc5ba83ff5522c8eab08520c1edecf70ccbfd0da5a32d8a3", "class_name": "RelatedNodeInfo"}]}, "text": "[52] W. A. Bainbridge, J. Hart, E. S. Kim and B. Scassellati, \u201dThe effect of presence on human-\nrobot interaction,\u201d RO-MAN 2008 - The 17th IEEE International Symposium on Robot and\nHuman Interactive Communication, Munich, Germany, 2008, pp. 701-706, doi: 10.1109/RO-\nMAN.2008.4600749.\n[53] A. Vick, D. Surdilovic and J. Kr\u00a8 uger, \u201dSafe physical human-robot interaction with industrial\ndual-arm robots,\u201d 9th International Workshop on Robot Motion and Control, Kuslin, Poland,\n2013, pp. 264-269, doi: 10.1109/RoMoCo.2013.6614619.\n[54] Bragan\u00b8 ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019. A brief overview of the use of\ncollaborative robots in industry 4.0: Human role and safety. Occupational and environmental\nsafety and health, pp.641-650.\n[55] Bragan\u00b8 ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019. A brief overview of the use of\ncollaborative robots in industry 4.0: Human role and safety. Occupational and environmental\nsafety and health, pp.641-650.\n[56] Park, J.S., O\u2019Brien, J., Cai, C.J., Morris, M.R., Liang, P. and Bernstein, M.S., 2023, October.\nGenerative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual\nACM Symposium on User Interface Software and Technology (pp. 1-22).\nAppendix A Section title of first appendix\nAn appendix contains supplementary information that is not an essential part of the text itself but\nwhich may be helpful in providing a more comprehensive understanding of the research problem or\nit is information that is too cumbersome to be included in the body of the paper.\n17", "start_char_idx": 0, "end_char_idx": 1566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d9f52d2-3071-4ee8-a412-ef19afe74560": {"__data__": {"id_": "0d9f52d2-3071-4ee8-a412-ef19afe74560", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52", "node_type": "1", "metadata": {}, "hash": "81753b255890c1e9faaa2542760e9c0b56ae0263dfc788954b7f7b5ebddeaf9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}}, "text": "Human-Robot interaction through joint robot planning\nwith Large Language Models\nKosi Asuzu1*\n1*Birmingham City University.\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot generalisation capa-\nbilities, expanding their utility beyond natural language processing into various applications.\nLeveraging extensive web knowledge, these models generate meaningful text data in response\nto user-defined prompts, introducing a novel mode of interaction with software applications.\nRecent investigations have extended the generalizability of LLMs into the domain of robotics,\naddressing challenges in existing robot learning techniques such as reinforcement learning and\nimitation learning.", "start_char_idx": 0, "end_char_idx": 709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52": {"__data__": {"id_": "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d9f52d2-3071-4ee8-a412-ef19afe74560", "node_type": "1", "metadata": {}, "hash": "555d533be8d80d2d1ac5a4e5c0f9d743d94049d9de36614077629d25ccbe5783", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fc2ed2a-4859-4c63-99a1-98da3c706bfb", "node_type": "1", "metadata": {}, "hash": "e65bdb33425c9550ff3557a545aff066af2b5b09ac516e7d5d6bb40653a67018", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}}, "text": "This paper explores the application of LLMs for robot planning as an alter-\nnative approach to generate high-level robot plans based on prompts provided to the language\nmodel. The proposed methodology facilitates continuous user interaction and adjustment of task\nexecution plans in real-time. A pre-trained LLM is utilized for collaborative human-robot plan-\nning using natural language, complemented by Vision Language Models (VLMs) responsible for\ngenerating scene descriptions incorporated into the prompt for contextual grounding.\nEvaluation of the system is conducted within the VIMABench benchmark simulated environ-\nment.", "start_char_idx": 710, "end_char_idx": 1339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8fc2ed2a-4859-4c63-99a1-98da3c706bfb": {"__data__": {"id_": "8fc2ed2a-4859-4c63-99a1-98da3c706bfb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52", "node_type": "1", "metadata": {}, "hash": "81753b255890c1e9faaa2542760e9c0b56ae0263dfc788954b7f7b5ebddeaf9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37fc32c5-6cd8-4dac-b4fe-f058d588625f", "node_type": "1", "metadata": {}, "hash": "7b2141d767cf0cf83153a7606ef946705fd486949811a308da4943f4e1b1ffa7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation of the system is conducted within the VIMABench benchmark simulated environ-\nment. Further practicality assessment involves experimentation with the system on a robotic arm\nengaged in tabletop manipulation activities. The observed results reveal that soliciting human\nfeedback for zero-shot plans generated by LLMs in robot manipulation yields an 8.6% perfor-\nmance increase in simulated evaluations and a 14% increase in physical evaluations compared to\nthe baseline, showcasing the efficacy of the proposed approach.", "start_char_idx": 1246, "end_char_idx": 1775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37fc32c5-6cd8-4dac-b4fe-f058d588625f": {"__data__": {"id_": "37fc32c5-6cd8-4dac-b4fe-f058d588625f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fc2ed2a-4859-4c63-99a1-98da3c706bfb", "node_type": "1", "metadata": {}, "hash": "e65bdb33425c9550ff3557a545aff066af2b5b09ac516e7d5d6bb40653a67018", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "177dd3e4-5571-4ead-9052-bd8f23b37f89", "node_type": "1", "metadata": {}, "hash": "5ad1e2384df87657a290b1a7d59385f49000073fb75be3f077f97dd63efccd38", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}}, "text": "Keywords: human-robot interaction, robotics, robot-arm, large language models, computer vision,\nnatural language processing\n1 Introduction\nThe field of robotics has experienced notable progress, primarily propelled by the integration of deep\nlearning techniques in perception, planning, and manipulation [3]. Recent advancements in Large\nLanguage Models (LLMs), exemplified by GPT and its successors, have positioned these models as\ntransformative tools across diverse robot learning applications [4][5].", "start_char_idx": 1776, "end_char_idx": 2280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "177dd3e4-5571-4ead-9052-bd8f23b37f89": {"__data__": {"id_": "177dd3e4-5571-4ead-9052-bd8f23b37f89", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37fc32c5-6cd8-4dac-b4fe-f058d588625f", "node_type": "1", "metadata": {}, "hash": "7b2141d767cf0cf83153a7606ef946705fd486949811a308da4943f4e1b1ffa7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e283248-217b-4607-a828-330db0cb3c4b", "node_type": "1", "metadata": {}, "hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "class_name": "RelatedNodeInfo"}}, "text": "With the impending realisation\nof General-Purpose Robots (GPRs), LLMs have garnered significant attention for their potential\nto revolutionize robot planning, a crucial aspect of autonomous systems [12]. The capacity to plan\nactions, make informed decisions based on language-defined instructions, and adapt to dynamic\nenvironments is paramount for robots operating in real-world scenarios.", "start_char_idx": 2281, "end_char_idx": 2671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aaa052f9-cbb5-41bf-aa39-144bb278d76f": {"__data__": {"id_": "aaa052f9-cbb5-41bf-aa39-144bb278d76f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fd74188-bb64-49a6-b643-06b173e30da9", "node_type": "1", "metadata": {}, "hash": "9ee9bc9123c08777edb87d85d2f3880f2aee358156c16c23e6f82bc125110c21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}}, "text": "This research explores the application of LLMs in the domain of robot planning, emphasizing\nthe incorporation of human feedback to enhance the robustness and accuracy of the generated task\nexecution plan. The proposed end-to-end solution leverages zero-shot learning vision models for\nobject detection, vision-language models for scene description, and the manipulation of a physical\nrobot arm using generated instructions. Traditionally, robot planning relied on carefully crafted\nalgorithms, heuristics, or learning-based techniques like reinforcement learning or imitation learning.", "start_char_idx": 0, "end_char_idx": 585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fd74188-bb64-49a6-b643-06b173e30da9": {"__data__": {"id_": "3fd74188-bb64-49a6-b643-06b173e30da9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aaa052f9-cbb5-41bf-aa39-144bb278d76f", "node_type": "1", "metadata": {}, "hash": "dd95b0e93f5cb8845523cdc34b5b1f970c2e3fc9bcd398f5936dc050acabdb60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "892913dc-5691-4b8c-b89a-7cbd4b290020", "node_type": "1", "metadata": {}, "hash": "1bceddfe3ffcf273ebbfa06b93837bc5d69f8796c8b590711d0bbea9508503d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}}, "text": "While fruitful, these approaches often encounter challenges in unstructured environments, diverse\ntasks, and nuanced human interactions [14].\n1\n\nLLMs, with their proficiency in comprehending and generating open-ended, human-like text,\nintroduce new possibilities in natural language understanding, generation, and reasoning [15]. These\nmodels excel in processing and generating human-readable instructions, bridging the gap between\nhuman intent and robot actions. The proposed methodology utilizes GPT language models to gener-\nate high-level robot plans, facilitating the execution of robotic tasks.", "start_char_idx": 586, "end_char_idx": 1186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "892913dc-5691-4b8c-b89a-7cbd4b290020": {"__data__": {"id_": "892913dc-5691-4b8c-b89a-7cbd4b290020", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fd74188-bb64-49a6-b643-06b173e30da9", "node_type": "1", "metadata": {}, "hash": "9ee9bc9123c08777edb87d85d2f3880f2aee358156c16c23e6f82bc125110c21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8420ba4-1923-44bc-ad6f-f039cb8dac2d", "node_type": "1", "metadata": {}, "hash": "8b01d54ef9a09f2ac8d5701b51c62bf346548f23c8f564aacfdaf281c9386833", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}}, "text": "These plans consist of high-level\nAPI functions (referred to as \u2019skills\u2019[43]) employing rule-based motion and manipulation algorithms\nand deep learning models like Vision Transformers for zero-shot object detection and Vision Lan-\nguage Models (VLMs) for scene understanding [22]. This approach allows the language model to\ngenerate plans using these skills as interfaces for perception and manipulation.", "start_char_idx": 1187, "end_char_idx": 1591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8420ba4-1923-44bc-ad6f-f039cb8dac2d": {"__data__": {"id_": "a8420ba4-1923-44bc-ad6f-f039cb8dac2d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "892913dc-5691-4b8c-b89a-7cbd4b290020", "node_type": "1", "metadata": {}, "hash": "1bceddfe3ffcf273ebbfa06b93837bc5d69f8796c8b590711d0bbea9508503d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4", "node_type": "1", "metadata": {}, "hash": "790a7bf0b5173889d4b1c24df255ccf215069fa4026b62f0de1178babcff6e2c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}}, "text": "This approach allows the language model to\ngenerate plans using these skills as interfaces for perception and manipulation.\nThis research endeavors to enrich the ongoing dialogue concerning the future trajectory of\nrobotics, where intelligent planning, natural language interaction, and the transformative influence\nof Large Language Models (LLMs) coalesce to redefine the manner in which robots perceive, navi-\ngate, and engage with their surroundings.", "start_char_idx": 1468, "end_char_idx": 1921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4": {"__data__": {"id_": "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8420ba4-1923-44bc-ad6f-f039cb8dac2d", "node_type": "1", "metadata": {}, "hash": "8b01d54ef9a09f2ac8d5701b51c62bf346548f23c8f564aacfdaf281c9386833", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6bc5afb-53ff-400c-92b8-660acb67c15e", "node_type": "1", "metadata": {}, "hash": "d0c630e1f7dd315529172d79870f78a2edebead71c5acc5a9f34737766324fab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}}, "text": "Consequently, the delineated contributions of this paper\nare as follows:\n1.Human-interactive general purpose robotic system : This system extends LLM plan gener-\nation for robotics by incorporating a feedback loop and message stream, enabling users to provide\nfeedback on generated LLM plans.\n2.Scene understanding : Utilizing a VLM, this contribution involves incorporating a description of\nthe robot\u2019s scene. The description is then added to the prompt provided to the LLM for planning,\nserving as a grounding mechanism for the LLM.", "start_char_idx": 1922, "end_char_idx": 2456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6bc5afb-53ff-400c-92b8-660acb67c15e": {"__data__": {"id_": "b6bc5afb-53ff-400c-92b8-660acb67c15e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4", "node_type": "1", "metadata": {}, "hash": "790a7bf0b5173889d4b1c24df255ccf215069fa4026b62f0de1178babcff6e2c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cfc6a798-5b46-44dd-a887-491516a74f13", "node_type": "1", "metadata": {}, "hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "class_name": "RelatedNodeInfo"}}, "text": "This study employs LLMs to formulate robot plans for the execution of human instructions\nconveyed in natural language.", "start_char_idx": 2457, "end_char_idx": 2575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4f63a6b-d521-4b74-bce9-8970fa75e32f": {"__data__": {"id_": "a4f63a6b-d521-4b74-bce9-8970fa75e32f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d560d311-94bc-47ae-bbbc-2a83a13270e2", "node_type": "1", "metadata": {}, "hash": "7c21e94fcd4343da4c5dd8348aea69671b28765ff39fc12f1c8b7308fda915ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}}, "text": "It facilitates real-time interaction and allows plan adjustments by the\nhuman user, as illustrated in Figure 1. The feedback, instructions, and plans are stored as context,\nintegrated into the message stream to enhance the responsiveness of the LLM. Subsequent evaluations\nscrutinize the proposed approach within a simulated robot learning benchmark, complemented by\nassessments on an operational robotic platform tailored for specific tabletop manipulation tasks.\nFigure 1 : Human robot interaction through joint planning with LLM system diagram.", "start_char_idx": 0, "end_char_idx": 547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d560d311-94bc-47ae-bbbc-2a83a13270e2": {"__data__": {"id_": "d560d311-94bc-47ae-bbbc-2a83a13270e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4f63a6b-d521-4b74-bce9-8970fa75e32f", "node_type": "1", "metadata": {}, "hash": "edeb9f9f1ab9c2c2dbd7c036eb7467c9426a4721eef29b21a25eba66f1b0c6df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02531e66-29d7-4df8-89b7-184cb314c3f6", "node_type": "1", "metadata": {}, "hash": "6138f49b24f87e8e692cffb8eddd4f2170b05c5331eabc3fb451118ac7ad2898", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1 : Human robot interaction through joint planning with LLM system diagram. The diagram\nemphasizes the intricate dynamics between the human user and the LLM, with the storage of inter-\naction context in the message stream and subsequent execution of the robot plan.\n1.1 Related Work\n1.1.1 Human-Robot Interaction\nHuman-robot interaction (HRI) is a multidisciplinary field that explores the design, development,\nand evaluation of systems where humans and robots interact in various settings. Over the years,\nHRI has witnessed significant advancements, leading to an array of research and applications [23].", "start_char_idx": 465, "end_char_idx": 1077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02531e66-29d7-4df8-89b7-184cb314c3f6": {"__data__": {"id_": "02531e66-29d7-4df8-89b7-184cb314c3f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d560d311-94bc-47ae-bbbc-2a83a13270e2", "node_type": "1", "metadata": {}, "hash": "7c21e94fcd4343da4c5dd8348aea69671b28765ff39fc12f1c8b7308fda915ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dfbe97c-0ec0-466d-95a7-85757c75dc0a", "node_type": "1", "metadata": {}, "hash": "d53393852cb46475ca82e9d69fc700fdbe2f9a2a3fc7ebc96dde0a92d8b94f78", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}}, "text": "In collaborative and assistive robotics, researchers explore scenarios where robots work alongside\nhumans to enhance productivity or provide assistance [24] [26]. Pioneering investigations in human-\nrobot interaction have explored various applications, such as robot-assisted therapy. Notable research\nin this domain includes efforts to leverage social robots for aiding low-functioning children with autism\n2\n\n[25]. The broader landscape of human-robot interaction encompasses a diverse spectrum of studies\nand advancements, covering aspects like social robotics, design principles, collaborative frameworks,\nethical considerations, and technological innovations[53][52].", "start_char_idx": 1078, "end_char_idx": 1750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dfbe97c-0ec0-466d-95a7-85757c75dc0a": {"__data__": {"id_": "3dfbe97c-0ec0-466d-95a7-85757c75dc0a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02531e66-29d7-4df8-89b7-184cb314c3f6", "node_type": "1", "metadata": {}, "hash": "6138f49b24f87e8e692cffb8eddd4f2170b05c5331eabc3fb451118ac7ad2898", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a766a312-c240-47be-a6b9-6c8a0f30effe", "node_type": "1", "metadata": {}, "hash": "ec53f0895b0c8a11a281f1984e2817f33719d99ecec0c3f7476e2d1675b7667e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}}, "text": "In the context of this research project,\na dyadic interaction system is devised to facilitate human-robot interaction in the planning of robot\ntasks, involving collaboration between a single robot and a human partner.\n1.1.2 Collaborative Robot arms in Industry\nCollaborative robotics, also known as cobots, represent a transformative paradigm in the manufac-\nturing and industrial sectors [54]. These robots are designed to work alongside human operators,\nfacilitating safer, more efficient, and highly flexible production processes [27].", "start_char_idx": 1751, "end_char_idx": 2289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a766a312-c240-47be-a6b9-6c8a0f30effe": {"__data__": {"id_": "a766a312-c240-47be-a6b9-6c8a0f30effe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dfbe97c-0ec0-466d-95a7-85757c75dc0a", "node_type": "1", "metadata": {}, "hash": "d53393852cb46475ca82e9d69fc700fdbe2f9a2a3fc7ebc96dde0a92d8b94f78", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "843dae4f-9177-4be2-bea3-15b167c43370", "node_type": "1", "metadata": {}, "hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "class_name": "RelatedNodeInfo"}}, "text": "A convergence of\nresearch in safety, human-robot interaction, task allocation, programming interfaces, industry-specific\napplications, and connectivity has laid the foundation for the widespread adoption of collaborative\nrobots in a variety of industrial settings [28].", "start_char_idx": 2290, "end_char_idx": 2559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c24ccb24-0ecf-4a0d-b736-aa3765f5e854": {"__data__": {"id_": "c24ccb24-0ecf-4a0d-b736-aa3765f5e854", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2aa7ecba-c61b-49b5-b220-1fca807df300", "node_type": "1", "metadata": {}, "hash": "ddd4af6f205afd4350802a4fc56bde65123e0db010883f40acdb16002984615d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}}, "text": "Collaborative robotics has brought transformative changes to the industrial landscape, offer-\ning innovative solutions for improving efficiency, safety, and flexibility in manufacturing processes\n[29]. Research conducted by Marcel Bergerman and Silvio M. Maeta primarily concentrates on the\ndesign of collaborative robots and their seamless integration into established farming processes [31].\nInvestigations in this domain have delved into factors such as comprehending and optimizing the\ninteraction between human workers and robots to elevate productivity. Effectively allocating tasks\nbetween human and robot workers represents a crucial research challenge.", "start_char_idx": 0, "end_char_idx": 661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2aa7ecba-c61b-49b5-b220-1fca807df300": {"__data__": {"id_": "2aa7ecba-c61b-49b5-b220-1fca807df300", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c24ccb24-0ecf-4a0d-b736-aa3765f5e854", "node_type": "1", "metadata": {}, "hash": "9ab438c1bb38602e11a4223744801a09bf364180371e9cc2104ebed5d9396e6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e140397-4561-4086-b5d8-5b4f8095eeb4", "node_type": "1", "metadata": {}, "hash": "5208ebac2c3a164f6f46957a83033eeba166d492a678adcd2608295f233e24f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}}, "text": "Effectively allocating tasks\nbetween human and robot workers represents a crucial research challenge.\nAs the field continues to evolve, the development of even more intuitive and adaptive collabo-\nrative robots is expected, addressing increasingly complex challenges in modern manufacturing. A\nframework is proposed to enable joint real-time planning between humans and intelligent collabo-\nrative robots using natural language. This approach is intended for application in industries where\nhumans and robots collaborate in shared environments.", "start_char_idx": 560, "end_char_idx": 1104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e140397-4561-4086-b5d8-5b4f8095eeb4": {"__data__": {"id_": "4e140397-4561-4086-b5d8-5b4f8095eeb4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2aa7ecba-c61b-49b5-b220-1fca807df300", "node_type": "1", "metadata": {}, "hash": "ddd4af6f205afd4350802a4fc56bde65123e0db010883f40acdb16002984615d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c", "node_type": "1", "metadata": {}, "hash": "0c1b1c724b8ffd4a837485ab4eec43d46f65e0154f89b7fa693c1374ad073a08", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}}, "text": "This approach is intended for application in industries where\nhumans and robots collaborate in shared environments.\nPrior investigations have delved into the integration of collaborative robot arms within Indus-\ntry 4.0 manufacturing environments, demonstrating their capability to enhance overall production\nefficiency by aiding workers in both physical and cognitive tasks [55]. The study underscores the\ndynamic nature of human involvement in the manufacturing process and emphasizes the necessity\nfor human operators to adapt effectively to interact with these tools. The current research delves\ninto language as an interface for facilitating dynamic interaction within this context.", "start_char_idx": 989, "end_char_idx": 1676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c": {"__data__": {"id_": "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e140397-4561-4086-b5d8-5b4f8095eeb4", "node_type": "1", "metadata": {}, "hash": "5208ebac2c3a164f6f46957a83033eeba166d492a678adcd2608295f233e24f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "178675e5-5624-4cba-86c4-41443ef806be", "node_type": "1", "metadata": {}, "hash": "fc7d1236a106213f3b335aa885a1d5fecf82aacdcc4aa6f6a759d2d86d7c6018", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}}, "text": "The current research delves\ninto language as an interface for facilitating dynamic interaction within this context.\n1.1.3 Large Language Models\nThe Transformer architecture, introduced by Vaswani et al. [32], represents a foundational break-\nthrough in the development of LLMs. Transformers rely on self-attention mechanisms to capture\ncomplex dependencies between words in a sequence, enabling parallelism and scale. This architecture\nforms the basis of many subsequent language models, such as BERT, GPT, and XLNet [33].\nLLMs represent a revolutionary leap in natural language processing and understanding.", "start_char_idx": 1561, "end_char_idx": 2169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "178675e5-5624-4cba-86c4-41443ef806be": {"__data__": {"id_": "178675e5-5624-4cba-86c4-41443ef806be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c", "node_type": "1", "metadata": {}, "hash": "0c1b1c724b8ffd4a837485ab4eec43d46f65e0154f89b7fa693c1374ad073a08", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "node_type": "1", "metadata": {}, "hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "class_name": "RelatedNodeInfo"}}, "text": "LLMs represent a revolutionary leap in natural language processing and understanding. The\ncombination of sophisticated architectures, massive data-sets, fine-tuning strategies and multilingual\ncapabilities has paved the way for their widespread adoption and applications across various domains\n[35].\nAs research in this field continues to evolve, the development of more sophisticated, context-aware,\nand ethically aligned language models is expected, offering exciting opportunities and challenges for\nthe future of AI-powered language understanding and generation. One of such future opportunities\ninclude enabling new forms of language inference for robotics application.", "start_char_idx": 2084, "end_char_idx": 2758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b70a610f-59be-4ed3-9210-3fd4af7e34cb": {"__data__": {"id_": "b70a610f-59be-4ed3-9210-3fd4af7e34cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00df92de-8dba-43a8-af11-19ce18c1df5e", "node_type": "1", "metadata": {}, "hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "00df92de-8dba-43a8-af11-19ce18c1df5e", "node_type": "1", "metadata": {}, "hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "class_name": "RelatedNodeInfo"}}, "text": "One of such future opportunities\ninclude enabling new forms of language inference for robotics application.\n1.1.4 Zero-shot object detection\nZero-shot learning aims to identify objects without available labels during training, enabling classifiers\nto recognize unseen classes [38]. Zero-shot detection (ZSD) is designed to simultaneously localize\nand recognize objects from novel categories [38][42].", "start_char_idx": 0, "end_char_idx": 400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c26192c-4bb1-4fe5-92a4-0cfd6afc672c": {"__data__": {"id_": "8c26192c-4bb1-4fe5-92a4-0cfd6afc672c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccdc64e7-8578-402c-adfc-7f9bedebc2fa", "node_type": "1", "metadata": {}, "hash": "bf6da2e2d1f0cc03419c25db15eaf97b5350d817d5a1b0184cf50ec73e9e05d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}}, "text": "ZSD algorithms fall into classifier-based and\ninstance-based methods. Classifier-based methods concentrate on amalgamating a traditional object\ndetection framework with a zero-shot learning classifier. In the case of instance-based methods,\ninspired by synthesis techniques in zero-shot learning, the typical approach involves initially training\na traditional object detection with seen objects and subsequently updating the confidence predictor\nusing synthesized images or visual features.\n3\n\nThis paper employs two classifier-based methods, namely SAM-CLiP[39][40] and ViLD[41], for\nidentifying objects in a scene based on a query.", "start_char_idx": 0, "end_char_idx": 633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccdc64e7-8578-402c-adfc-7f9bedebc2fa": {"__data__": {"id_": "ccdc64e7-8578-402c-adfc-7f9bedebc2fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c26192c-4bb1-4fe5-92a4-0cfd6afc672c", "node_type": "1", "metadata": {}, "hash": "460bcb7c0b5f667968b98861d24282042078edbea5d43fe84392f6ca9967b179", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e748def4-5a55-4031-947f-9004d685688c", "node_type": "1", "metadata": {}, "hash": "bd0d69ebba0018a2cae92f34220d050df176edcfae5ad7ee016ac226178cb12c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}}, "text": "SAM-CLiP utilizes contrastive learning on both natural language and images to enable zero-shot\nobject classification. The Segment Anything Model (SAM) is a segmentation model facilitating data\nannotation and zero-shot transfer through prompt engineering. CLiP replaces the original classifier\nwith zero-shot learning classifiers [1].\nIn the second vision approach, ViLD leverages vision and language knowledge distillation. This\ninvolves distilling knowledge from a pre-trained open-vocabulary image classification model into a\ntwo-stage detector.", "start_char_idx": 634, "end_char_idx": 1181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e748def4-5a55-4031-947f-9004d685688c": {"__data__": {"id_": "e748def4-5a55-4031-947f-9004d685688c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccdc64e7-8578-402c-adfc-7f9bedebc2fa", "node_type": "1", "metadata": {}, "hash": "bf6da2e2d1f0cc03419c25db15eaf97b5350d817d5a1b0184cf50ec73e9e05d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7919a516-e2f9-4871-b726-7ef1660feebd", "node_type": "1", "metadata": {}, "hash": "12eca0246b68dca61368d35769141668b919226c4837cac3a4554e92d06603d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}}, "text": "The selection of ViLD and SAM-CLiP as the zero-shot detection models in this study is predicated\non their accessibility and prior utilization in analogous projects such as SayCan and Instruct2Act.\nIt\u2019s essential to note that this research does not conduct a comprehensive assessment of all existing\nzero-shot models or determine the optimal ones; rather, the choice of models is contingent upon their\navailability.\nViLD comprises two components, learning with text embeddings and image embeddings derived\nfrom an open vocabulary image classification model.", "start_char_idx": 1182, "end_char_idx": 1738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7919a516-e2f9-4871-b726-7ef1660feebd": {"__data__": {"id_": "7919a516-e2f9-4871-b726-7ef1660feebd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e748def4-5a55-4031-947f-9004d685688c", "node_type": "1", "metadata": {}, "hash": "bd0d69ebba0018a2cae92f34220d050df176edcfae5ad7ee016ac226178cb12c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70f7146f-e6e4-4d84-900e-aac0131533f9", "node_type": "1", "metadata": {}, "hash": "54c22477f765376339278c39e2303b60bb566b14ff94499aa0e136020d07a1c7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}}, "text": "The ViLD model, also used in SayCan and\nSocratic Models, serves as the vision model in this implementation [43][44]. This literature review\nhighlights key approaches in the realm of zero-shot learning and detection, providing a foundation\nfor the current research.\n1.1.5 LLMs for Robot Planning\nThe integration of LLMs with robotics has opened new frontiers in robot planning and execution\n[17]. These models empower robots with natural language understanding, enabling them to interact\nwith humans and external systems through text or speech [10].", "start_char_idx": 1739, "end_char_idx": 2287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70f7146f-e6e4-4d84-900e-aac0131533f9": {"__data__": {"id_": "70f7146f-e6e4-4d84-900e-aac0131533f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7919a516-e2f9-4871-b726-7ef1660feebd", "node_type": "1", "metadata": {}, "hash": "12eca0246b68dca61368d35769141668b919226c4837cac3a4554e92d06603d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3de7bbe1-118a-45de-ab42-327e9f821b94", "node_type": "1", "metadata": {}, "hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "class_name": "RelatedNodeInfo"}}, "text": "One of the foundational applications of LLMs in robotics is the understanding of natural lan-\nguage commands [4] [9]. Research in this area has explored methods for mapping text or spoken\nlanguage into executable robot commands [1].", "start_char_idx": 2288, "end_char_idx": 2520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25f39df5-e777-427c-a9a0-a661f7ca48ab": {"__data__": {"id_": "25f39df5-e777-427c-a9a0-a661f7ca48ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4023064-a0ff-4c94-a537-214611f7c55a", "node_type": "1", "metadata": {}, "hash": "129965f24bfbb769058b4d3fd980d88037a8532e8c3a23f925e816b06b5d172e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}}, "text": "Seminal works by Brohan et al. and Jiang et al. have\ndemonstrated the feasibility of converting human instructions into robot actions. LLMs have been\nemployed to improve robotic task planning and execution [37] [20]. TidyBot show how LLMs can be\nused in the personalisation of robot policy [2].\nPrior studies have delved into the application of Large Language Models (LLMs) in planning\nwithin a communicative environment [47]. In their work, a specialized framework was formulated for\ncooperative agents operating within a multi-agent embodied environment.", "start_char_idx": 0, "end_char_idx": 556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4023064-a0ff-4c94-a537-214611f7c55a": {"__data__": {"id_": "f4023064-a0ff-4c94-a537-214611f7c55a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25f39df5-e777-427c-a9a0-a661f7ca48ab", "node_type": "1", "metadata": {}, "hash": "701b4d322c8220cb4525b80dfb544d9e0d5bb220b33935aac593b039bc297249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78598da2-b279-46d4-aaf0-fefca676c30d", "node_type": "1", "metadata": {}, "hash": "84af333610b2ca463f614a413ca1625a4dd006b5b1eabfb18da9a66c0b22ab73", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}}, "text": "Showcasing the capabilities\nof the GPT-4 model, the investigation illustrated its capacity to outperform robust planning-based\nmethods, exemplifying emergent effective communication devoid of the necessity for fine-tuning.\nThe LLM-MCTS (Monte Carlo Tree Search) investigation contributed valuable insights by reveal-\ning that LLMs not only provide a policy for action but also offer a commonsense model of the world\n[45]. Monte Carlo Tree Search, a powerful search algorithm, was employed in this exploration. This\ninvolved integrating the world model into the policy for a search algorithm like MCTS.", "start_char_idx": 557, "end_char_idx": 1158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78598da2-b279-46d4-aaf0-fefca676c30d": {"__data__": {"id_": "78598da2-b279-46d4-aaf0-fefca676c30d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4023064-a0ff-4c94-a537-214611f7c55a", "node_type": "1", "metadata": {}, "hash": "129965f24bfbb769058b4d3fd980d88037a8532e8c3a23f925e816b06b5d172e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8", "node_type": "1", "metadata": {}, "hash": "dbbbe91bdc83edd1df308d2fa8a0a72ea92a71d197e06c3f9f2f9f5df0d1745a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}}, "text": "This\ninvolved integrating the world model into the policy for a search algorithm like MCTS. Concurrently,\nLLMs with Tree-of-Thought (LLM-ToT) approach adopted a tree-based strategy, enabling LLMs to\nengage in deliberate decision-making by assessing various reasoning paths and self-evaluating choices\nto determine the subsequent course of action [46].\nThe results of these studies highlight the effectiveness of search-based plan generation over policies\ninduced by LLMs. Nevertheless, given the straightforward nature of the tasks in these experiments,\nthe decision is made to refrain from employing the search-based approach.", "start_char_idx": 1067, "end_char_idx": 1694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8": {"__data__": {"id_": "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78598da2-b279-46d4-aaf0-fefca676c30d", "node_type": "1", "metadata": {}, "hash": "84af333610b2ca463f614a413ca1625a4dd006b5b1eabfb18da9a66c0b22ab73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7159ecb2-ba9a-49a2-9294-ac6a6e52ccf7", "node_type": "1", "metadata": {}, "hash": "7bd07ca2b81e49b420b289efd8ef3dfd63a79a6b5b3d9f6292c8f5b8d4843012", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}}, "text": "Instead, reliance is placed\non the LLM-induced policies in their original form.\nRecent research endeavors have prominently concentrated on formulating task plans grounded\nin high-level natural language descriptions, thereby endowing robots with the capability to execute\nintricate tasks with minimal human intervention [34] [21] [18]. Yu et al. advanced the field by utilizing\na language model to generate rewards applicable to robots for skill synthesis [19].", "start_char_idx": 1695, "end_char_idx": 2155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7159ecb2-ba9a-49a2-9294-ac6a6e52ccf7": {"__data__": {"id_": "7159ecb2-ba9a-49a2-9294-ac6a6e52ccf7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8", "node_type": "1", "metadata": {}, "hash": "dbbbe91bdc83edd1df308d2fa8a0a72ea92a71d197e06c3f9f2f9f5df0d1745a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d18e9348-dfee-468f-85ad-d27e16ec8090", "node_type": "1", "metadata": {}, "hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "class_name": "RelatedNodeInfo"}}, "text": "Another notable\napproach, denoted as LLMs with optimal Planning proficiency (LLM+P), leverages language models\nto produce planning directives that robots can subsequently employ for executing complex tasks\n[12].", "start_char_idx": 2156, "end_char_idx": 2367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f051495e-b4cf-462e-b42a-67f9b5ebd4c0": {"__data__": {"id_": "f051495e-b4cf-462e-b42a-67f9b5ebd4c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43d26dba-e6a1-48c4-ac6f-513f6d1ceace", "node_type": "1", "metadata": {}, "hash": "267c7fbf07b5d0bb58522393e8e32d301d090e8df80c8a47130bf8523929250b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}}, "text": "LLM+P employs language models to address Planning Domain Definition Language (PDDL)-\nbased planning problems. Significantly, the researchers demonstrate that the language understanding\ncapabilities of LLMs empower them to apply common sense reasoning to terms within the PDDL\nproblem.\n4\n\nThe decision to omit the PDDL approach in this study is grounded in the intricate nature\nof parsing PDDL instructions into robot actions, as such intricacies fall beyond the scope of this\ninvestigation.", "start_char_idx": 0, "end_char_idx": 490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43d26dba-e6a1-48c4-ac6f-513f6d1ceace": {"__data__": {"id_": "43d26dba-e6a1-48c4-ac6f-513f6d1ceace", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f051495e-b4cf-462e-b42a-67f9b5ebd4c0", "node_type": "1", "metadata": {}, "hash": "6065556a5f00d36a5b50590335aa6dbdcccaaffc81ced2cbb8a1474da8acd3cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1407828d-734e-4103-b05f-1c3429cf8784", "node_type": "1", "metadata": {}, "hash": "d3facc49994d9386495173f35ec304aac0bb4a2ef1d695076d72034dda496f7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}}, "text": "LLM-Brain paper proposes a memory and action system for embodied control and configuration\nfor a nervous system with the LLM acting as the central brain[17]. Code-as-Policies uses language\nmodels to generate executable python code using API directives which can then be given to a robot\nas policies [7]. Methods such as Instruct2Act and ProgPrompt focus on generating instructions using\npredefined high level robot functions without any programming primitives [1] [6].", "start_char_idx": 491, "end_char_idx": 959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1407828d-734e-4103-b05f-1c3429cf8784": {"__data__": {"id_": "1407828d-734e-4103-b05f-1c3429cf8784", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43d26dba-e6a1-48c4-ac6f-513f6d1ceace", "node_type": "1", "metadata": {}, "hash": "267c7fbf07b5d0bb58522393e8e32d301d090e8df80c8a47130bf8523929250b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c17b65e8-1f37-4a18-beea-1450866636f4", "node_type": "1", "metadata": {}, "hash": "5310d24e111644c1c2707d0a1662cc413afef6eaf1196a3abae16ea9ab676bba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}}, "text": "This approach\nstands as the prevailing method in contemporary literature and is the methodology employed in the\ndevelopment of the system in this research [8] [9] [11] [17] [18]. While ProgPrompt integrates program-\nming language structures, such as loops, lists, and function definitions, into its robot plan generation,\nthis approach diverges by refraining from the explicit utilisation of programming structures. Instead,\nmost implementation complexities are managed behind the API definition, akin to Instruct2Act.\nThis streamlined approach simplifies the generated LLM plans and the feedback process.", "start_char_idx": 960, "end_char_idx": 1565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c17b65e8-1f37-4a18-beea-1450866636f4": {"__data__": {"id_": "c17b65e8-1f37-4a18-beea-1450866636f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1407828d-734e-4103-b05f-1c3429cf8784", "node_type": "1", "metadata": {}, "hash": "d3facc49994d9386495173f35ec304aac0bb4a2ef1d695076d72034dda496f7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "689d7d2f-c966-411d-9425-fc11d5ac733d", "node_type": "1", "metadata": {}, "hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "class_name": "RelatedNodeInfo"}}, "text": "This streamlined approach simplifies the generated LLM plans and the feedback process.\nThe synergy between LLMs and robotics has ushered in an era of more intuitive, versatile, and\nadaptive robotic systems [37]. As research in this field advances, the development of robots that under-\nstand and communicate with humans through natural language continues to unlock new possibilities\nin domains such as home automation, healthcare, education, and industry [2].", "start_char_idx": 1479, "end_char_idx": 1938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae020f6-3531-47c0-acfb-5c2635f4d78f": {"__data__": {"id_": "2ae020f6-3531-47c0-acfb-5c2635f4d78f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b9a7b44-a020-4a1e-8471-3b9ca588f56d", "node_type": "1", "metadata": {}, "hash": "b1cc68771ccfea5f64075ce2e2c3038740627bd4ee7729ff067718d300853fa9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}}, "text": "Table 1 : Physical robot environment evaluation\nRelated Projects\nProject name Methodology Evaluation tech-\nniqueReference\nInstruct2Act Using multimodal instructions pro-\nvided to pre-trained model to gen-\nerate highlevel robot plan based on\npython APIsSimulation [1]\nTinyBot Using language models to generate\nrobot plans based on user prefer-\nences from a set of examples via\nprevious interactionsSimulation &\nPhysical Robot[2]\nPaLM-E Using multi-modal transformer\nmodel trained end-to-end on\nvision, language and continuous\nactions for sequential robot manip-\nulation planning, visual question\nanswering and captioningPhysical Robot", "start_char_idx": 0, "end_char_idx": 633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b9a7b44-a020-4a1e-8471-3b9ca588f56d": {"__data__": {"id_": "2b9a7b44-a020-4a1e-8471-3b9ca588f56d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae020f6-3531-47c0-acfb-5c2635f4d78f", "node_type": "1", "metadata": {}, "hash": "9353ff451cd12e3010e3138f92e760878685958101b286893477b9d9384ec63a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13103365-ee6e-468d-8679-47406168ad1e", "node_type": "1", "metadata": {}, "hash": "2cef2a32a39108ae84b7d9b1e8083a885385f33cd56b7bf2c146ceaa5592baa2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}}, "text": "visual question\nanswering and captioningPhysical Robot [4]\nCode-As-Policies Using code writing language mod-\nels to write python code which is\nthen used as a robot policySimulation [7]\nProgPrompt Using program like instructions\nas input to pre-trained language\nmodel to generate programmatic\nplan instructions for robot controlSimulation [6]\nLLM-BRAIn Using a fine-tuned LLM to gener-\nate robot behaviour tree to control\nrobotSimulation [17]\nLLM-P Using language models to generate\nplans in the planning domain def-\ninition language (PDDL) given a\nhigh level planning", "start_char_idx": 579, "end_char_idx": 1146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13103365-ee6e-468d-8679-47406168ad1e": {"__data__": {"id_": "13103365-ee6e-468d-8679-47406168ad1e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b9a7b44-a020-4a1e-8471-3b9ca588f56d", "node_type": "1", "metadata": {}, "hash": "b1cc68771ccfea5f64075ce2e2c3038740627bd4ee7729ff067718d300853fa9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2970cdd8-7cc5-441e-9180-3f0d35aed72b", "node_type": "1", "metadata": {}, "hash": "4c9853b85b783cb771531a481132463bfdd8af64a380cf044efc97796273b404", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}}, "text": "in the planning domain def-\ninition language (PDDL) given a\nhigh level planning problemSimulation [12]\nRT-2 Trained transformer model using\ninternet scale vision and language\ndata as well as low level robot con-\ntrol instructions which is then used\nfor generating robot actions given\nvisual and language instructionsPhysical Robot [37]\nSayCan Using pre-trained language models\nto suggest robot affordances based\non provided instructionPhysical Robot [43]\n5\n\n2 Joint robot planning with Large Language Models\nOur system integrates LLMs to enable collaborative robot planning through interactions with human\nusers as shown in Figure 2.", "start_char_idx": 1067, "end_char_idx": 1700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2970cdd8-7cc5-441e-9180-3f0d35aed72b": {"__data__": {"id_": "2970cdd8-7cc5-441e-9180-3f0d35aed72b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13103365-ee6e-468d-8679-47406168ad1e", "node_type": "1", "metadata": {}, "hash": "2cef2a32a39108ae84b7d9b1e8083a885385f33cd56b7bf2c146ceaa5592baa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae2718cf-b324-43b2-80af-608864bbd060", "node_type": "1", "metadata": {}, "hash": "41191277b84541d7df93f9645c55c3dea9a038dc3a4860f9d7db93c451522625", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}}, "text": "to enable collaborative robot planning through interactions with human\nusers as shown in Figure 2. The primary objective is to establish a dynamic planning environment\nwhere humans can work in unison with robots by providing high-level task instructions, feedback,\nand plan adjustments.\nIn this paper few-shot prompting is used to guide the model to generate outputs from a defined\ndistribution of actions (\u03a0). The set of low level skills (\u03a0) and the language descriptions (\u03a0 d) are\nprovided along with the instruction ( I) and a description of the scene ( S) where the robot is located.", "start_char_idx": 1602, "end_char_idx": 2189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae2718cf-b324-43b2-80af-608864bbd060": {"__data__": {"id_": "ae2718cf-b324-43b2-80af-608864bbd060", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2970cdd8-7cc5-441e-9180-3f0d35aed72b", "node_type": "1", "metadata": {}, "hash": "4c9853b85b783cb771531a481132463bfdd8af64a380cf044efc97796273b404", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "node_type": "1", "metadata": {}, "hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "class_name": "RelatedNodeInfo"}}, "text": "2.1 Language Grounding\nLLMs are not naturally grounded in the real world, providing a set of API skills give the LLMs\ngrounding to enable them take actions in the real world.", "start_char_idx": 2190, "end_char_idx": 2364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3406b63b-8760-44bc-bad2-60f68ad85a22": {"__data__": {"id_": "3406b63b-8760-44bc-bad2-60f68ad85a22", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dac9c22d-586d-41fc-9efe-27c747d1c629", "node_type": "1", "metadata": {}, "hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01020a24-b2d0-43f9-9c09-4f28a0ff2557", "node_type": "1", "metadata": {}, "hash": "6269235c94a7bc3062c7ede7e97fd37080ac9da714262a6869652370674c96c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dac9c22d-586d-41fc-9efe-27c747d1c629", "node_type": "1", "metadata": {}, "hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "class_name": "RelatedNodeInfo"}}, "text": "The skill functions constrain completions to\nthe skill descriptions, enhancing the LLM\u2019s awareness of the robot\u2019s capabilities and ensuring that\nthe generated natural language actions are both feasible and contextually appropriate.\nIn contrast to the approach used in designing affordances in SayCan, this methodology deviates\nby omitting the utilisation of a reinforcement learning algorithm for acquiring language-conditioned\nvalue functions. Instead, the definition of robot affordances relies on the manual programming of\nmanipulation and motion algorithms.", "start_char_idx": 0, "end_char_idx": 561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01020a24-b2d0-43f9-9c09-4f28a0ff2557": {"__data__": {"id_": "01020a24-b2d0-43f9-9c09-4f28a0ff2557", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dac9c22d-586d-41fc-9efe-27c747d1c629", "node_type": "1", "metadata": {}, "hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3406b63b-8760-44bc-bad2-60f68ad85a22", "node_type": "1", "metadata": {}, "hash": "7fbace106ea6f4b25b6e38a3bf6a53a4196dc0947e0a21d89e22cb7ed4be64ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dac9c22d-586d-41fc-9efe-27c747d1c629", "node_type": "1", "metadata": {}, "hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "class_name": "RelatedNodeInfo"}}, "text": "Utilizing the GPT-4 Vision Language Model [48] is employed to furnish scene descriptions ( S)\nfor the LLM prompt, thereby enabling the integration of object affordances into the LLM\u2019s plan\ngeneration.", "start_char_idx": 562, "end_char_idx": 762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b86032ac-9442-4216-ba6b-9afa9612fbb9": {"__data__": {"id_": "b86032ac-9442-4216-ba6b-9afa9612fbb9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "123060e0-ef82-4211-9ac2-8b7958ed9ffc", "node_type": "1", "metadata": {}, "hash": "95414fca3517dc99c4ff51a8cc9e59023a318f416a9a2ff8d7713847633af75a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2 : Continuous feedback process between human user and LLM to generate robot plan.\nThe human provides feedback instructions in natural language which the LLM uses to update the\ngenerated plan\n2.2 Perception with Foundation Vision Models\n. As part of the set of robot skills, perception actions are defined, incorporating foundational vision\nmodels for zero-shot object detection and scene understanding.The vision models are employed for\nobject identification within the scene, yielding bounding boxes for detected objects. Subsequently,\npixel-to-coordinate values are computed.", "start_char_idx": 0, "end_char_idx": 585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "123060e0-ef82-4211-9ac2-8b7958ed9ffc": {"__data__": {"id_": "123060e0-ef82-4211-9ac2-8b7958ed9ffc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b86032ac-9442-4216-ba6b-9afa9612fbb9", "node_type": "1", "metadata": {}, "hash": "e2be555d9672279a599f6c588df01119d17a574b8e09da5c03b0e605ff876723", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69cf0473-e62a-4861-a3ed-572985701870", "node_type": "1", "metadata": {}, "hash": "18d4eca43dea4efeefa8283975dd857a1fd63576753abdb9984ff10350712d31", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}}, "text": "Subsequently,\npixel-to-coordinate values are computed. The perception functions operate beneath the rule-based\nactions, albeit not directly presented as skills to the LLM. Full object generalisation is possible by\ncombining the SAM model for image segmentation and the CLiP model for classification of those\nsegments. In the case of the ViLD model, a pre-defined list of possible objects is provided to the\n6\n\nAlgorithm 1 LLM robot joint plan generation\nGiven : A high level instruction I, state S,", "start_char_idx": 531, "end_char_idx": 1029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69cf0473-e62a-4861-a3ed-572985701870": {"__data__": {"id_": "69cf0473-e62a-4861-a3ed-572985701870", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "123060e0-ef82-4211-9ac2-8b7958ed9ffc", "node_type": "1", "metadata": {}, "hash": "95414fca3517dc99c4ff51a8cc9e59023a318f416a9a2ff8d7713847633af75a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c056a44-4ed4-4f29-af60-bc75de435d21", "node_type": "1", "metadata": {}, "hash": "09f50c0914d40b0677f0cd60cc8d2c95d9539728e47158bc6f06161a876ab8fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}}, "text": "state S, and \u03a0 a set of skills and their language descriptions \u03a0 d\n1:I\u21d0Instruction\n2:\u03a0\u21d0Skills\n3:S\u21d0State\n4:MessageStream \u21d0Store previous messages and responses\n5:MaxFeedback \u21d0MaxFeedbackCount\n6:while I\u0338=stopdo\n7: S\u21d0SceneDescription (Camera )\n8: P\u21d0LLMPlanGenerator (I, S,\u03a0,", "start_char_idx": 1021, "end_char_idx": 1292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c056a44-4ed4-4f29-af60-bc75de435d21": {"__data__": {"id_": "4c056a44-4ed4-4f29-af60-bc75de435d21", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69cf0473-e62a-4861-a3ed-572985701870", "node_type": "1", "metadata": {}, "hash": "18d4eca43dea4efeefa8283975dd857a1fd63576753abdb9984ff10350712d31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8b7d0cb-c03d-4972-abaa-0708bbb9beb0", "node_type": "1", "metadata": {}, "hash": "2cff1b743defb5186f9604f7853541fa11c6ecb594631d5bbf1c7539ff153019", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}}, "text": "S,\u03a0, MessageStream )\n9: MessageStream +I \u25b7 add instruction to message stream\n10: MessageSteam +P \u25b7 add llm generated plan to message steam\n11: NumFeedback \u21d00\n12: InstructionApproved \u21d0RequestApproval (P)\n13: while InstructionApproved \u0338=True &&NumFeedback < MaxFeedback do\n14: Feedback \u21d0RequestFeedback (P)\n15: P\u21d0LLMPlanGenerator (F, S, \u03a0,", "start_char_idx": 1288, "end_char_idx": 1625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8b7d0cb-c03d-4972-abaa-0708bbb9beb0": {"__data__": {"id_": "d8b7d0cb-c03d-4972-abaa-0708bbb9beb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c056a44-4ed4-4f29-af60-bc75de435d21", "node_type": "1", "metadata": {}, "hash": "09f50c0914d40b0677f0cd60cc8d2c95d9539728e47158bc6f06161a876ab8fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed", "node_type": "1", "metadata": {}, "hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "class_name": "RelatedNodeInfo"}}, "text": "S, \u03a0, MessageStream )\n16: MessageSteam +F\n17: MessageSteam +P\n18: InstructionApproved \u21d0RequestApproval (P)\n19: end while\n20: RobotExecute (P)\u25b7execute robot instructions using low level APIs based on generated plan\n21:end while\nmodel before initiation. Notably, the model does not detect objects beyond the predetermined options\nin the list.", "start_char_idx": 1620, "end_char_idx": 1960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e10709d3-92ff-47c2-8da9-27e20d05e99a": {"__data__": {"id_": "e10709d3-92ff-47c2-8da9-27e20d05e99a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8", "node_type": "1", "metadata": {}, "hash": "03b1f2896e55ef3b8f6fbdae3ba07a7ce31759d72afa364cc9a42db2f38504e5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}}, "text": "Notably, the model does not detect objects beyond the predetermined options\nin the list. Each item in the specified list is assigned a probability score, subsequently employed to\nidentify the presence of objects in the scene and ascertain their respective locations.\n2.3 Prompt Design\n. The GPT-3.5 model [49] is utilized for prompting through an accessible API. The prompt design for\nthe LLM encompasses instructions regarding the available robot skills \u03a0 dand illustrative examples\ndemonstrating the application of these skills in conjunction. This methodology aligns with the few-\nshot prompting technique.", "start_char_idx": 0, "end_char_idx": 609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8": {"__data__": {"id_": "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e10709d3-92ff-47c2-8da9-27e20d05e99a", "node_type": "1", "metadata": {}, "hash": "8426468f595b3ee5efbb038992da0dc251fcbb2b82fb90372edbac528d5996bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43987c9d-fcbe-4f29-bc90-36429a295adb", "node_type": "1", "metadata": {}, "hash": "cedf9092f16b2f6c47c56ededc38c07f479c8dbbc066c878269739ef9c9f0f27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}}, "text": "This methodology aligns with the few-\nshot prompting technique.\nA prompt template is formulated by integrating the set of robot skills \u03a0, the language descriptions\nof these skills \u03a0 d, example plans with corresponding instructions Pe, a given instruction I, and a\ndescription of the current robot location, constituting the state S. These variables are then passed\ninto the LLM plan generation function which is used to generate a robot execution plan \u03c1as shown\nin Figure 3.", "start_char_idx": 546, "end_char_idx": 1020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43987c9d-fcbe-4f29-bc90-36429a295adb": {"__data__": {"id_": "43987c9d-fcbe-4f29-bc90-36429a295adb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8", "node_type": "1", "metadata": {}, "hash": "03b1f2896e55ef3b8f6fbdae3ba07a7ce31759d72afa364cc9a42db2f38504e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "852d47eb-cef2-4b84-9efd-e212ab46cff6", "node_type": "1", "metadata": {}, "hash": "67ed8dd45ee9d1fdaa95ebb04e711e8c2010b803ed23d75a2df419d00a00d41f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}}, "text": "The action scoring approach described in SayCan is not employed; instead, a few-shot\nprompting approach, as outlined in Instruct2Act, is utilized.\n2.4 The Message Stream\nThe incorporation of human interactivity in robot planning is facilitated through the utilisation of\nthe message persistence feature within the OpenAI API. This capability enables the utilisation of a\nthread of conversations within the prompt. New instructions from the user and previously generated\nplans by the LLM are appended to the message stream, as depicted in Figure 1. User messages are\ncategorized into instructions and feedback.", "start_char_idx": 1021, "end_char_idx": 1630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "852d47eb-cef2-4b84-9efd-e212ab46cff6": {"__data__": {"id_": "852d47eb-cef2-4b84-9efd-e212ab46cff6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43987c9d-fcbe-4f29-bc90-36429a295adb", "node_type": "1", "metadata": {}, "hash": "cedf9092f16b2f6c47c56ededc38c07f479c8dbbc066c878269739ef9c9f0f27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1400cf08-5d42-45ab-a44d-1e465d29fd06", "node_type": "1", "metadata": {}, "hash": "b9ca4fb4756075afcad20038f20b28a9139b117a7d0eb06abcc4179b514986dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}}, "text": "User messages are\ncategorized into instructions and feedback. The size of the message stream is constrained by removing\nprior context to avoid exceeding the context window limit.\nThe interactive planning and execution process begins with users providing high-level task descrip-\ntions and instructions in natural language, subsequently processed by the LLM. Plan generation\nbased on this initial input is the next step, where the LLM generates an initial task plan. However,\nthis plan may be incomplete or sub-optimal, which is why users are given the opportunity to provide\nfeedback and adjustment commands through natural language interactions.", "start_char_idx": 1569, "end_char_idx": 2215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1400cf08-5d42-45ab-a44d-1e465d29fd06": {"__data__": {"id_": "1400cf08-5d42-45ab-a44d-1e465d29fd06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "852d47eb-cef2-4b84-9efd-e212ab46cff6", "node_type": "1", "metadata": {}, "hash": "67ed8dd45ee9d1fdaa95ebb04e711e8c2010b803ed23d75a2df419d00a00d41f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "node_type": "1", "metadata": {}, "hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "class_name": "RelatedNodeInfo"}}, "text": "The iterative planning\nprocess continues as the system interprets and acts on user feedback, adjusting the plan until user\nsatisfaction is achieved. The final agreed-upon plan is executed by the robot in the real or simulated\nenvironment.", "start_char_idx": 2216, "end_char_idx": 2454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97a9637e-637b-43a3-9842-34beced18813": {"__data__": {"id_": "97a9637e-637b-43a3-9842-34beced18813", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e16e2fef-45e6-4638-b561-ddd00e0ebee7", "node_type": "1", "metadata": {}, "hash": "d7d7854eef3fd161be99b1c15ecd44115e00458723b60f0f40a88f46926ea5f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}}, "text": "The final agreed-upon plan is executed by the robot in the real or simulated\nenvironment.\n7\n\nFigure 3 : Instructions, skills and their examples and scene descriptions are combined into a single\nprompt and sent to the LLM, the LLM then uses this information to generate the robot execution\nplan.\n2.5 Motion and Manipulation\nA set of rule-based actions is devised for the robot, encompassing operations such as pick-and-place,\npick, move-left, etc. These actions are meticulously crafted and fine-tuned for enhanced accuracy.", "start_char_idx": 0, "end_char_idx": 523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e16e2fef-45e6-4638-b561-ddd00e0ebee7": {"__data__": {"id_": "e16e2fef-45e6-4638-b561-ddd00e0ebee7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97a9637e-637b-43a3-9842-34beced18813", "node_type": "1", "metadata": {}, "hash": "48614cd16bec97836dad5b95c06197d80f37579c1129ff5551b7e22146561300", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2631d6e0-e208-4389-a568-5d229055d8b6", "node_type": "1", "metadata": {}, "hash": "0aa3fedd69ffa975dc8d4964c40f90cd75cee65629da79f28f4e82b62112f4d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}}, "text": "These actions are meticulously crafted and fine-tuned for enhanced accuracy.\nThe functions are accessible through APIs and can be executed sequentially when the robot plan\nis generated. Termed as \u2019skills,\u2019 these actions represent a defined set of activities achievable by the\nrobot.\nThis methodology integrates the components of the research, emphasizing the comprehensive\napproach utilized to facilitate joint robot planning through human-robot interaction with the\nassistance of LLMs.\n3 Experimental Setup and Procedure\nTo empirically assess the effectiveness of the system, a series of experiments were conducted in\nboth simulated and physical environments, each involving a solitary human participant.", "start_char_idx": 447, "end_char_idx": 1152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2631d6e0-e208-4389-a568-5d229055d8b6": {"__data__": {"id_": "2631d6e0-e208-4389-a568-5d229055d8b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e16e2fef-45e6-4638-b561-ddd00e0ebee7", "node_type": "1", "metadata": {}, "hash": "d7d7854eef3fd161be99b1c15ecd44115e00458723b60f0f40a88f46926ea5f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7de4be5c-f717-43bd-b932-844e2b16d00b", "node_type": "1", "metadata": {}, "hash": "3ae53c81581c77be64d487bb916b2b7d6ff393711bd7a5e1a69286ad9be8dd1e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}}, "text": "A terminal\nenvironment was provided to facilitate the transmission of textual instructions and feedback to\nthe system. Throughout the experiment, the human participant interacted with the robot, issuing\nhigh-level natural language instructions, feedback, and adjustment commands. The robot\u2019s language\nunderstanding component, powered by an LLM, interpreted and responded to these instructions\nand commands. The planning and execution process remained dynamic, incorporating iterative plan\nadjustments in response to the singular user\u2019s feedback.\n3.1 Simulated Evaluation\nThe VIMABench is a simulation benchmark of multi-modal prompts for robotics [5].", "start_char_idx": 1153, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7de4be5c-f717-43bd-b932-844e2b16d00b": {"__data__": {"id_": "7de4be5c-f717-43bd-b932-844e2b16d00b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2631d6e0-e208-4389-a568-5d229055d8b6", "node_type": "1", "metadata": {}, "hash": "0aa3fedd69ffa975dc8d4964c40f90cd75cee65629da79f28f4e82b62112f4d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba291a7d-f31c-411c-93a0-8fc765f10b12", "node_type": "1", "metadata": {}, "hash": "99bd8b1f09aab1adca233c6511d6054908f60b57d3f1c95a38f047132da27015", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}}, "text": "It interleaves\ntextual and visual tokens. The benchmark contains thousands of procedural generated table top tasks\nwith multi-modal prompts with systematic evaluation protocols for generalisation.\nSeveral representative meta tasks were chosen from VIMABench, amounting to a total of three\ntasks. Spanning from simple object manipulation to visual reasoning, these tasks were selected to\nassess the proposed methods within the tabletop manipulation domain, as illustrated in Figure 4.\nThe evaluation encompasses the following tasks:\n1.Task 1 Scene Rearrangement : The robot is positioned within a tabletop environment containing\nobjects and receptors.", "start_char_idx": 1805, "end_char_idx": 2455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba291a7d-f31c-411c-93a0-8fc765f10b12": {"__data__": {"id_": "ba291a7d-f31c-411c-93a0-8fc765f10b12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7de4be5c-f717-43bd-b932-844e2b16d00b", "node_type": "1", "metadata": {}, "hash": "3ae53c81581c77be64d487bb916b2b7d6ff393711bd7a5e1a69286ad9be8dd1e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "node_type": "1", "metadata": {}, "hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "class_name": "RelatedNodeInfo"}}, "text": "A target scene image is supplied, prompting the system to reconfigure the\nenvironment to align with the specified target scene.", "start_char_idx": 2456, "end_char_idx": 2583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08fa381b-8c67-4078-b3ca-6d13b5c81bf5": {"__data__": {"id_": "08fa381b-8c67-4078-b3ca-6d13b5c81bf5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "364c343f-141b-46ce-aae0-fb2eb01030c0", "node_type": "1", "metadata": {}, "hash": "feeeb8d26418ed9d6394bd2ac86c43f1f2e3b97bc98a0e05f5f79ab75c854512", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}}, "text": "2.Task 2 Visual Manipulation : The robot is situated within a tabletop environment, furnished\nwith objects and receptors, and directed to manipulate the objects to the receptors in a specified\nsequence.\n8\n\n3.Task 3 Object Rotation : The robot is situated in a tabletop environment, featuring an object,\nand is subjected to a prompt specifying a distinct target angular rotation for the object.\nFigure 4 : The simulated tasks used for evaluation are Task 1: Rearrange Scene, Task 2: Visual\nManipulation, Task 3: Rotate. The image shows an example scene setup and instruction for each task.", "start_char_idx": 0, "end_char_idx": 588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "364c343f-141b-46ce-aae0-fb2eb01030c0": {"__data__": {"id_": "364c343f-141b-46ce-aae0-fb2eb01030c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08fa381b-8c67-4078-b3ca-6d13b5c81bf5", "node_type": "1", "metadata": {}, "hash": "c28d50d68db0b6a5103bff6a3139c74bba43ba788f89d7cad7c231a2e22ca245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9b8f352-4f3f-441a-addd-f009db814b6a", "node_type": "1", "metadata": {}, "hash": "b1810a20499ef513027b8a2a8ab4183faaa074b2c5fa4c673d7c80bb25dc8b37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}}, "text": "The image shows an example scene setup and instruction for each task.\nVIMABench categorizes tasks into partitions, including combinatorial generalization ,place-\nment generalization , and novel object generalization . The benchmark offers two prompt\nconfiguration modes: single modality, involving textual information only, and multi-modality, incor-\nporating both texts and images. Experiments are conducted 10 times for each partition, modality,\nand task, resulting in a total of 60 runs per task.\nIn the simulated environment, the exclusive utilisation of the SAM-CLIP vision system is exam-\nined.", "start_char_idx": 519, "end_char_idx": 1119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9b8f352-4f3f-441a-addd-f009db814b6a": {"__data__": {"id_": "c9b8f352-4f3f-441a-addd-f009db814b6a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "364c343f-141b-46ce-aae0-fb2eb01030c0", "node_type": "1", "metadata": {}, "hash": "feeeb8d26418ed9d6394bd2ac86c43f1f2e3b97bc98a0e05f5f79ab75c854512", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfe44657-e45d-4d05-850e-65108df04c44", "node_type": "1", "metadata": {}, "hash": "353b41a426b7cf03b3d993da237f27c01f92cc479cf855becfc25666dc9ed5d9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}}, "text": "The system is evaluated under two conditions: one without feedback to the LLM post-plan\ngeneration, and another allowing users to provide feedback, which is then incorporated into the\nmessage stream, serving as contextual input for subsequent plan generation by the LLM.\n3.2 Physical Evaluation\nIn the physical experiment, a robotic system is utilized. The experimental setup closely corresponds\nto the simulated environment, as depicted in Figure 5, incorporating a top-view camera for vision\nskills and employing analogous objects and receptors.\nRobot Arm .", "start_char_idx": 1120, "end_char_idx": 1679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfe44657-e45d-4d05-850e-65108df04c44": {"__data__": {"id_": "dfe44657-e45d-4d05-850e-65108df04c44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9b8f352-4f3f-441a-addd-f009db814b6a", "node_type": "1", "metadata": {}, "hash": "b1810a20499ef513027b8a2a8ab4183faaa074b2c5fa4c673d7c80bb25dc8b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dc9fdbf-24a7-4bd6-9bc9-1e2121ffeccd", "node_type": "1", "metadata": {}, "hash": "1d2f644acc70be51507da3b8d99113407e8954150b915c0b22175388c871f516", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}}, "text": "Robot Arm . The Niryo Ned robot is a collaborative robot arm which can be used in both\nresearch and industrial applications [50], this is used as the robot arm for the experiment.The robot\nconnects to a shared network with an Apple M1 MacBook serving as the controller device, equipped\nwith up to 10 CPU cores, a high-performance GPU boasting 16 cores, a Neural Engine, and a Unified\nMemory Architecture with 16GB of RAM, facilitating instruction transmission to the robot arm over\nthe network.", "start_char_idx": 1668, "end_char_idx": 2162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5dc9fdbf-24a7-4bd6-9bc9-1e2121ffeccd": {"__data__": {"id_": "5dc9fdbf-24a7-4bd6-9bc9-1e2121ffeccd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfe44657-e45d-4d05-850e-65108df04c44", "node_type": "1", "metadata": {}, "hash": "353b41a426b7cf03b3d993da237f27c01f92cc479cf855becfc25666dc9ed5d9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "node_type": "1", "metadata": {}, "hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "class_name": "RelatedNodeInfo"}}, "text": "Image and Depth Camera A testbed setup is established for the experiment, incorporating\nthe installation of the Intel RealSense D435i image and depth camera.", "start_char_idx": 2163, "end_char_idx": 2320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a8b215a-9e7a-4085-9a9f-9ca9650f5bde": {"__data__": {"id_": "7a8b215a-9e7a-4085-9a9f-9ca9650f5bde", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b293d45b-6ae1-4bbc-b549-50431cbbddcf", "node_type": "1", "metadata": {}, "hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b293d45b-6ae1-4bbc-b549-50431cbbddcf", "node_type": "1", "metadata": {}, "hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "class_name": "RelatedNodeInfo"}}, "text": "The robot is positioned in\na stable configuration beneath the camera setup to ensure accuracy in pixel-to-coordinate value\ntransformations, as depicted in Figure 5 [51].\nFor the physical evaluation, two tabletop manipulation tasks closely resembling the visual manipu-\nlation task outlined in VIMABench are selected, as depicted in Figure 6. The evaluation encompasses\nthe following tasks:\n1.Task A Pick and Place Single Object : In this task, the robot is positioned within a scene featuring\na solitary Lego block positioned between two receptors\u2014a green box and a blue box.", "start_char_idx": 0, "end_char_idx": 575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "569fcf26-224b-459a-9c7a-28d69b951b76": {"__data__": {"id_": "569fcf26-224b-459a-9c7a-28d69b951b76", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d1dc7fb-4980-4e57-9586-804dba9f54f0", "node_type": "1", "metadata": {}, "hash": "4b678d8932d9be538acf4c3613dad8bc821424ab755fbe1b390d0057e9fe55e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}}, "text": "The system\nis directed to position the block in a designated receptor, for example, \u2019 place the blue block in the\ngreen bowl \u2019.\n2.Task B Pick and Place Multiple Objects :In this task, the robot is situated within a scene compris-\ning multiple Lego blocks and two receptors\u2014a green box and a blue box. The system is directed to\narrange multiple items into distinct receptors in a predetermined order, for instance, placing the\nred block in the blue bowl, followed by positioning the green block in the correspondingly colored\nbowl.", "start_char_idx": 0, "end_char_idx": 530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d1dc7fb-4980-4e57-9586-804dba9f54f0": {"__data__": {"id_": "6d1dc7fb-4980-4e57-9586-804dba9f54f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "569fcf26-224b-459a-9c7a-28d69b951b76", "node_type": "1", "metadata": {}, "hash": "7e87d9593d814155c7d48f91f85a9bbb2cbba63fedf84a142e0360962dede27d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2808af67-c8e9-418e-90f5-e2906405b7f2", "node_type": "1", "metadata": {}, "hash": "6431444ec23e4c4b7d1520afe373af05ba2aeb92af886656d63c8351fe26c70b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}}, "text": "i.e \u2019 put the red block in the blue bowl, then the green block in the matching colored bowl \u2019.\n9\n\nOur system is evaluated on 3 variations: plan generation with feedback, plan generation without\nfeedback and plan generation without scene description.\nFigure 5 : The experimental setup comprises a Niryo Ned robot arm, an Intel RealSense D435i\ndepth camera, and colored bowls (green and blue) serving as receptors, along with a collection of\nmulticolored Lego blocks.\nThe system\u2019s performance is assessed independently using SAM-CLiP and ViLD vision systems.", "start_char_idx": 531, "end_char_idx": 1087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2808af67-c8e9-418e-90f5-e2906405b7f2": {"__data__": {"id_": "2808af67-c8e9-418e-90f5-e2906405b7f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d1dc7fb-4980-4e57-9586-804dba9f54f0", "node_type": "1", "metadata": {}, "hash": "4b678d8932d9be538acf4c3613dad8bc821424ab755fbe1b390d0057e9fe55e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "299d1b08-d852-4fe2-9474-826673130b7a", "node_type": "1", "metadata": {}, "hash": "13c85c0f682d630555b16d79c23b531d82b8b99006700cbcc636ea1fa81a432e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}}, "text": "The system\u2019s performance is assessed independently using SAM-CLiP and ViLD vision systems.\nEach task within every system variation is executed 10 times for each vision model, resulting in a\ntotal of 40 tests for each system configuration.\nThe quantitative metric employed for gauging the performance of the various approaches is the\ntask success rate.\nFigure 6 : The simulated environment features a top view camera which is used for detecting object\nsin the scene. The physical environment is configured to closely emulate the settings utilized in the\nsimulated evaluation, employing analogous objects and configurations.", "start_char_idx": 997, "end_char_idx": 1619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "299d1b08-d852-4fe2-9474-826673130b7a": {"__data__": {"id_": "299d1b08-d852-4fe2-9474-826673130b7a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2808af67-c8e9-418e-90f5-e2906405b7f2", "node_type": "1", "metadata": {}, "hash": "6431444ec23e4c4b7d1520afe373af05ba2aeb92af886656d63c8351fe26c70b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91a84ae8-5bd9-40b0-96f7-ec19ecba5e7f", "node_type": "1", "metadata": {}, "hash": "b430f2a9c249f8dc3c94d777b6dd8230f8eb62d536c24635ab6b5a5edad3bbd2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}}, "text": "4 Results\nTable 2 demonstrates the system\u2019s performance in executing scene rearrangement tasks prompted\nthrough single and multi-modal configurations. Notably, the system exhibits an inability to success-\nfully execute any scene rearrangement tasks when prompted with a single modality. This limitation\narises from the absence of contextual information, specifically the target scene image, in single-modal\nprompts.\nAs highlighted in Table 2, the introduction of feedback provides a 18.5% increase in multi modality\ntasks, and a 2.5% performance drop over the baseline on single modality tasks.", "start_char_idx": 1620, "end_char_idx": 2214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91a84ae8-5bd9-40b0-96f7-ec19ecba5e7f": {"__data__": {"id_": "91a84ae8-5bd9-40b0-96f7-ec19ecba5e7f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "299d1b08-d852-4fe2-9474-826673130b7a", "node_type": "1", "metadata": {}, "hash": "13c85c0f682d630555b16d79c23b531d82b8b99006700cbcc636ea1fa81a432e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "00821df1-46e5-4f24-9733-627c63fa7193", "node_type": "1", "metadata": {}, "hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "class_name": "RelatedNodeInfo"}}, "text": "This improvement is\nattributed to the heightened complexity in generating plans for multi-modal prompts.\nIn the context of the visual manipulation task (Task 2), the base implementation surpasses the\nfeedback system.", "start_char_idx": 2215, "end_char_idx": 2431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97a1dcc6-a59a-4661-8ffc-9b545bf9dd76": {"__data__": {"id_": "97a1dcc6-a59a-4661-8ffc-9b545bf9dd76", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f71aadfd-9c6b-45bd-a77b-94f1df020654", "node_type": "1", "metadata": {}, "hash": "e0d4061533952de6eddedc02c9fe4b3218fb1c3d8c702762fc97373d4ccfcc5b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}}, "text": "This observation raises the suspicion that discrepancies in task execution accuracy,\nrather than plan generation, may underlie the system\u2019s comparative performance. Negative results are\nobserved primarily in tasks prompted with a single modality, aligning with the inherent limitations\nassociated with insufficient contextual information.\nObservations during experiments reveal consistent plan generation errors by the model for specific\ninstructions, and even after feedback is provided for a particular mistake,", "start_char_idx": 0, "end_char_idx": 513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f71aadfd-9c6b-45bd-a77b-94f1df020654": {"__data__": {"id_": "f71aadfd-9c6b-45bd-a77b-94f1df020654", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97a1dcc6-a59a-4661-8ffc-9b545bf9dd76", "node_type": "1", "metadata": {}, "hash": "9e0ac55c08a531f2659f3b5c401a2c1ab6927fc3a15cfc88eb91ab0a3f08b98d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d78b9fc-b73f-438e-87b2-a96cc536ae59", "node_type": "1", "metadata": {}, "hash": "4c319aa84e073dbc4c6849db4ca8ec39e60134a9f9cf9cb32f53f1e7871057c7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}}, "text": "and even after feedback is provided for a particular mistake, the model tends to repeat\n10\n\nTable 2 : VIMABench simulated environment evaluation\nSingle modality1Multi modality2\nProject Task 1 Task 2 Task 3 Task 1 Task 2 Task 3\nInstruct2Act 0.0% 70% 90% 40% 63.3% 76.66%\nInstruct2Act w/Feedback 0.0% 66% 90% 46.66% 73.33% 93.", "start_char_idx": 452, "end_char_idx": 776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d78b9fc-b73f-438e-87b2-a96cc536ae59": {"__data__": {"id_": "0d78b9fc-b73f-438e-87b2-a96cc536ae59", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f71aadfd-9c6b-45bd-a77b-94f1df020654", "node_type": "1", "metadata": {}, "hash": "e0d4061533952de6eddedc02c9fe4b3218fb1c3d8c702762fc97373d4ccfcc5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "327e5dac-8077-4076-bb6c-a583b893c8a3", "node_type": "1", "metadata": {}, "hash": "cb7afd3cdaf26a7eb75fcad8b4d6b54fdc43a20e0d572a0d3829fa0ad2ca9a1a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}}, "text": "66% 73.33% 93.3%\nNote: The success rate is calculated by finding the average success of task completion for each task across the 3\npartitions.\n1For single modality only textual data is provided to the context of the model.\n2For multi modality both texts and images are provided as contexts to the model.\nthese errors once the message stream is cleared. This recurrence is attributed to the absence of a\nmemory system that would retain feedback and context across experiments.\nThe language model demonstrates generalisation to new objects and employs novel approaches in\ncalling the APIs beyond the provided examples.", "start_char_idx": 762, "end_char_idx": 1378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "327e5dac-8077-4076-bb6c-a583b893c8a3": {"__data__": {"id_": "327e5dac-8077-4076-bb6c-a583b893c8a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d78b9fc-b73f-438e-87b2-a96cc536ae59", "node_type": "1", "metadata": {}, "hash": "4c319aa84e073dbc4c6849db4ca8ec39e60134a9f9cf9cb32f53f1e7871057c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfb015cc-9ce0-4787-b95f-c36bc0299591", "node_type": "1", "metadata": {}, "hash": "94175c9d7b2ee891b0a49c0da989de3fc6c4fc46704d187710aadd2728bd1188", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}}, "text": "Although infrequent, the model may respond with\nnatural language outputs outside the defined API, and providing feedback instructions corrects such\noccurrences.\nFigure 7 : The human user provides the instruction \u2019 place the green block in the matching colored\nbowl\u2019. The LLM uses this instruction to generate a plan which is then executed by the physical robot\nTable 3 presents the outcomes of the physical evaluation conducted on the system. The findings\nindicate that the incorporation of scene descriptions yields an 8.5% reduction to system success rate\nwhile adding an average of 7 seconds to the overall system execution time.", "start_char_idx": 1379, "end_char_idx": 2011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfb015cc-9ce0-4787-b95f-c36bc0299591": {"__data__": {"id_": "dfb015cc-9ce0-4787-b95f-c36bc0299591", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "327e5dac-8077-4076-bb6c-a583b893c8a3", "node_type": "1", "metadata": {}, "hash": "cb7afd3cdaf26a7eb75fcad8b4d6b54fdc43a20e0d572a0d3829fa0ad2ca9a1a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "node_type": "1", "metadata": {}, "hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "class_name": "RelatedNodeInfo"}}, "text": "For these reasons the\nscene description module may be considered dispensable in future iterations.\nFurthermore, discernible variations in success rates among zero-shot object detection models\nare observed. The ViLD model excels in accurately locating specified items within the scene but\nencounters challenges in distinguishing color differences between objects, as illustrated in Figure 9.", "start_char_idx": 2012, "end_char_idx": 2402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44c18575-34fe-4375-9c77-1ef9114c4c7b": {"__data__": {"id_": "44c18575-34fe-4375-9c77-1ef9114c4c7b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f", "node_type": "1", "metadata": {}, "hash": "bcb1d7eb7f29940126ede54e55cc7cdbc87bd4e1e3123b988201dcd5519d4476", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}}, "text": "Consequently, the model demonstrates inaccuracies, notably in predicting instances where multi-\nple objects of the same type but in different colors coexist, sometimes failing entirely to identify\nany instance of the object, as exemplified in query 2 of Figure 9. In contrast, Figure 10 illustrates\nthat the SAM-CLiP approach exhibits superior color identification capabilities.", "start_char_idx": 0, "end_char_idx": 378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f": {"__data__": {"id_": "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44c18575-34fe-4375-9c77-1ef9114c4c7b", "node_type": "1", "metadata": {}, "hash": "ce7c00aa9f6fa4986f628d9164cd7d1dca495272067ad3f72c40696b07edd459", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03f0766f-3c01-439f-9584-20eb51f4c2d8", "node_type": "1", "metadata": {}, "hash": "0b9dcaa8ae594c5dbd668b55433c58ceae46a5b2cf0adbace3250252fbd4e28a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}}, "text": "However, it is asso-\nciated with a significantly higher latency; during our experimental evaluations, the ViLD approach\ntook an average of 15 seconds for inference, while the SAM-CLiP approach took an average of 144\nseconds. Although the SAM-CLiP approach achieves heightened accuracy in detecting objects with\ndiverse colors, it occasionally groups disparate objects together if they share the same color\u2014such\nas categorizing a square green container as a green block, as demonstrated in query 1 in Figure 10.", "start_char_idx": 379, "end_char_idx": 889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03f0766f-3c01-439f-9584-20eb51f4c2d8": {"__data__": {"id_": "03f0766f-3c01-439f-9584-20eb51f4c2d8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f", "node_type": "1", "metadata": {}, "hash": "bcb1d7eb7f29940126ede54e55cc7cdbc87bd4e1e3123b988201dcd5519d4476", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c15d4371-261d-44c7-91d9-b2f2cff903f1", "node_type": "1", "metadata": {}, "hash": "9a486ed878a6ba424e35b3f0874b45e56e14635cfd8f81106354241826d2fd4f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}}, "text": "Observations indicate that zero-shot object detection models exhibit superior performance on\nsimulation images within the benchmark environment compared to real-world images tested on an\nactual robot setup. The accuracy of the detection models does not achieve commensurate levels when\n11\n\nFigure 8 : The human user has provided a prompt to place the green block in the blue bowl . The\nLLM uses the instruction provided and the affordance skills available to generate the plan which is\nthen executed by the robot\nFigure 9 : Zero-shot object recognition employing ViLD is presented in the diagram.", "start_char_idx": 890, "end_char_idx": 1486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c15d4371-261d-44c7-91d9-b2f2cff903f1": {"__data__": {"id_": "c15d4371-261d-44c7-91d9-b2f2cff903f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03f0766f-3c01-439f-9584-20eb51f4c2d8", "node_type": "1", "metadata": {}, "hash": "0b9dcaa8ae594c5dbd668b55433c58ceae46a5b2cf0adbace3250252fbd4e28a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98760a13-7772-4e2d-937c-4aa36a6342d9", "node_type": "1", "metadata": {}, "hash": "f54816904dcd3562a9dd50465272063930fc5fa2bd8e3fdd4a2fe55d8f0ede55", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}}, "text": "The upper\nimage depicts the original scene, and the lower image exhibits a green bounding box drawn around\nthe identified target object. The diagram illustrates the algorithm\u2019s inability to identify query 2\nandquery 3 , accompanied by an erroneous prediction for query 4 .\nFigure 10 : Zero-shot object recognition using SAM-CLiP is depicted in the diagram. The upper\nimage represents the original scene, while the lower image displays a green bounding box drawn\naround the identified target object. The illustration demonstrates the model\u2019s accurate recognition\nofquery 2 ,query 3 andquery 4 .", "start_char_idx": 1487, "end_char_idx": 2080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98760a13-7772-4e2d-937c-4aa36a6342d9": {"__data__": {"id_": "98760a13-7772-4e2d-937c-4aa36a6342d9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c15d4371-261d-44c7-91d9-b2f2cff903f1", "node_type": "1", "metadata": {}, "hash": "9a486ed878a6ba424e35b3f0874b45e56e14635cfd8f81106354241826d2fd4f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "node_type": "1", "metadata": {}, "hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "class_name": "RelatedNodeInfo"}}, "text": "However, it exhibits a failure in identifying the object in query\n1, as it misidentifies the green box as the green block.", "start_char_idx": 2081, "end_char_idx": 2203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb04e912-9cbc-4e1c-86ac-d85de4ccff08": {"__data__": {"id_": "eb04e912-9cbc-4e1c-86ac-d85de4ccff08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954", "node_type": "1", "metadata": {}, "hash": "86cbff7ce78b43a24c4db9b3f7375685af462134cfe22ec2e608d287f08fb4a9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}}, "text": "12\n\nTable 3 : Physical robot environment evaluation\nViLD1SAM-CLiP2\nMethodology Task A Task B Task A Task B\nw/Feedback 60% 40% 60% 80%\nw/o Feedback 20% 40% 70% 80%\nw/o Scene Description 30% 40% 80% 70%\nNote: Each task is executed 10 times on the physical robot within the environment setup.\n1For the ViLD system the location of the receptors were hard-coded because the model was having difficulty identifying\nthe receptors in the scene.", "start_char_idx": 0, "end_char_idx": 436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954": {"__data__": {"id_": "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb04e912-9cbc-4e1c-86ac-d85de4ccff08", "node_type": "1", "metadata": {}, "hash": "d1312d5238dc41d2cbccad134c6baa007e6662a8fe4daf00fb72cb9878f183d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04651f94-3b5b-4c58-b540-df3a6f9bc4d1", "node_type": "1", "metadata": {}, "hash": "b9b79ab6674569f0567521af59d5a561b06c0671f5b552d130147cca7d87429f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}}, "text": "2The SAM-CLiP model used in the physical robot is the exact same as used in the VIMABench simulated environment.\nconfronted with real-world embodied images. This suggests a potential barrier to the practical appli-\ncation of this approach in real-world scenarios, necessitating the development of zero-shot learning\ndetection models tailored for real-world embodied images.\n5 Discussion\nThe presented research introduces a methodology to facilitate collaborative robot planning through\nhuman-robot interaction, harnessing LLMs. Subsequent evaluations were conducted in a simulated\nrobotics benchmark and on a physical robot arm setup.", "start_char_idx": 437, "end_char_idx": 1071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04651f94-3b5b-4c58-b540-df3a6f9bc4d1": {"__data__": {"id_": "04651f94-3b5b-4c58-b540-df3a6f9bc4d1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954", "node_type": "1", "metadata": {}, "hash": "86cbff7ce78b43a24c4db9b3f7375685af462134cfe22ec2e608d287f08fb4a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a2c72d9-1e14-4918-bfa0-042c67e7dfee", "node_type": "1", "metadata": {}, "hash": "7233a467228b714f73d823314517c2ca65cfcaa6864be66d41ab6c1349f546a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}}, "text": "The ensuing discussion outlines significant\nfindings and insights gleaned from the experiments.\nThe proficient completion of object manipulation tasks by the robot arm, guided by human\nusers, underscores the efficacy of the approach. Leveraging language understanding and generation\ncomponents based on LLMs facilitated natural language communication, empowering users to issue\nhigh-level commands, deliver feedback, and optimize task plans dynamically during execution. This\nadaptability stands out as a pivotal strength of the methodology, indicative of the potential for\nheightened user agency in the realm of human-robot collaboration.", "start_char_idx": 1072, "end_char_idx": 1711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a2c72d9-1e14-4918-bfa0-042c67e7dfee": {"__data__": {"id_": "3a2c72d9-1e14-4918-bfa0-042c67e7dfee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04651f94-3b5b-4c58-b540-df3a6f9bc4d1", "node_type": "1", "metadata": {}, "hash": "b9b79ab6674569f0567521af59d5a561b06c0671f5b552d130147cca7d87429f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "node_type": "1", "metadata": {}, "hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "class_name": "RelatedNodeInfo"}}, "text": "The system demonstrated its capacity to understand and execute a variety of user instructions,\nsuch as placing and relocating objects. While these set of manipulations are limited this system can\nbe extended to support more complex skills that can be composed together to achieve a larger set\nof tasks.\nLeveraging the VIMABench environment for bench-marking provided valuable insights into the\nsystem\u2019s performance, enabling an evaluation of its proficiency in comprehending and responding to\na diverse range of language instructions. The environment wrapper facilitated real-time feedback and\nadjustment, aligning the approach with principles of adaptability and continuous improvement.", "start_char_idx": 1712, "end_char_idx": 2399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "194e47d5-07a0-403c-be14-d68931a214fd": {"__data__": {"id_": "194e47d5-07a0-403c-be14-d68931a214fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf72c77f-d210-49e0-9fbc-868444937960", "node_type": "1", "metadata": {}, "hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf72c77f-d210-49e0-9fbc-868444937960", "node_type": "1", "metadata": {}, "hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "class_name": "RelatedNodeInfo"}}, "text": "In quantitative metrics, the system demonstrated 8.6% performance increase in simulated eval-\nuations and a 14% increase in physical evaluations. This shows that the method can be further\nexplored to enable collaborative task planning between humans and robots in physical and virtual\nenvironments.\nHowever, there are areas for improvement. As with many language models, the system\u2019s under-\nstanding and generation capabilities may occasionally exhibit limitations.", "start_char_idx": 0, "end_char_idx": 465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "865e3761-95fb-4411-91d9-ac4d0dfcd283": {"__data__": {"id_": "865e3761-95fb-4411-91d9-ac4d0dfcd283", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35ef8e42-ebd3-4e50-a73a-7713e2e392bf", "node_type": "1", "metadata": {}, "hash": "6e827ed0b282c86f15af3feab69830d6a9dd1c0dbc1f34fd66835e5cdc2f9590", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}}, "text": "There is a need to address\nambiguities in user instructions and improve the system\u2019s capacity to ask clarifying questions when\nencountering unclear or contradictory language input.\nAdditionally, the study emphasizes the need for computational optimization, aligning with prior\nresearch on the computational constraints of language models, to enhance efficiency for deployment\nin real robotic systems.\n6 Conclusion\nThis research presents a system utilizing LLMs to facilitate collaborative robot planning through\nhuman-robot interaction.", "start_char_idx": 0, "end_char_idx": 536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35ef8e42-ebd3-4e50-a73a-7713e2e392bf": {"__data__": {"id_": "35ef8e42-ebd3-4e50-a73a-7713e2e392bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "865e3761-95fb-4411-91d9-ac4d0dfcd283", "node_type": "1", "metadata": {}, "hash": "249e92979f223a88197eb309e23eba1fd65a0a63e8300f295dfbe1cde2df86ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ed73b61-29d1-4d79-a72f-31b0ba4720db", "node_type": "1", "metadata": {}, "hash": "0ae7eedeb692c45a07c9cde61cee9d91230660fb218cb8eaa3ee913ade2cb584", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}}, "text": "The amalgamation of language models with robot planning and execution\nintroduces compelling opportunities and challenges, paving the way for the evolution of intelligent\nand interactive robots. Previous studies have demonstrated LLMs\u2019 potential as generalist planners\nfor robot task execution in natural language. This research aims to contribute by transforming LLM-\ngenerated plans into an interactive process, fostering continuous interaction between the user and\nthe robot through the LLM.\n13\n\nProspective avenues for the project involve exploring the involvement of other AI language mod-\nels in evaluating plans generated by the LLM and providing iterative feedback until successful task\ncompletion.", "start_char_idx": 537, "end_char_idx": 1242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ed73b61-29d1-4d79-a72f-31b0ba4720db": {"__data__": {"id_": "9ed73b61-29d1-4d79-a72f-31b0ba4720db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35ef8e42-ebd3-4e50-a73a-7713e2e392bf", "node_type": "1", "metadata": {}, "hash": "6e827ed0b282c86f15af3feab69830d6a9dd1c0dbc1f34fd66835e5cdc2f9590", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93be767d-58e9-4eb5-b5a2-ad7da310f65a", "node_type": "1", "metadata": {}, "hash": "9764ff814aa500b14c71356f01880c6f2af4a672e1eba723a4507abd8d97164c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}}, "text": "Alternatively, implementing a comprehensive memory system for the Robot LLM, akin\nto Generative Agents\u2019 [56] work, could enhance the LLM\u2019s ability to actively learn from these\ninteractions.\nReferences\n[1] Huang, S., Jiang, Z., Dong, H., Qiao, Y., Gao, P. and Li, H. (2023). Instruct2Act: Mapping Multi-\nmodality Instructions to Robotic Actions with Large Language Model. arXiv (Cornell University).\ndoi:https://doi.org/10.48550/arxiv.2305.11176.", "start_char_idx": 1243, "end_char_idx": 1688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93be767d-58e9-4eb5-b5a2-ad7da310f65a": {"__data__": {"id_": "93be767d-58e9-4eb5-b5a2-ad7da310f65a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ed73b61-29d1-4d79-a72f-31b0ba4720db", "node_type": "1", "metadata": {}, "hash": "0ae7eedeb692c45a07c9cde61cee9d91230660fb218cb8eaa3ee913ade2cb584", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47d6cb48-779a-494b-975f-025463507e59", "node_type": "1", "metadata": {}, "hash": "210a83a2134c257f1ce783ed19e8e3eac631dfd168c568a856212687b72e360b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}}, "text": "doi:https://doi.org/10.48550/arxiv.2305.11176.\n[2] Wu, J., Antonova, R., Kan, A., Lepert, M., Zeng, A., Song, S., Bohg, J., Rusinkiewicz, S. and\nFunkhouser, T., 2023. Tidybot: Personalized robot assistance with large language models. arXiv\npreprint arXiv:2305.05658.", "start_char_idx": 1642, "end_char_idx": 1908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47d6cb48-779a-494b-975f-025463507e59": {"__data__": {"id_": "47d6cb48-779a-494b-975f-025463507e59", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93be767d-58e9-4eb5-b5a2-ad7da310f65a", "node_type": "1", "metadata": {}, "hash": "9764ff814aa500b14c71356f01880c6f2af4a672e1eba723a4507abd8d97164c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31", "node_type": "1", "metadata": {}, "hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "class_name": "RelatedNodeInfo"}}, "text": "arXiv\npreprint arXiv:2305.05658.\n[3] A. I. K\u00b4 aroly, P. Galambos, J. Kuti and I. J. Rudas, \u201dDeep Learning in Robotics: Survey on Model\nStructures and Training Strategies,\u201d in IEEE Transactions on Systems, Man, and Cybernetics:\nSystems, vol. 51, no.", "start_char_idx": 1876, "end_char_idx": 2124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "030bf9d4-f9b9-4807-a491-0d4610cefc62": {"__data__": {"id_": "030bf9d4-f9b9-4807-a491-0d4610cefc62", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760", "node_type": "1", "metadata": {}, "hash": "c00d28ffe836ebd6e04fce57a6a04c55006e7fa9ddb04017304f1a79018364c4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}}, "text": "51, no. 1, pp. 266-279, Jan. 2021, doi: 10.1109/TSMC.2020.3018325.\n[4] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson,\nJ., Vuong, Q., Yu, T. and Huang, W., 2023. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378.", "start_char_idx": 0, "end_char_idx": 287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760": {"__data__": {"id_": "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "030bf9d4-f9b9-4807-a491-0d4610cefc62", "node_type": "1", "metadata": {}, "hash": "5d1bc46103b700e7ccf868c592a49bc69875078abd29799b2238815e343fe9c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "372512df-6ff4-4419-99e5-ad456f68de6b", "node_type": "1", "metadata": {}, "hash": "1aa6f5d6cac54932996d81e12eb65a20de89b2c694df5d192ad4b54ae2d36a7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2303.03378.\n[5] Kovalev, A.K., Panov, A.I. Application of Pretrained Large Language Models\nin Embodied Artificial Intelligence. Dokl. Math. 106 (Suppl 1), S85\u2013S90 (2022).", "start_char_idx": 255, "end_char_idx": 446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "372512df-6ff4-4419-99e5-ad456f68de6b": {"__data__": {"id_": "372512df-6ff4-4419-99e5-ad456f68de6b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760", "node_type": "1", "metadata": {}, "hash": "c00d28ffe836ebd6e04fce57a6a04c55006e7fa9ddb04017304f1a79018364c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977", "node_type": "1", "metadata": {}, "hash": "4e5528fee7d90b6129d8e3fcdd26cae5576a02fd5d78722bf5631e35307bec2c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}}, "text": "Math. 106 (Suppl 1), S85\u2013S90 (2022).\nhttps://doi.org/10.1134/S1064562422060138\n[6] Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J. and\nGarg, A., 2023, May. Progprompt: Generating situated robot task plans using large language\nmodels. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp.", "start_char_idx": 410, "end_char_idx": 765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977": {"__data__": {"id_": "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "372512df-6ff4-4419-99e5-ad456f68de6b", "node_type": "1", "metadata": {}, "hash": "1aa6f5d6cac54932996d81e12eb65a20de89b2c694df5d192ad4b54ae2d36a7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "646b8540-05fd-4678-aff4-24dbf948a091", "node_type": "1", "metadata": {}, "hash": "e98b70145d33dbe10dee7d7de26e8eb394f414e3e4805701777d28882882451f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}}, "text": "In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 11523-\n11530). IEEE.\n[7] Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P. and Zeng, A.,\n2023, May. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) (pp. 9493-9500). IEEE.", "start_char_idx": 689, "end_char_idx": 1051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "646b8540-05fd-4678-aff4-24dbf948a091": {"__data__": {"id_": "646b8540-05fd-4678-aff4-24dbf948a091", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977", "node_type": "1", "metadata": {}, "hash": "4e5528fee7d90b6129d8e3fcdd26cae5576a02fd5d78722bf5631e35307bec2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ffe4f91-6837-4828-b359-5f577da1ff25", "node_type": "1", "metadata": {}, "hash": "a83d3cab9bb95b9b881b6c56908f1d1499c1cbecf192617b2b8bb43200034206", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}}, "text": "9493-9500). IEEE.\n[8] Huang, W., Abbeel, P., Pathak, D. and Mordatch, I., 2022, June. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning (pp. 9118-9147). PMLR.\n[9] Zhao, X., Li, M., Weber, C., Hafez, M.B. and Wermter, S., 2023.", "start_char_idx": 1034, "end_char_idx": 1350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ffe4f91-6837-4828-b359-5f577da1ff25": {"__data__": {"id_": "8ffe4f91-6837-4828-b359-5f577da1ff25", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "646b8540-05fd-4678-aff4-24dbf948a091", "node_type": "1", "metadata": {}, "hash": "e98b70145d33dbe10dee7d7de26e8eb394f414e3e4805701777d28882882451f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "node_type": "1", "metadata": {}, "hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "class_name": "RelatedNodeInfo"}}, "text": "and Wermter, S., 2023. Chat with the environment:\nInteractive multimodal perception using large language models.", "start_char_idx": 1328, "end_char_idx": 1440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4506df3-328d-4f3b-9c4e-abfe21c4b0e3": {"__data__": {"id_": "f4506df3-328d-4f3b-9c4e-abfe21c4b0e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77a457c2-7d13-4288-abbc-dcf01a46b622", "node_type": "1", "metadata": {}, "hash": "5e120b1f46ebc87188435070c95b0b660fa1395e8a0d26d456b0522955bd7cfe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}}, "text": "Chat with the environment:\nInteractive multimodal perception using large language models. arXiv preprint arXiv:2303.08268.\n[10] Zhang, B. and Soh, H., 2023. Large language models as zero-shot human models for human-robot\ninteraction. arXiv preprint arXiv:2303.03548.\n[11] Xie, Y., Yu, C., Zhu, T., Bai, J., Gong, Z. and Soh, H., 2023.", "start_char_idx": 0, "end_char_idx": 334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77a457c2-7d13-4288-abbc-dcf01a46b622": {"__data__": {"id_": "77a457c2-7d13-4288-abbc-dcf01a46b622", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4506df3-328d-4f3b-9c4e-abfe21c4b0e3", "node_type": "1", "metadata": {}, "hash": "8d24655c3c289158f664a09c2c8ffd76d1cd10e78accf706b5b9d49283566200", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d14b14ed-9d86-4380-afb6-492ca0707b9e", "node_type": "1", "metadata": {}, "hash": "2b09bca843911747300ed1c764bf8d0abcf7e9583f0e82bbe10fe6f99c0fa1ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}}, "text": "Translating natural language to\nplanning goals with large-language models. arXiv preprint arXiv:2302.05128.\n[12] Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J. and Stone, P., 2023. Llm+ p: Empow-\nering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.", "start_char_idx": 335, "end_char_idx": 641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d14b14ed-9d86-4380-afb6-492ca0707b9e": {"__data__": {"id_": "d14b14ed-9d86-4380-afb6-492ca0707b9e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77a457c2-7d13-4288-abbc-dcf01a46b622", "node_type": "1", "metadata": {}, "hash": "5e120b1f46ebc87188435070c95b0b660fa1395e8a0d26d456b0522955bd7cfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "762eeb22-f099-43ce-97a4-4489a833421f", "node_type": "1", "metadata": {}, "hash": "dc5ca7518521c3832b86bcc99698c947c581f4a3028333bc9f0d6d883fe76698", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2304.11477.\n[13] Wake, N., Kanehara, A., Sasabuchi, K., Takamatsu, J. and Ikeuchi, K., 2023. Chatgpt\nempowered long-step robot control in various environments: A case application. arXiv preprint\narXiv:2304.03893\n[14] Ye, F., Zhang, S., Wang, P. and Chan, C.-Y. (2021).", "start_char_idx": 609, "end_char_idx": 898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "762eeb22-f099-43ce-97a4-4489a833421f": {"__data__": {"id_": "762eeb22-f099-43ce-97a4-4489a833421f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d14b14ed-9d86-4380-afb6-492ca0707b9e", "node_type": "1", "metadata": {}, "hash": "2b09bca843911747300ed1c764bf8d0abcf7e9583f0e82bbe10fe6f99c0fa1ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83c13dfa-f71b-4b13-948a-39bb24be72c7", "node_type": "1", "metadata": {}, "hash": "b16b63256fa2713297a89e4bdfac015ffc59305a2d1d2be13339825b5432df4d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}}, "text": "(2021). A Survey of Deep Reinforce-\nment Learning Algorithms for Motion Planning and Control of Autonomous Vehicles.\ndoi:https://doi.org/10.1109/iv48863.2021.9575880.\n14\n\n[15] Hadi, Muhammad Usman; tashi, qasem al; Qureshi, Rizwan; Shah, Abbas; muneer,\namgad; Irfan, Muhammad; et al. (2023). Large Language Models: A Comprehensive Sur-\nvey of its Applications, Challenges, Limitations, and Future Prospects.", "start_char_idx": 891, "end_char_idx": 1298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83c13dfa-f71b-4b13-948a-39bb24be72c7": {"__data__": {"id_": "83c13dfa-f71b-4b13-948a-39bb24be72c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "762eeb22-f099-43ce-97a4-4489a833421f", "node_type": "1", "metadata": {}, "hash": "dc5ca7518521c3832b86bcc99698c947c581f4a3028333bc9f0d6d883fe76698", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e7643626-84dd-4b5c-9902-528a44aea517", "node_type": "1", "metadata": {}, "hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "class_name": "RelatedNodeInfo"}}, "text": "TechRxiv. Preprint.\nhttps://doi.org/10.36227/techrxiv.23589741.v3\n[16] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. and Cao, Y., 2022.", "start_char_idx": 1299, "end_char_idx": 1451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96132b0f-5d0c-4595-846a-dfaa5aa4d96c": {"__data__": {"id_": "96132b0f-5d0c-4595-846a-dfaa5aa4d96c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "099d8859-ce61-4540-ab3f-9498a5174a42", "node_type": "1", "metadata": {}, "hash": "3ff518db952502edde0416aee8616833cfa722f454b3cc8b4263372b1eddcc7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}}, "text": "React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\n[17] Lykov, A. and Tsetserukou, D., 2023. LLM-BRAIn: AI-driven Fast Generation of Robot\nBehaviour Tree based on Large Language Model. arXiv preprint arXiv:2305.19352.\n[18] Ding, Y., Zhang, X., Amiri, S., Cao, N., Yang, H., Esselink, C. and Zhang, S., 2022.", "start_char_idx": 0, "end_char_idx": 349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "099d8859-ce61-4540-ab3f-9498a5174a42": {"__data__": {"id_": "099d8859-ce61-4540-ab3f-9498a5174a42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96132b0f-5d0c-4595-846a-dfaa5aa4d96c", "node_type": "1", "metadata": {}, "hash": "0d1fba54a9b9ef2cdcb85a879c0ae852119619ce73615052558363bb156722b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce79aab2-672c-4686-9725-b0a6eb169b08", "node_type": "1", "metadata": {}, "hash": "d5ffcb4343736012565e71ba7c6c0a63eb17c35a7dcaa8285cdd9dd2bcce0f6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}}, "text": "Robot task\nplanning and situation handling in open worlds. arXiv preprint arXiv:2210.01287.\n[19] Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.H., Arenas, M.G., Chiang, H.T.L., Erez,\nT., Hasenclever, L., Humplik, J. and Ichter, B., 2023. Language to Rewards for Robotic Skill\nSynthesis. arXiv preprint arXiv:2306.08647.", "start_char_idx": 350, "end_char_idx": 672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce79aab2-672c-4686-9725-b0a6eb169b08": {"__data__": {"id_": "ce79aab2-672c-4686-9725-b0a6eb169b08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "099d8859-ce61-4540-ab3f-9498a5174a42", "node_type": "1", "metadata": {}, "hash": "3ff518db952502edde0416aee8616833cfa722f454b3cc8b4263372b1eddcc7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88de296f-fdfe-4280-a61d-295f376a5dfa", "node_type": "1", "metadata": {}, "hash": "ee6d58168312ef6393bac703e4c688ebf6eff5af80b5c5ffd731465e8c9346ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2306.08647.\n[20] Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A.,\nZhu, Y. and Fan, L., 2022. Vima: General robot manipulation with multimodal prompts. arXiv\npreprint arXiv:2210.03094.\n[21] Ding, Y., Zhang, X., Paxton, C. and Zhang, S., 2023.", "start_char_idx": 640, "end_char_idx": 944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88de296f-fdfe-4280-a61d-295f376a5dfa": {"__data__": {"id_": "88de296f-fdfe-4280-a61d-295f376a5dfa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce79aab2-672c-4686-9725-b0a6eb169b08", "node_type": "1", "metadata": {}, "hash": "d5ffcb4343736012565e71ba7c6c0a63eb17c35a7dcaa8285cdd9dd2bcce0f6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56bf4afc-039f-40c3-8a55-23add43f8a96", "node_type": "1", "metadata": {}, "hash": "1920f198d01d71637da2af45d2633e07ed5a24d432f04e2970498bf4954aaeff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}}, "text": "Task and motion planning with large\nlanguage models for object rearrangement. arXiv preprint arXiv:2303.06247.\n[22] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale.", "start_char_idx": 945, "end_char_idx": 1300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56bf4afc-039f-40c3-8a55-23add43f8a96": {"__data__": {"id_": "56bf4afc-039f-40c3-8a55-23add43f8a96", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88de296f-fdfe-4280-a61d-295f376a5dfa", "node_type": "1", "metadata": {}, "hash": "ee6d58168312ef6393bac703e4c688ebf6eff5af80b5c5ffd731465e8c9346ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047", "node_type": "1", "metadata": {}, "hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "class_name": "RelatedNodeInfo"}}, "text": "An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929.", "start_char_idx": 1225, "end_char_idx": 1333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9eea138e-4fc5-42bf-a900-cdd43e150767": {"__data__": {"id_": "9eea138e-4fc5-42bf-a900-cdd43e150767", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "535af1ae-4b4b-4082-9a2b-ecb9c394b99e", "node_type": "1", "metadata": {}, "hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "535af1ae-4b4b-4082-9a2b-ecb9c394b99e", "node_type": "1", "metadata": {}, "hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2010.11929.\n[23] Dahiya, Abhinav, Alexander M. Aroyo, Kerstin Dautenhahn, and Stephen L. Smith.", "start_char_idx": 0, "end_char_idx": 116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e8db431-5af6-4297-90bd-cb0af84599ce": {"__data__": {"id_": "8e8db431-5af6-4297-90bd-cb0af84599ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40f356cb-9551-4638-ace0-da392e7add13", "node_type": "1", "metadata": {}, "hash": "63c3a5a9c49467cbb8e3e78e2d8ae02f47e2fe0ff8bb6845e31a15871f307292", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}}, "text": "\u201dA survey of\nmulti-agent Human\u2013Robot Interaction systems.\u201d Robotics and Autonomous Systems 161 (2023):\n104335.\n[24] Sharkawy, A.N., 2021. Human-robot interaction: Applications. arXiv preprint arXiv:2102.00928.\n[25] Billard, A., Robins, B., Nadel, J. and Dautenhahn, K., 2007. Building robota, a mini-humanoid\nrobot for the rehabilitation of children with autism.", "start_char_idx": 0, "end_char_idx": 362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40f356cb-9551-4638-ace0-da392e7add13": {"__data__": {"id_": "40f356cb-9551-4638-ace0-da392e7add13", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e8db431-5af6-4297-90bd-cb0af84599ce", "node_type": "1", "metadata": {}, "hash": "210e449fb73d7fa079fff6e98a296274ac20848b1310156fac3b433ccd0fc1db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6eff7521-1348-4f47-b7b6-1c821b83f958", "node_type": "1", "metadata": {}, "hash": "f006a46552b1347492357f5f52717aba3d77a541a7c75eb13d1ba56d77be7d3d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}}, "text": "Building robota, a mini-humanoid\nrobot for the rehabilitation of children with autism. Assistive Technology, 19(1), pp.37-49.\n[26] Clabaugh, C., Tsiakas, K. and Mataric, M., 2017, September. Predicting preschool mathematics\nperformance of children with a socially assistive robot tutor. In Proceedings of the Synergies\nbetween Learning and Interaction Workshop@ IROS, Vancouver, BC, Canada (pp. 24-28).", "start_char_idx": 276, "end_char_idx": 678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6eff7521-1348-4f47-b7b6-1c821b83f958": {"__data__": {"id_": "6eff7521-1348-4f47-b7b6-1c821b83f958", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40f356cb-9551-4638-ace0-da392e7add13", "node_type": "1", "metadata": {}, "hash": "63c3a5a9c49467cbb8e3e78e2d8ae02f47e2fe0ff8bb6845e31a15871f307292", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a1dbca4-bbe4-45ad-bc9c-9698765ed087", "node_type": "1", "metadata": {}, "hash": "b1725dd95cfe0f8baa6b86b0cfed26a1ec8416a84f1f920c68e85d0815667e62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}}, "text": "24-28).\n[27] Knudsen, M. and Kaivo-Oja, J., 2020. Collaborative robots: Frontiers of current literature.\nJournal of Intelligent Systems: Theory and Applications, 3(2), pp.13-20.\n[28] Vicentini, F., 2021. Collaborative robotics: a survey. Journal of Mechanical Design, 143(4),\np.040802.\n[29] Javaid, M., Haleem, A., Singh, R.P., Rab, S. and Suman, R., 2022.", "start_char_idx": 671, "end_char_idx": 1027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a1dbca4-bbe4-45ad-bc9c-9698765ed087": {"__data__": {"id_": "1a1dbca4-bbe4-45ad-bc9c-9698765ed087", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6eff7521-1348-4f47-b7b6-1c821b83f958", "node_type": "1", "metadata": {}, "hash": "f006a46552b1347492357f5f52717aba3d77a541a7c75eb13d1ba56d77be7d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf283e40-2254-4948-9e2a-828766fc97db", "node_type": "1", "metadata": {}, "hash": "7bc9b9f1e7a4ad2c5ce1162899103974b4c1d556195b0faa3ee33946edce9e4e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}}, "text": "Significant applications of\nCobots in the field of manufacturing. Cognitive Robotics, 2, pp.222-233.\n[30] Huang, C., Mees, O., Zeng, A. and Burgard, W., 2023, May. Visual language maps for robot\nnavigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA)(pp.\n10608-10615). IEEE.", "start_char_idx": 1028, "end_char_idx": 1330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf283e40-2254-4948-9e2a-828766fc97db": {"__data__": {"id_": "cf283e40-2254-4948-9e2a-828766fc97db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a1dbca4-bbe4-45ad-bc9c-9698765ed087", "node_type": "1", "metadata": {}, "hash": "b1725dd95cfe0f8baa6b86b0cfed26a1ec8416a84f1f920c68e85d0815667e62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc", "node_type": "1", "metadata": {}, "hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "class_name": "RelatedNodeInfo"}}, "text": "10608-10615). IEEE.\n[31] Bergerman, M., Maeta, S.M., Zhang, J., Freitas, G.M., Hamner, B., Singh,\nS. and Kantor, G. (2015). Robot Farmers: Autonomous Orchard Vehicles Help\nTree Fruit Production. IEEE Robotics & Automation Magazine, 22(1), pp.54\u201363.", "start_char_idx": 1311, "end_char_idx": 1559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "459eccb1-3c03-4448-996f-fe0ba067dcbe": {"__data__": {"id_": "459eccb1-3c03-4448-996f-fe0ba067dcbe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf391a4a-f8a5-414f-8052-a04fc17a5661", "node_type": "1", "metadata": {}, "hash": "b78f6ca49abe05da07584314db3b86d24d8de540e8fc66eec55fe74e1d8e8366", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}}, "text": "IEEE Robotics & Automation Magazine, 22(1), pp.54\u201363.\ndoi:https://doi.org/10.1109/mra.2014.2369292.\n15\n\n[32] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,  L. and\nPolosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems,\n30.", "start_char_idx": 0, "end_char_idx": 305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf391a4a-f8a5-414f-8052-a04fc17a5661": {"__data__": {"id_": "cf391a4a-f8a5-414f-8052-a04fc17a5661", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "459eccb1-3c03-4448-996f-fe0ba067dcbe", "node_type": "1", "metadata": {}, "hash": "ec4b7ace06fd0da472959fad6fc51f964d5f9fe12244da33f1f3b91ca77e63c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a4f3de1-9319-414a-9290-a52ca78149cb", "node_type": "1", "metadata": {}, "hash": "f87eec113670817d6be03361dc961bdf33b6483af3667311d611389f8d9dd6cd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}}, "text": "Attention is all you need. Advances in neural information processing systems,\n30.\n[33] Lin, T., Wang, Y., Liu, X. and Qiu, X., 2022. A survey of transformers. AI Open.\n[34] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020. Language models are few-shot learners.", "start_char_idx": 224, "end_char_idx": 582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a4f3de1-9319-414a-9290-a52ca78149cb": {"__data__": {"id_": "5a4f3de1-9319-414a-9290-a52ca78149cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf391a4a-f8a5-414f-8052-a04fc17a5661", "node_type": "1", "metadata": {}, "hash": "b78f6ca49abe05da07584314db3b86d24d8de540e8fc66eec55fe74e1d8e8366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fb55683-a046-4f97-8148-64028440f7f5", "node_type": "1", "metadata": {}, "hash": "bb2193dd3ed3261d6d10371bf70db1e5912286b439bfe9f4182d09a23defcf69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}}, "text": "Language models are few-shot learners.\nAdvances in neural information processing systems, 33, pp.1877-1901.\n[35] Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R. and McHardy, R., 2023. Challenges\nand applications of large language models. arXiv preprint arXiv:2307.10169.", "start_char_idx": 544, "end_char_idx": 828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fb55683-a046-4f97-8148-64028440f7f5": {"__data__": {"id_": "2fb55683-a046-4f97-8148-64028440f7f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a4f3de1-9319-414a-9290-a52ca78149cb", "node_type": "1", "metadata": {}, "hash": "f87eec113670817d6be03361dc961bdf33b6483af3667311d611389f8d9dd6cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fbe7842-cbe3-4e3f-b84c-fbed358d25b8", "node_type": "1", "metadata": {}, "hash": "389babc3bf60ecff5f785e09b0d9f380ed675ff1f2f1307c5eb895c394e803a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2307.10169.\n[36] Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam,\nM., Chaplot, D.S., Maksymets, O. and Gokaslan, A., 2021. Habitat 2.0: Training home assistants\nto rearrange their habitat. Advances in Neural Information Processing Systems, 34, pp.251-266.", "start_char_idx": 796, "end_char_idx": 1118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fbe7842-cbe3-4e3f-b84c-fbed358d25b8": {"__data__": {"id_": "9fbe7842-cbe3-4e3f-b84c-fbed358d25b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fb55683-a046-4f97-8148-64028440f7f5", "node_type": "1", "metadata": {}, "hash": "bb2193dd3ed3261d6d10371bf70db1e5912286b439bfe9f4182d09a23defcf69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "node_type": "1", "metadata": {}, "hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "class_name": "RelatedNodeInfo"}}, "text": "Advances in Neural Information Processing Systems, 34, pp.251-266.\n[37] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess,\nD., Dubey, A., Finn, C. and Florence, P., 2023. Rt-2: Vision-language-action models transfer web\nknowledge to robotic control. arXiv preprint arXiv:2307.15818.", "start_char_idx": 1052, "end_char_idx": 1379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50d2cb4e-539a-439e-8c3b-15d446c31e28": {"__data__": {"id_": "50d2cb4e-539a-439e-8c3b-15d446c31e28", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e099c449-ef0c-4987-942d-5ec5644ff227", "node_type": "1", "metadata": {}, "hash": "d1696c29466420958ddec1b4435573f7a712be07a83771973ce3ef7738c5afb4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2307.15818.\n[38] Tan, C., Xu, X. and Shen, F., 2021. A survey of zero shot detection: methods and applications.\nCognitive Robotics, 1, pp.159-167.\n[39] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S.,\nBerg, A.C., Lo, W.Y. and Doll\u00b4 ar, P., 2023.", "start_char_idx": 0, "end_char_idx": 315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e099c449-ef0c-4987-942d-5ec5644ff227": {"__data__": {"id_": "e099c449-ef0c-4987-942d-5ec5644ff227", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50d2cb4e-539a-439e-8c3b-15d446c31e28", "node_type": "1", "metadata": {}, "hash": "b3fc4a1ee231ced40ee8a071ba235203ed24add774e1511f04377023ba519cc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "388d4767-5b8d-46de-b7f4-4a2caf2522a2", "node_type": "1", "metadata": {}, "hash": "9d8655c2cb390d20dc316ea78766dc2b87623f7b3fd97ccaa94d7ae3896a5572", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}}, "text": "and Doll\u00b4 ar, P., 2023. Segment anything. arXiv preprint arXiv:2304.02643.\n[40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,\nA., Mishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning (pp. 8748-8763).", "start_char_idx": 292, "end_char_idx": 654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "388d4767-5b8d-46de-b7f4-4a2caf2522a2": {"__data__": {"id_": "388d4767-5b8d-46de-b7f4-4a2caf2522a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e099c449-ef0c-4987-942d-5ec5644ff227", "node_type": "1", "metadata": {}, "hash": "d1696c29466420958ddec1b4435573f7a712be07a83771973ce3ef7738c5afb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06502e61-6ba0-4ef4-a63c-68d5451b5616", "node_type": "1", "metadata": {}, "hash": "97ea51abc01b4199f80b2a8e1e9d55f3eea5e90c474f8594a919edfee3982040", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}}, "text": "In International conference on machine learning (pp. 8748-8763).\nPMLR.\n[41] Gu, X., Lin, T.Y., Kuo, W. and Cui, Y., 2021. Open-vocabulary object detection via vision and\nlanguage knowledge distillation. arXiv preprint arXiv:2104.13921.\n[42] Zhu, P., Wang, H. and Saligrama, V., 2019. Zero shot detection.", "start_char_idx": 590, "end_char_idx": 894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06502e61-6ba0-4ef4-a63c-68d5451b5616": {"__data__": {"id_": "06502e61-6ba0-4ef4-a63c-68d5451b5616", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "388d4767-5b8d-46de-b7f4-4a2caf2522a2", "node_type": "1", "metadata": {}, "hash": "9d8655c2cb390d20dc316ea78766dc2b87623f7b3fd97ccaa94d7ae3896a5572", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b346816f-0a6e-466f-b893-f155bc546628", "node_type": "1", "metadata": {}, "hash": "9f4287785f0215816114c8b2c2ec0700b99f75295f09ac2d7d8ccca1ce4b7240", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}}, "text": "Zero shot detection. IEEE Transactions on Circuits\nand Systems for Video Technology, 30(4), pp.998-1010.\n[43] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakr-\nishnan, K., Hausman, K. and Herzog, A., 2022. Do as i can, not as i say: Grounding language in\nrobotic affordances.", "start_char_idx": 874, "end_char_idx": 1196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b346816f-0a6e-466f-b893-f155bc546628": {"__data__": {"id_": "b346816f-0a6e-466f-b893-f155bc546628", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06502e61-6ba0-4ef4-a63c-68d5451b5616", "node_type": "1", "metadata": {}, "hash": "97ea51abc01b4199f80b2a8e1e9d55f3eea5e90c474f8594a919edfee3982040", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "47cda882-478b-46e5-a57e-2c079162b9bc", "node_type": "1", "metadata": {}, "hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2204.01691.", "start_char_idx": 1197, "end_char_idx": 1229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e62f2851-38fd-44c0-b6bc-7eb4b8759ed3": {"__data__": {"id_": "e62f2851-38fd-44c0-b6bc-7eb4b8759ed3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5660390a-65cd-4b17-a5eb-c4b9096f2ef7", "node_type": "1", "metadata": {}, "hash": "f317894c2cdc0c7569aca7fcf09efe0062b377a718dcb69f51f9ca3498e33913", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2204.01691.\n[44] Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker, S., Tombari, F., Purohit,\nA., Ryoo, M., Sindhwani, V. and Lee, J., 2022. Socratic models: Composing zero-shot multimodal\nreasoning with language. arXiv preprint arXiv:2204.00598.\n[45] Zhao, Z., Lee, W.S.", "start_char_idx": 0, "end_char_idx": 312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5660390a-65cd-4b17-a5eb-c4b9096f2ef7": {"__data__": {"id_": "5660390a-65cd-4b17-a5eb-c4b9096f2ef7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e62f2851-38fd-44c0-b6bc-7eb4b8759ed3", "node_type": "1", "metadata": {}, "hash": "df6d8a8b6a568dd41f46abc88a09e47fcf4c43cd3e14c92c377ca8bf08b43adc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0432442a-9644-4a91-ad45-7f1da1102faa", "node_type": "1", "metadata": {}, "hash": "c462c0f6c7c92c84256c8a3034916f1d902527910dacb8c15ce819537e208112", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}}, "text": "[45] Zhao, Z., Lee, W.S. and Hsu, D., 2023. Large Language Models as Commonsense Knowledge for\nLarge-Scale Task Planning. arXiv preprint arXiv:2305.14078.\n[46] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y. and Narasimhan, K., 2023.\nTree of thoughts: Deliberate problem solving with large language models.", "start_char_idx": 288, "end_char_idx": 611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0432442a-9644-4a91-ad45-7f1da1102faa": {"__data__": {"id_": "0432442a-9644-4a91-ad45-7f1da1102faa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5660390a-65cd-4b17-a5eb-c4b9096f2ef7", "node_type": "1", "metadata": {}, "hash": "f317894c2cdc0c7569aca7fcf09efe0062b377a718dcb69f51f9ca3498e33913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9", "node_type": "1", "metadata": {}, "hash": "d311716123fef415e3973110269cfd4c893341810de7132a15bd2ff2a1c19987", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}}, "text": "Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint\narXiv:2305.10601.\n[47] Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J.B., Shu, T. and Gan, C., 2023.\nBuilding cooperative embodied agents modularly with large language models. arXiv preprint\narXiv:2307.02485.", "start_char_idx": 539, "end_char_idx": 848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9": {"__data__": {"id_": "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0432442a-9644-4a91-ad45-7f1da1102faa", "node_type": "1", "metadata": {}, "hash": "c462c0f6c7c92c84256c8a3034916f1d902527910dacb8c15ce819537e208112", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a74b2f86-6ba6-4db3-a1d8-b14ef775ea9b", "node_type": "1", "metadata": {}, "hash": "e09758e7807d5f507011f6361fb9b4128426bdb1521f79f92cc9396ed88d00ed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:2307.02485.\n[48] Chen, L., Zhang, Y., Ren, S., Zhao, H., Cai, Z., Wang, Y., Wang, P., Liu, T. and Chang,\nB., 2023. Towards end-to-end embodied decision making via multi-modal large language model:\nExplorations with gpt4-vision and beyond. arXiv preprint arXiv:2310.02071.", "start_char_idx": 816, "end_char_idx": 1108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a74b2f86-6ba6-4db3-a1d8-b14ef775ea9b": {"__data__": {"id_": "a74b2f86-6ba6-4db3-a1d8-b14ef775ea9b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9", "node_type": "1", "metadata": {}, "hash": "d311716123fef415e3973110269cfd4c893341810de7132a15bd2ff2a1c19987", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bef3ce88-7a33-4020-8720-0336a725ec9d", "node_type": "1", "metadata": {}, "hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2310.02071.\n16\n\n[49] Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y. and\nZhou, J., 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\n[50] Niryo. (n.d.).", "start_char_idx": 1076, "end_char_idx": 1353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b282e0c1-33b9-4da3-93ab-7f38fb16b3df": {"__data__": {"id_": "b282e0c1-33b9-4da3-93ab-7f38fb16b3df", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7", "node_type": "1", "metadata": {}, "hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7", "node_type": "1", "metadata": {}, "hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "class_name": "RelatedNodeInfo"}}, "text": "[50] Niryo. (n.d.). Ned2 Overview. [online] Available at: https://niryo.com/products-cobots/robot-\nned-2/.\n[51] Intel\u00ae RealSenseTM Depth and Tracking Cameras. (2019). Depth Camera\nD435i \u2013 Intel \u00aeRealSenseTM Depth and Tracking Cameras. [online] Available at:\nhttps://www.intelrealsense.com/depth-camera-d435i/.", "start_char_idx": 0, "end_char_idx": 309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf7170d0-b23f-4eb2-8665-b82cffcd222d": {"__data__": {"id_": "cf7170d0-b23f-4eb2-8665-b82cffcd222d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d524de96-acb3-4502-97c4-cd3cbec8f6cc", "node_type": "1", "metadata": {}, "hash": "f7034d6d82e4337886853ca637966fb14d54083b4f2150fe077f586cfd05ec3c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}}, "text": "[52] W. A. Bainbridge, J. Hart, E. S. Kim and B. Scassellati, \u201dThe effect of presence on human-\nrobot interaction,\u201d RO-MAN 2008 - The 17th IEEE International Symposium on Robot and\nHuman Interactive Communication, Munich, Germany, 2008, pp. 701-706, doi: 10.1109/RO-\nMAN.2008.4600749.", "start_char_idx": 0, "end_char_idx": 284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d524de96-acb3-4502-97c4-cd3cbec8f6cc": {"__data__": {"id_": "d524de96-acb3-4502-97c4-cd3cbec8f6cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf7170d0-b23f-4eb2-8665-b82cffcd222d", "node_type": "1", "metadata": {}, "hash": "a8a0154637fe7aba3ecc034ce903888d886ae9c1e74f5b6def9cb0f97008539a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1335020b-6427-4796-ad05-853b7bd61c16", "node_type": "1", "metadata": {}, "hash": "8dc44cc4865e3e6ca895744beaf580fd46dfa84ba9bf92e09a7e68287c33584b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}}, "text": "[53] A. Vick, D. Surdilovic and J. Kr\u00a8 uger, \u201dSafe physical human-robot interaction with industrial\ndual-arm robots,\u201d 9th International Workshop on Robot Motion and Control, Kuslin, Poland,\n2013, pp. 264-269, doi: 10.1109/RoMoCo.2013.6614619.\n[54] Bragan\u00b8 ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019.", "start_char_idx": 285, "end_char_idx": 599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1335020b-6427-4796-ad05-853b7bd61c16": {"__data__": {"id_": "1335020b-6427-4796-ad05-853b7bd61c16", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d524de96-acb3-4502-97c4-cd3cbec8f6cc", "node_type": "1", "metadata": {}, "hash": "f7034d6d82e4337886853ca637966fb14d54083b4f2150fe077f586cfd05ec3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae83b356-b35c-441a-bc88-3cab957b97bb", "node_type": "1", "metadata": {}, "hash": "4f9c32aaae38899b6270ff3cfd00bd9223a3922ca7d9a6791e5dfec0311712e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}}, "text": "A brief overview of the use of\ncollaborative robots in industry 4.0: Human role and safety. Occupational and environmental\nsafety and health, pp.641-650.\n[55] Bragan\u00b8 ca, S., Costa, E., Castellucci, I. and Arezes, P.M., 2019. A brief overview of the use of\ncollaborative robots in industry 4.0: Human role and safety. Occupational and environmental\nsafety and health, pp.641-650.", "start_char_idx": 600, "end_char_idx": 979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae83b356-b35c-441a-bc88-3cab957b97bb": {"__data__": {"id_": "ae83b356-b35c-441a-bc88-3cab957b97bb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1335020b-6427-4796-ad05-853b7bd61c16", "node_type": "1", "metadata": {}, "hash": "8dc44cc4865e3e6ca895744beaf580fd46dfa84ba9bf92e09a7e68287c33584b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98e06e1f-432e-4d60-9c09-f7ce436cf92c", "node_type": "1", "metadata": {}, "hash": "328e1f0333a6bba7cc5ba83ff5522c8eab08520c1edecf70ccbfd0da5a32d8a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}}, "text": "Occupational and environmental\nsafety and health, pp.641-650.\n[56] Park, J.S., O\u2019Brien, J., Cai, C.J., Morris, M.R., Liang, P. and Bernstein, M.S., 2023, October.\nGenerative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual\nACM Symposium on User Interface Software and Technology (pp. 1-22).", "start_char_idx": 918, "end_char_idx": 1242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98e06e1f-432e-4d60-9c09-f7ce436cf92c": {"__data__": {"id_": "98e06e1f-432e-4d60-9c09-f7ce436cf92c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae83b356-b35c-441a-bc88-3cab957b97bb", "node_type": "1", "metadata": {}, "hash": "4f9c32aaae38899b6270ff3cfd00bd9223a3922ca7d9a6791e5dfec0311712e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def", "node_type": "1", "metadata": {}, "hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "class_name": "RelatedNodeInfo"}}, "text": "1-22).\nAppendix A Section title of first appendix\nAn appendix contains supplementary information that is not an essential part of the text itself but\nwhich may be helpful in providing a more comprehensive understanding of the research problem or\nit is information that is too cumbersome to be included in the body of the paper.\n17", "start_char_idx": 1236, "end_char_idx": 1566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"c335fb7f-ba4c-48a7-89cf-ae97f51db883": {"doc_hash": "9c604718ef6786a2d3c469c7e84753fbee8684de7df8eeaa428a1896292a875b", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5": {"doc_hash": "f3c91c0d358ab417b3cd882be828694963c80ef55063b3b6353540ee099d39b3", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "d7c1c087-905a-41a0-88d3-92cf9a603438": {"doc_hash": "92c59f4b31df5f7036a093fc391ac397a9af3cd9b0fa8d9f5f9b867eca821a16", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "16196373-14c0-4390-a0db-8ac5b663447f": {"doc_hash": "a3bb0a70efb7e3bd85ef9366744791638d2b2ba8ee5c685b33fa3703bf08ae85", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "cf090498-501a-4e10-b6fb-411592ea0e5a": {"doc_hash": "966b50a7de4bc3c9db2edf6e0f9181e5400d27f9ea7080b98187603088eacdc9", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0": {"doc_hash": "26dadeb55b9a9dda5b8ff991782fa75702849eef198a1029d9d2d2ff5d59c323", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "d208a291-bfa1-4867-9fa5-3c4617ebd24f": {"doc_hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "ref_doc_id": "ad8f3299-76b9-4412-b26d-0c99273842f0"}, "8e283248-217b-4607-a828-330db0cb3c4b": {"doc_hash": "e80798e8ecdd71aae1cf0508df8f66344db2fb1581a4c0304627dbcfb701a1e5", "ref_doc_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883"}, "cfc6a798-5b46-44dd-a887-491516a74f13": {"doc_hash": "e47ebd675648293e95a7ad096ae4e166114ed8998d4aa4b6d4fb3876799e9b5c", "ref_doc_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883"}, "843dae4f-9177-4be2-bea3-15b167c43370": {"doc_hash": "47999a4cb1cf3f7243cd8128ee5a757d03f8651920fca3b797703589e674d2c9", "ref_doc_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883"}, "6944eebd-6e1a-49f1-82eb-910d01b2d95a": {"doc_hash": "9f5506c1b0089a8e4abcfc0176c267a6b418954beb36fecac2008cb8cfa030f1", "ref_doc_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883"}, "00df92de-8dba-43a8-af11-19ce18c1df5e": {"doc_hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "ref_doc_id": "c335fb7f-ba4c-48a7-89cf-ae97f51db883"}, "3de7bbe1-118a-45de-ab42-327e9f821b94": {"doc_hash": "23d839aa30e27b56b1ee122ff17883d124e4ec83a6e1c6b00034232e496fb50d", "ref_doc_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5"}, "d18e9348-dfee-468f-85ad-d27e16ec8090": {"doc_hash": "edc6019973ac82f6dcf178038cf4af9171d8c1ff54acbe1c761fab5565a926f2", "ref_doc_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5"}, "689d7d2f-c966-411d-9425-fc11d5ac733d": {"doc_hash": "181df5a55444439b58cbb7b5a23c8eb197ebdb9c477bfd03783438a24eb19347", "ref_doc_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5"}, "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de": {"doc_hash": "c6fdd23c38d9de30da311c85a9abeec7f048df81e2e7f37839f11dff6451207b", "ref_doc_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5"}, "dac9c22d-586d-41fc-9efe-27c747d1c629": {"doc_hash": "239e1b95d902278233e04684cff6fbbea715fab9ce18ec36ad6d6649456caa5f", "ref_doc_id": "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5"}, "8e7527b6-f6c8-4689-8d08-a92632a126ed": {"doc_hash": "e15f43c2cb0e283eb588ba8855a26be9e36045701e40702fbc33ba2a6be9c994", "ref_doc_id": "d7c1c087-905a-41a0-88d3-92cf9a603438"}, "73dd9204-cda7-4bbd-b64c-daf82a689ddd": {"doc_hash": "847ab116c81ef605c55d746c79796963abea51defeb468795e40678ebc7bf1b7", "ref_doc_id": "d7c1c087-905a-41a0-88d3-92cf9a603438"}, "adfb9d0c-8f3c-4922-be13-e6b17a2cb838": {"doc_hash": "73867eca6e45d3a0c7079b53f66be273b24f23b00bd8bb8abd074b51e780b1de", "ref_doc_id": "d7c1c087-905a-41a0-88d3-92cf9a603438"}, "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4": {"doc_hash": "dc90f678c5990994d497249f5f2a7aa28a84813bb991fb9e31b59228fb7a9dde", "ref_doc_id": "d7c1c087-905a-41a0-88d3-92cf9a603438"}, "b293d45b-6ae1-4bbc-b549-50431cbbddcf": {"doc_hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "ref_doc_id": "d7c1c087-905a-41a0-88d3-92cf9a603438"}, "00821df1-46e5-4f24-9733-627c63fa7193": {"doc_hash": "30a778202306f0690b8ae3935bc1c1c64aea6a48ab375658ae64bcb78355903c", "ref_doc_id": "16196373-14c0-4390-a0db-8ac5b663447f"}, "0b7fa9d2-6028-4406-b856-ccb1daad3a58": {"doc_hash": "10438eae9fad6119f4c01199e20ea419f0e780aa30c7a47e523a82da63cd2ab0", "ref_doc_id": "16196373-14c0-4390-a0db-8ac5b663447f"}, "938608bd-cfa6-4c4a-a770-fa7ac060dc4c": {"doc_hash": "74ee91f12007d78a54fe1132e13088ed201c9d6d2ce661ea3db71236bbb76151", "ref_doc_id": "16196373-14c0-4390-a0db-8ac5b663447f"}, "fa0483c5-b5ed-490a-b7d3-c6972582bc61": {"doc_hash": "f41e59e2ca9704b7c3a28d932b7320a9228be82287be979bfd04d52fb6af522b", "ref_doc_id": "16196373-14c0-4390-a0db-8ac5b663447f"}, "cf72c77f-d210-49e0-9fbc-868444937960": {"doc_hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "ref_doc_id": "16196373-14c0-4390-a0db-8ac5b663447f"}, "dcd36521-18d4-4ede-889d-47fb5fc3bf31": {"doc_hash": "da3228634726e05e7c2a5a690aee309c97f0f883e7efe320df2eb18c27dca594", "ref_doc_id": "cf090498-501a-4e10-b6fb-411592ea0e5a"}, "88403054-34c4-4fdf-b50c-5918c1f3f7e1": {"doc_hash": "530e6d662ecd0d3fc0e6fe907255f52929ad4c748bda6bd360c35875a5d62067", "ref_doc_id": "cf090498-501a-4e10-b6fb-411592ea0e5a"}, "e7643626-84dd-4b5c-9902-528a44aea517": {"doc_hash": "3528033746e05ee12e72b958fcaa8c22f5cafda61680fe08abe0df99d59c2381", "ref_doc_id": "cf090498-501a-4e10-b6fb-411592ea0e5a"}, "013731c3-85e2-48ed-9aff-b7ec67c9c047": {"doc_hash": "c3b734ef894df027710cecac2df4108ff267130949804cca07c846e19dfb07d1", "ref_doc_id": "cf090498-501a-4e10-b6fb-411592ea0e5a"}, "535af1ae-4b4b-4082-9a2b-ecb9c394b99e": {"doc_hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "ref_doc_id": "cf090498-501a-4e10-b6fb-411592ea0e5a"}, "b6e6766d-037f-44f7-b12e-ef6e22966bfc": {"doc_hash": "4df0b38567b1e58d02e5716cb1061ac2da067dfcc38e89a7abe500acc5151c8f", "ref_doc_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0"}, "4b70d562-31c4-4c4f-9f8b-c14d239ea067": {"doc_hash": "6b5eed51adc15ef41bd6babc3a1f188bd22706f8641d76b5199642756c26a673", "ref_doc_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0"}, "47cda882-478b-46e5-a57e-2c079162b9bc": {"doc_hash": "3cfeac1d62bca83e38017e52733672232cd64d95b6fe1b212a7d4993ab10f1b6", "ref_doc_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0"}, "bef3ce88-7a33-4020-8720-0336a725ec9d": {"doc_hash": "0ac9afee3511ac5107085b4097741905d1dc1256d3f3005470d32ace74ba6db6", "ref_doc_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0"}, "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7": {"doc_hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "ref_doc_id": "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0"}, "5b44ef94-6ea4-4c20-b24f-b48496a28def": {"doc_hash": "15b87502c9f0bacd0b412ba576d9a69b4833e3b7b614eba2ff9c525242128dde", "ref_doc_id": "d208a291-bfa1-4867-9fa5-3c4617ebd24f"}, "0d9f52d2-3071-4ee8-a412-ef19afe74560": {"doc_hash": "555d533be8d80d2d1ac5a4e5c0f9d743d94049d9de36614077629d25ccbe5783", "ref_doc_id": "8e283248-217b-4607-a828-330db0cb3c4b"}, "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52": {"doc_hash": "81753b255890c1e9faaa2542760e9c0b56ae0263dfc788954b7f7b5ebddeaf9c", "ref_doc_id": "8e283248-217b-4607-a828-330db0cb3c4b"}, "8fc2ed2a-4859-4c63-99a1-98da3c706bfb": {"doc_hash": "e65bdb33425c9550ff3557a545aff066af2b5b09ac516e7d5d6bb40653a67018", "ref_doc_id": "8e283248-217b-4607-a828-330db0cb3c4b"}, "37fc32c5-6cd8-4dac-b4fe-f058d588625f": {"doc_hash": "7b2141d767cf0cf83153a7606ef946705fd486949811a308da4943f4e1b1ffa7", "ref_doc_id": "8e283248-217b-4607-a828-330db0cb3c4b"}, "177dd3e4-5571-4ead-9052-bd8f23b37f89": {"doc_hash": "5ad1e2384df87657a290b1a7d59385f49000073fb75be3f077f97dd63efccd38", "ref_doc_id": "8e283248-217b-4607-a828-330db0cb3c4b"}, "aaa052f9-cbb5-41bf-aa39-144bb278d76f": {"doc_hash": "dd95b0e93f5cb8845523cdc34b5b1f970c2e3fc9bcd398f5936dc050acabdb60", "ref_doc_id": "cfc6a798-5b46-44dd-a887-491516a74f13"}, "3fd74188-bb64-49a6-b643-06b173e30da9": {"doc_hash": "9ee9bc9123c08777edb87d85d2f3880f2aee358156c16c23e6f82bc125110c21", "ref_doc_id": "cfc6a798-5b46-44dd-a887-491516a74f13"}, "892913dc-5691-4b8c-b89a-7cbd4b290020": {"doc_hash": "1bceddfe3ffcf273ebbfa06b93837bc5d69f8796c8b590711d0bbea9508503d1", "ref_doc_id": "cfc6a798-5b46-44dd-a887-491516a74f13"}, "a8420ba4-1923-44bc-ad6f-f039cb8dac2d": {"doc_hash": "8b01d54ef9a09f2ac8d5701b51c62bf346548f23c8f564aacfdaf281c9386833", "ref_doc_id": "cfc6a798-5b46-44dd-a887-491516a74f13"}, "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4": {"doc_hash": "790a7bf0b5173889d4b1c24df255ccf215069fa4026b62f0de1178babcff6e2c", "ref_doc_id": "cfc6a798-5b46-44dd-a887-491516a74f13"}, "b6bc5afb-53ff-400c-92b8-660acb67c15e": {"doc_hash": "d0c630e1f7dd315529172d79870f78a2edebead71c5acc5a9f34737766324fab", "ref_doc_id": "cfc6a798-5b46-44dd-a887-491516a74f13"}, "a4f63a6b-d521-4b74-bce9-8970fa75e32f": {"doc_hash": "edeb9f9f1ab9c2c2dbd7c036eb7467c9426a4721eef29b21a25eba66f1b0c6df", "ref_doc_id": "843dae4f-9177-4be2-bea3-15b167c43370"}, "d560d311-94bc-47ae-bbbc-2a83a13270e2": {"doc_hash": "7c21e94fcd4343da4c5dd8348aea69671b28765ff39fc12f1c8b7308fda915ce", "ref_doc_id": "843dae4f-9177-4be2-bea3-15b167c43370"}, "02531e66-29d7-4df8-89b7-184cb314c3f6": {"doc_hash": "6138f49b24f87e8e692cffb8eddd4f2170b05c5331eabc3fb451118ac7ad2898", "ref_doc_id": "843dae4f-9177-4be2-bea3-15b167c43370"}, "3dfbe97c-0ec0-466d-95a7-85757c75dc0a": {"doc_hash": "d53393852cb46475ca82e9d69fc700fdbe2f9a2a3fc7ebc96dde0a92d8b94f78", "ref_doc_id": "843dae4f-9177-4be2-bea3-15b167c43370"}, "a766a312-c240-47be-a6b9-6c8a0f30effe": {"doc_hash": "ec53f0895b0c8a11a281f1984e2817f33719d99ecec0c3f7476e2d1675b7667e", "ref_doc_id": "843dae4f-9177-4be2-bea3-15b167c43370"}, "c24ccb24-0ecf-4a0d-b736-aa3765f5e854": {"doc_hash": "9ab438c1bb38602e11a4223744801a09bf364180371e9cc2104ebed5d9396e6c", "ref_doc_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a"}, "2aa7ecba-c61b-49b5-b220-1fca807df300": {"doc_hash": "ddd4af6f205afd4350802a4fc56bde65123e0db010883f40acdb16002984615d", "ref_doc_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a"}, "4e140397-4561-4086-b5d8-5b4f8095eeb4": {"doc_hash": "5208ebac2c3a164f6f46957a83033eeba166d492a678adcd2608295f233e24f8", "ref_doc_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a"}, "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c": {"doc_hash": "0c1b1c724b8ffd4a837485ab4eec43d46f65e0154f89b7fa693c1374ad073a08", "ref_doc_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a"}, "178675e5-5624-4cba-86c4-41443ef806be": {"doc_hash": "fc7d1236a106213f3b335aa885a1d5fecf82aacdcc4aa6f6a759d2d86d7c6018", "ref_doc_id": "6944eebd-6e1a-49f1-82eb-910d01b2d95a"}, "b70a610f-59be-4ed3-9210-3fd4af7e34cb": {"doc_hash": "0ec41697dc8d4661c2ecec77ec25246aeee18ba6145a03359b66eb513c364ca4", "ref_doc_id": "00df92de-8dba-43a8-af11-19ce18c1df5e"}, "8c26192c-4bb1-4fe5-92a4-0cfd6afc672c": {"doc_hash": "460bcb7c0b5f667968b98861d24282042078edbea5d43fe84392f6ca9967b179", "ref_doc_id": "3de7bbe1-118a-45de-ab42-327e9f821b94"}, "ccdc64e7-8578-402c-adfc-7f9bedebc2fa": {"doc_hash": "bf6da2e2d1f0cc03419c25db15eaf97b5350d817d5a1b0184cf50ec73e9e05d0", "ref_doc_id": "3de7bbe1-118a-45de-ab42-327e9f821b94"}, "e748def4-5a55-4031-947f-9004d685688c": {"doc_hash": "bd0d69ebba0018a2cae92f34220d050df176edcfae5ad7ee016ac226178cb12c", "ref_doc_id": "3de7bbe1-118a-45de-ab42-327e9f821b94"}, "7919a516-e2f9-4871-b726-7ef1660feebd": {"doc_hash": "12eca0246b68dca61368d35769141668b919226c4837cac3a4554e92d06603d6", "ref_doc_id": "3de7bbe1-118a-45de-ab42-327e9f821b94"}, "70f7146f-e6e4-4d84-900e-aac0131533f9": {"doc_hash": "54c22477f765376339278c39e2303b60bb566b14ff94499aa0e136020d07a1c7", "ref_doc_id": "3de7bbe1-118a-45de-ab42-327e9f821b94"}, "25f39df5-e777-427c-a9a0-a661f7ca48ab": {"doc_hash": "701b4d322c8220cb4525b80dfb544d9e0d5bb220b33935aac593b039bc297249", "ref_doc_id": "d18e9348-dfee-468f-85ad-d27e16ec8090"}, "f4023064-a0ff-4c94-a537-214611f7c55a": {"doc_hash": "129965f24bfbb769058b4d3fd980d88037a8532e8c3a23f925e816b06b5d172e", "ref_doc_id": "d18e9348-dfee-468f-85ad-d27e16ec8090"}, "78598da2-b279-46d4-aaf0-fefca676c30d": {"doc_hash": "84af333610b2ca463f614a413ca1625a4dd006b5b1eabfb18da9a66c0b22ab73", "ref_doc_id": "d18e9348-dfee-468f-85ad-d27e16ec8090"}, "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8": {"doc_hash": "dbbbe91bdc83edd1df308d2fa8a0a72ea92a71d197e06c3f9f2f9f5df0d1745a", "ref_doc_id": "d18e9348-dfee-468f-85ad-d27e16ec8090"}, "7159ecb2-ba9a-49a2-9294-ac6a6e52ccf7": {"doc_hash": "7bd07ca2b81e49b420b289efd8ef3dfd63a79a6b5b3d9f6292c8f5b8d4843012", "ref_doc_id": "d18e9348-dfee-468f-85ad-d27e16ec8090"}, "f051495e-b4cf-462e-b42a-67f9b5ebd4c0": {"doc_hash": "6065556a5f00d36a5b50590335aa6dbdcccaaffc81ced2cbb8a1474da8acd3cb", "ref_doc_id": "689d7d2f-c966-411d-9425-fc11d5ac733d"}, "43d26dba-e6a1-48c4-ac6f-513f6d1ceace": {"doc_hash": "267c7fbf07b5d0bb58522393e8e32d301d090e8df80c8a47130bf8523929250b", "ref_doc_id": "689d7d2f-c966-411d-9425-fc11d5ac733d"}, "1407828d-734e-4103-b05f-1c3429cf8784": {"doc_hash": "d3facc49994d9386495173f35ec304aac0bb4a2ef1d695076d72034dda496f7f", "ref_doc_id": "689d7d2f-c966-411d-9425-fc11d5ac733d"}, "c17b65e8-1f37-4a18-beea-1450866636f4": {"doc_hash": "5310d24e111644c1c2707d0a1662cc413afef6eaf1196a3abae16ea9ab676bba", "ref_doc_id": "689d7d2f-c966-411d-9425-fc11d5ac733d"}, "2ae020f6-3531-47c0-acfb-5c2635f4d78f": {"doc_hash": "9353ff451cd12e3010e3138f92e760878685958101b286893477b9d9384ec63a", "ref_doc_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de"}, "2b9a7b44-a020-4a1e-8471-3b9ca588f56d": {"doc_hash": "b1cc68771ccfea5f64075ce2e2c3038740627bd4ee7729ff067718d300853fa9", "ref_doc_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de"}, "13103365-ee6e-468d-8679-47406168ad1e": {"doc_hash": "2cef2a32a39108ae84b7d9b1e8083a885385f33cd56b7bf2c146ceaa5592baa2", "ref_doc_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de"}, "2970cdd8-7cc5-441e-9180-3f0d35aed72b": {"doc_hash": "4c9853b85b783cb771531a481132463bfdd8af64a380cf044efc97796273b404", "ref_doc_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de"}, "ae2718cf-b324-43b2-80af-608864bbd060": {"doc_hash": "41191277b84541d7df93f9645c55c3dea9a038dc3a4860f9d7db93c451522625", "ref_doc_id": "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de"}, "3406b63b-8760-44bc-bad2-60f68ad85a22": {"doc_hash": "7fbace106ea6f4b25b6e38a3bf6a53a4196dc0947e0a21d89e22cb7ed4be64ac", "ref_doc_id": "dac9c22d-586d-41fc-9efe-27c747d1c629"}, "01020a24-b2d0-43f9-9c09-4f28a0ff2557": {"doc_hash": "6269235c94a7bc3062c7ede7e97fd37080ac9da714262a6869652370674c96c1", "ref_doc_id": "dac9c22d-586d-41fc-9efe-27c747d1c629"}, "b86032ac-9442-4216-ba6b-9afa9612fbb9": {"doc_hash": "e2be555d9672279a599f6c588df01119d17a574b8e09da5c03b0e605ff876723", "ref_doc_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed"}, "123060e0-ef82-4211-9ac2-8b7958ed9ffc": {"doc_hash": "95414fca3517dc99c4ff51a8cc9e59023a318f416a9a2ff8d7713847633af75a", "ref_doc_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed"}, "69cf0473-e62a-4861-a3ed-572985701870": {"doc_hash": "18d4eca43dea4efeefa8283975dd857a1fd63576753abdb9984ff10350712d31", "ref_doc_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed"}, "4c056a44-4ed4-4f29-af60-bc75de435d21": {"doc_hash": "09f50c0914d40b0677f0cd60cc8d2c95d9539728e47158bc6f06161a876ab8fb", "ref_doc_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed"}, "d8b7d0cb-c03d-4972-abaa-0708bbb9beb0": {"doc_hash": "2cff1b743defb5186f9604f7853541fa11c6ecb594631d5bbf1c7539ff153019", "ref_doc_id": "8e7527b6-f6c8-4689-8d08-a92632a126ed"}, "e10709d3-92ff-47c2-8da9-27e20d05e99a": {"doc_hash": "8426468f595b3ee5efbb038992da0dc251fcbb2b82fb90372edbac528d5996bd", "ref_doc_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd"}, "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8": {"doc_hash": "03b1f2896e55ef3b8f6fbdae3ba07a7ce31759d72afa364cc9a42db2f38504e5", "ref_doc_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd"}, "43987c9d-fcbe-4f29-bc90-36429a295adb": {"doc_hash": "cedf9092f16b2f6c47c56ededc38c07f479c8dbbc066c878269739ef9c9f0f27", "ref_doc_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd"}, "852d47eb-cef2-4b84-9efd-e212ab46cff6": {"doc_hash": "67ed8dd45ee9d1fdaa95ebb04e711e8c2010b803ed23d75a2df419d00a00d41f", "ref_doc_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd"}, "1400cf08-5d42-45ab-a44d-1e465d29fd06": {"doc_hash": "b9ca4fb4756075afcad20038f20b28a9139b117a7d0eb06abcc4179b514986dc", "ref_doc_id": "73dd9204-cda7-4bbd-b64c-daf82a689ddd"}, "97a9637e-637b-43a3-9842-34beced18813": {"doc_hash": "48614cd16bec97836dad5b95c06197d80f37579c1129ff5551b7e22146561300", "ref_doc_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838"}, "e16e2fef-45e6-4638-b561-ddd00e0ebee7": {"doc_hash": "d7d7854eef3fd161be99b1c15ecd44115e00458723b60f0f40a88f46926ea5f0", "ref_doc_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838"}, "2631d6e0-e208-4389-a568-5d229055d8b6": {"doc_hash": "0aa3fedd69ffa975dc8d4964c40f90cd75cee65629da79f28f4e82b62112f4d2", "ref_doc_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838"}, "7de4be5c-f717-43bd-b932-844e2b16d00b": {"doc_hash": "3ae53c81581c77be64d487bb916b2b7d6ff393711bd7a5e1a69286ad9be8dd1e", "ref_doc_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838"}, "ba291a7d-f31c-411c-93a0-8fc765f10b12": {"doc_hash": "99bd8b1f09aab1adca233c6511d6054908f60b57d3f1c95a38f047132da27015", "ref_doc_id": "adfb9d0c-8f3c-4922-be13-e6b17a2cb838"}, "08fa381b-8c67-4078-b3ca-6d13b5c81bf5": {"doc_hash": "c28d50d68db0b6a5103bff6a3139c74bba43ba788f89d7cad7c231a2e22ca245", "ref_doc_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4"}, "364c343f-141b-46ce-aae0-fb2eb01030c0": {"doc_hash": "feeeb8d26418ed9d6394bd2ac86c43f1f2e3b97bc98a0e05f5f79ab75c854512", "ref_doc_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4"}, "c9b8f352-4f3f-441a-addd-f009db814b6a": {"doc_hash": "b1810a20499ef513027b8a2a8ab4183faaa074b2c5fa4c673d7c80bb25dc8b37", "ref_doc_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4"}, "dfe44657-e45d-4d05-850e-65108df04c44": {"doc_hash": "353b41a426b7cf03b3d993da237f27c01f92cc479cf855becfc25666dc9ed5d9", "ref_doc_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4"}, "5dc9fdbf-24a7-4bd6-9bc9-1e2121ffeccd": {"doc_hash": "1d2f644acc70be51507da3b8d99113407e8954150b915c0b22175388c871f516", "ref_doc_id": "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4"}, "7a8b215a-9e7a-4085-9a9f-9ca9650f5bde": {"doc_hash": "1ad88b98cb344026ce3173f659f04c2f8496cb5e9f97ebb487e48d1620d3248f", "ref_doc_id": "b293d45b-6ae1-4bbc-b549-50431cbbddcf"}, "569fcf26-224b-459a-9c7a-28d69b951b76": {"doc_hash": "7e87d9593d814155c7d48f91f85a9bbb2cbba63fedf84a142e0360962dede27d", "ref_doc_id": "00821df1-46e5-4f24-9733-627c63fa7193"}, "6d1dc7fb-4980-4e57-9586-804dba9f54f0": {"doc_hash": "4b678d8932d9be538acf4c3613dad8bc821424ab755fbe1b390d0057e9fe55e7", "ref_doc_id": "00821df1-46e5-4f24-9733-627c63fa7193"}, "2808af67-c8e9-418e-90f5-e2906405b7f2": {"doc_hash": "6431444ec23e4c4b7d1520afe373af05ba2aeb92af886656d63c8351fe26c70b", "ref_doc_id": "00821df1-46e5-4f24-9733-627c63fa7193"}, "299d1b08-d852-4fe2-9474-826673130b7a": {"doc_hash": "13c85c0f682d630555b16d79c23b531d82b8b99006700cbcc636ea1fa81a432e", "ref_doc_id": "00821df1-46e5-4f24-9733-627c63fa7193"}, "91a84ae8-5bd9-40b0-96f7-ec19ecba5e7f": {"doc_hash": "b430f2a9c249f8dc3c94d777b6dd8230f8eb62d536c24635ab6b5a5edad3bbd2", "ref_doc_id": "00821df1-46e5-4f24-9733-627c63fa7193"}, "97a1dcc6-a59a-4661-8ffc-9b545bf9dd76": {"doc_hash": "9e0ac55c08a531f2659f3b5c401a2c1ab6927fc3a15cfc88eb91ab0a3f08b98d", "ref_doc_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58"}, "f71aadfd-9c6b-45bd-a77b-94f1df020654": {"doc_hash": "e0d4061533952de6eddedc02c9fe4b3218fb1c3d8c702762fc97373d4ccfcc5b", "ref_doc_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58"}, "0d78b9fc-b73f-438e-87b2-a96cc536ae59": {"doc_hash": "4c319aa84e073dbc4c6849db4ca8ec39e60134a9f9cf9cb32f53f1e7871057c7", "ref_doc_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58"}, "327e5dac-8077-4076-bb6c-a583b893c8a3": {"doc_hash": "cb7afd3cdaf26a7eb75fcad8b4d6b54fdc43a20e0d572a0d3829fa0ad2ca9a1a", "ref_doc_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58"}, "dfb015cc-9ce0-4787-b95f-c36bc0299591": {"doc_hash": "94175c9d7b2ee891b0a49c0da989de3fc6c4fc46704d187710aadd2728bd1188", "ref_doc_id": "0b7fa9d2-6028-4406-b856-ccb1daad3a58"}, "44c18575-34fe-4375-9c77-1ef9114c4c7b": {"doc_hash": "ce7c00aa9f6fa4986f628d9164cd7d1dca495272067ad3f72c40696b07edd459", "ref_doc_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c"}, "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f": {"doc_hash": "bcb1d7eb7f29940126ede54e55cc7cdbc87bd4e1e3123b988201dcd5519d4476", "ref_doc_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c"}, "03f0766f-3c01-439f-9584-20eb51f4c2d8": {"doc_hash": "0b9dcaa8ae594c5dbd668b55433c58ceae46a5b2cf0adbace3250252fbd4e28a", "ref_doc_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c"}, "c15d4371-261d-44c7-91d9-b2f2cff903f1": {"doc_hash": "9a486ed878a6ba424e35b3f0874b45e56e14635cfd8f81106354241826d2fd4f", "ref_doc_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c"}, "98760a13-7772-4e2d-937c-4aa36a6342d9": {"doc_hash": "f54816904dcd3562a9dd50465272063930fc5fa2bd8e3fdd4a2fe55d8f0ede55", "ref_doc_id": "938608bd-cfa6-4c4a-a770-fa7ac060dc4c"}, "eb04e912-9cbc-4e1c-86ac-d85de4ccff08": {"doc_hash": "d1312d5238dc41d2cbccad134c6baa007e6662a8fe4daf00fb72cb9878f183d4", "ref_doc_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61"}, "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954": {"doc_hash": "86cbff7ce78b43a24c4db9b3f7375685af462134cfe22ec2e608d287f08fb4a9", "ref_doc_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61"}, "04651f94-3b5b-4c58-b540-df3a6f9bc4d1": {"doc_hash": "b9b79ab6674569f0567521af59d5a561b06c0671f5b552d130147cca7d87429f", "ref_doc_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61"}, "3a2c72d9-1e14-4918-bfa0-042c67e7dfee": {"doc_hash": "7233a467228b714f73d823314517c2ca65cfcaa6864be66d41ab6c1349f546a4", "ref_doc_id": "fa0483c5-b5ed-490a-b7d3-c6972582bc61"}, "194e47d5-07a0-403c-be14-d68931a214fd": {"doc_hash": "5f47a7213d73e4062da793dcd2e91000d0c073a6075da776ee2f483b943af897", "ref_doc_id": "cf72c77f-d210-49e0-9fbc-868444937960"}, "865e3761-95fb-4411-91d9-ac4d0dfcd283": {"doc_hash": "249e92979f223a88197eb309e23eba1fd65a0a63e8300f295dfbe1cde2df86ea", "ref_doc_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31"}, "35ef8e42-ebd3-4e50-a73a-7713e2e392bf": {"doc_hash": "6e827ed0b282c86f15af3feab69830d6a9dd1c0dbc1f34fd66835e5cdc2f9590", "ref_doc_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31"}, "9ed73b61-29d1-4d79-a72f-31b0ba4720db": {"doc_hash": "0ae7eedeb692c45a07c9cde61cee9d91230660fb218cb8eaa3ee913ade2cb584", "ref_doc_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31"}, "93be767d-58e9-4eb5-b5a2-ad7da310f65a": {"doc_hash": "9764ff814aa500b14c71356f01880c6f2af4a672e1eba723a4507abd8d97164c", "ref_doc_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31"}, "47d6cb48-779a-494b-975f-025463507e59": {"doc_hash": "210a83a2134c257f1ce783ed19e8e3eac631dfd168c568a856212687b72e360b", "ref_doc_id": "dcd36521-18d4-4ede-889d-47fb5fc3bf31"}, "030bf9d4-f9b9-4807-a491-0d4610cefc62": {"doc_hash": "5d1bc46103b700e7ccf868c592a49bc69875078abd29799b2238815e343fe9c4", "ref_doc_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1"}, "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760": {"doc_hash": "c00d28ffe836ebd6e04fce57a6a04c55006e7fa9ddb04017304f1a79018364c4", "ref_doc_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1"}, "372512df-6ff4-4419-99e5-ad456f68de6b": {"doc_hash": "1aa6f5d6cac54932996d81e12eb65a20de89b2c694df5d192ad4b54ae2d36a7a", "ref_doc_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1"}, "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977": {"doc_hash": "4e5528fee7d90b6129d8e3fcdd26cae5576a02fd5d78722bf5631e35307bec2c", "ref_doc_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1"}, "646b8540-05fd-4678-aff4-24dbf948a091": {"doc_hash": "e98b70145d33dbe10dee7d7de26e8eb394f414e3e4805701777d28882882451f", "ref_doc_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1"}, "8ffe4f91-6837-4828-b359-5f577da1ff25": {"doc_hash": "a83d3cab9bb95b9b881b6c56908f1d1499c1cbecf192617b2b8bb43200034206", "ref_doc_id": "88403054-34c4-4fdf-b50c-5918c1f3f7e1"}, "f4506df3-328d-4f3b-9c4e-abfe21c4b0e3": {"doc_hash": "8d24655c3c289158f664a09c2c8ffd76d1cd10e78accf706b5b9d49283566200", "ref_doc_id": "e7643626-84dd-4b5c-9902-528a44aea517"}, "77a457c2-7d13-4288-abbc-dcf01a46b622": {"doc_hash": "5e120b1f46ebc87188435070c95b0b660fa1395e8a0d26d456b0522955bd7cfe", "ref_doc_id": "e7643626-84dd-4b5c-9902-528a44aea517"}, "d14b14ed-9d86-4380-afb6-492ca0707b9e": {"doc_hash": "2b09bca843911747300ed1c764bf8d0abcf7e9583f0e82bbe10fe6f99c0fa1ac", "ref_doc_id": "e7643626-84dd-4b5c-9902-528a44aea517"}, "762eeb22-f099-43ce-97a4-4489a833421f": {"doc_hash": "dc5ca7518521c3832b86bcc99698c947c581f4a3028333bc9f0d6d883fe76698", "ref_doc_id": "e7643626-84dd-4b5c-9902-528a44aea517"}, "83c13dfa-f71b-4b13-948a-39bb24be72c7": {"doc_hash": "b16b63256fa2713297a89e4bdfac015ffc59305a2d1d2be13339825b5432df4d", "ref_doc_id": "e7643626-84dd-4b5c-9902-528a44aea517"}, "96132b0f-5d0c-4595-846a-dfaa5aa4d96c": {"doc_hash": "0d1fba54a9b9ef2cdcb85a879c0ae852119619ce73615052558363bb156722b7", "ref_doc_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047"}, "099d8859-ce61-4540-ab3f-9498a5174a42": {"doc_hash": "3ff518db952502edde0416aee8616833cfa722f454b3cc8b4263372b1eddcc7a", "ref_doc_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047"}, "ce79aab2-672c-4686-9725-b0a6eb169b08": {"doc_hash": "d5ffcb4343736012565e71ba7c6c0a63eb17c35a7dcaa8285cdd9dd2bcce0f6d", "ref_doc_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047"}, "88de296f-fdfe-4280-a61d-295f376a5dfa": {"doc_hash": "ee6d58168312ef6393bac703e4c688ebf6eff5af80b5c5ffd731465e8c9346ec", "ref_doc_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047"}, "56bf4afc-039f-40c3-8a55-23add43f8a96": {"doc_hash": "1920f198d01d71637da2af45d2633e07ed5a24d432f04e2970498bf4954aaeff", "ref_doc_id": "013731c3-85e2-48ed-9aff-b7ec67c9c047"}, "9eea138e-4fc5-42bf-a900-cdd43e150767": {"doc_hash": "10c23ac973aadf1cd734297f77e3cb5d5de433c69480737b0daaeb0df057bbb3", "ref_doc_id": "535af1ae-4b4b-4082-9a2b-ecb9c394b99e"}, "8e8db431-5af6-4297-90bd-cb0af84599ce": {"doc_hash": "210e449fb73d7fa079fff6e98a296274ac20848b1310156fac3b433ccd0fc1db", "ref_doc_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc"}, "40f356cb-9551-4638-ace0-da392e7add13": {"doc_hash": "63c3a5a9c49467cbb8e3e78e2d8ae02f47e2fe0ff8bb6845e31a15871f307292", "ref_doc_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc"}, "6eff7521-1348-4f47-b7b6-1c821b83f958": {"doc_hash": "f006a46552b1347492357f5f52717aba3d77a541a7c75eb13d1ba56d77be7d3d", "ref_doc_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc"}, "1a1dbca4-bbe4-45ad-bc9c-9698765ed087": {"doc_hash": "b1725dd95cfe0f8baa6b86b0cfed26a1ec8416a84f1f920c68e85d0815667e62", "ref_doc_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc"}, "cf283e40-2254-4948-9e2a-828766fc97db": {"doc_hash": "7bc9b9f1e7a4ad2c5ce1162899103974b4c1d556195b0faa3ee33946edce9e4e", "ref_doc_id": "b6e6766d-037f-44f7-b12e-ef6e22966bfc"}, "459eccb1-3c03-4448-996f-fe0ba067dcbe": {"doc_hash": "ec4b7ace06fd0da472959fad6fc51f964d5f9fe12244da33f1f3b91ca77e63c4", "ref_doc_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067"}, "cf391a4a-f8a5-414f-8052-a04fc17a5661": {"doc_hash": "b78f6ca49abe05da07584314db3b86d24d8de540e8fc66eec55fe74e1d8e8366", "ref_doc_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067"}, "5a4f3de1-9319-414a-9290-a52ca78149cb": {"doc_hash": "f87eec113670817d6be03361dc961bdf33b6483af3667311d611389f8d9dd6cd", "ref_doc_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067"}, "2fb55683-a046-4f97-8148-64028440f7f5": {"doc_hash": "bb2193dd3ed3261d6d10371bf70db1e5912286b439bfe9f4182d09a23defcf69", "ref_doc_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067"}, "9fbe7842-cbe3-4e3f-b84c-fbed358d25b8": {"doc_hash": "389babc3bf60ecff5f785e09b0d9f380ed675ff1f2f1307c5eb895c394e803a7", "ref_doc_id": "4b70d562-31c4-4c4f-9f8b-c14d239ea067"}, "50d2cb4e-539a-439e-8c3b-15d446c31e28": {"doc_hash": "b3fc4a1ee231ced40ee8a071ba235203ed24add774e1511f04377023ba519cc1", "ref_doc_id": "47cda882-478b-46e5-a57e-2c079162b9bc"}, "e099c449-ef0c-4987-942d-5ec5644ff227": {"doc_hash": "d1696c29466420958ddec1b4435573f7a712be07a83771973ce3ef7738c5afb4", "ref_doc_id": "47cda882-478b-46e5-a57e-2c079162b9bc"}, "388d4767-5b8d-46de-b7f4-4a2caf2522a2": {"doc_hash": "9d8655c2cb390d20dc316ea78766dc2b87623f7b3fd97ccaa94d7ae3896a5572", "ref_doc_id": "47cda882-478b-46e5-a57e-2c079162b9bc"}, "06502e61-6ba0-4ef4-a63c-68d5451b5616": {"doc_hash": "97ea51abc01b4199f80b2a8e1e9d55f3eea5e90c474f8594a919edfee3982040", "ref_doc_id": "47cda882-478b-46e5-a57e-2c079162b9bc"}, "b346816f-0a6e-466f-b893-f155bc546628": {"doc_hash": "9f4287785f0215816114c8b2c2ec0700b99f75295f09ac2d7d8ccca1ce4b7240", "ref_doc_id": "47cda882-478b-46e5-a57e-2c079162b9bc"}, "e62f2851-38fd-44c0-b6bc-7eb4b8759ed3": {"doc_hash": "df6d8a8b6a568dd41f46abc88a09e47fcf4c43cd3e14c92c377ca8bf08b43adc", "ref_doc_id": "bef3ce88-7a33-4020-8720-0336a725ec9d"}, "5660390a-65cd-4b17-a5eb-c4b9096f2ef7": {"doc_hash": "f317894c2cdc0c7569aca7fcf09efe0062b377a718dcb69f51f9ca3498e33913", "ref_doc_id": "bef3ce88-7a33-4020-8720-0336a725ec9d"}, "0432442a-9644-4a91-ad45-7f1da1102faa": {"doc_hash": "c462c0f6c7c92c84256c8a3034916f1d902527910dacb8c15ce819537e208112", "ref_doc_id": "bef3ce88-7a33-4020-8720-0336a725ec9d"}, "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9": {"doc_hash": "d311716123fef415e3973110269cfd4c893341810de7132a15bd2ff2a1c19987", "ref_doc_id": "bef3ce88-7a33-4020-8720-0336a725ec9d"}, "a74b2f86-6ba6-4db3-a1d8-b14ef775ea9b": {"doc_hash": "e09758e7807d5f507011f6361fb9b4128426bdb1521f79f92cc9396ed88d00ed", "ref_doc_id": "bef3ce88-7a33-4020-8720-0336a725ec9d"}, "b282e0c1-33b9-4da3-93ab-7f38fb16b3df": {"doc_hash": "50bfc48f226fc80448d07f3713dc0b2b83fd2d8d0f9841bafb7a804ec5cc812c", "ref_doc_id": "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7"}, "cf7170d0-b23f-4eb2-8665-b82cffcd222d": {"doc_hash": "a8a0154637fe7aba3ecc034ce903888d886ae9c1e74f5b6def9cb0f97008539a", "ref_doc_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def"}, "d524de96-acb3-4502-97c4-cd3cbec8f6cc": {"doc_hash": "f7034d6d82e4337886853ca637966fb14d54083b4f2150fe077f586cfd05ec3c", "ref_doc_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def"}, "1335020b-6427-4796-ad05-853b7bd61c16": {"doc_hash": "8dc44cc4865e3e6ca895744beaf580fd46dfa84ba9bf92e09a7e68287c33584b", "ref_doc_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def"}, "ae83b356-b35c-441a-bc88-3cab957b97bb": {"doc_hash": "4f9c32aaae38899b6270ff3cfd00bd9223a3922ca7d9a6791e5dfec0311712e6", "ref_doc_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def"}, "98e06e1f-432e-4d60-9c09-f7ce436cf92c": {"doc_hash": "328e1f0333a6bba7cc5ba83ff5522c8eab08520c1edecf70ccbfd0da5a32d8a3", "ref_doc_id": "5b44ef94-6ea4-4c20-b24f-b48496a28def"}}, "docstore/ref_doc_info": {"ad8f3299-76b9-4412-b26d-0c99273842f0": {"node_ids": ["c335fb7f-ba4c-48a7-89cf-ae97f51db883", "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5", "d7c1c087-905a-41a0-88d3-92cf9a603438", "16196373-14c0-4390-a0db-8ac5b663447f", "cf090498-501a-4e10-b6fb-411592ea0e5a", "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0", "d208a291-bfa1-4867-9fa5-3c4617ebd24f"], "metadata": {}}, "c335fb7f-ba4c-48a7-89cf-ae97f51db883": {"node_ids": ["8e283248-217b-4607-a828-330db0cb3c4b", "cfc6a798-5b46-44dd-a887-491516a74f13", "843dae4f-9177-4be2-bea3-15b167c43370", "6944eebd-6e1a-49f1-82eb-910d01b2d95a", "00df92de-8dba-43a8-af11-19ce18c1df5e"], "metadata": {}}, "6393d867-5bbe-45e1-b9ce-1b1eb674e6b5": {"node_ids": ["3de7bbe1-118a-45de-ab42-327e9f821b94", "d18e9348-dfee-468f-85ad-d27e16ec8090", "689d7d2f-c966-411d-9425-fc11d5ac733d", "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de", "dac9c22d-586d-41fc-9efe-27c747d1c629"], "metadata": {}}, "d7c1c087-905a-41a0-88d3-92cf9a603438": {"node_ids": ["8e7527b6-f6c8-4689-8d08-a92632a126ed", "73dd9204-cda7-4bbd-b64c-daf82a689ddd", "adfb9d0c-8f3c-4922-be13-e6b17a2cb838", "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4", "b293d45b-6ae1-4bbc-b549-50431cbbddcf"], "metadata": {}}, "16196373-14c0-4390-a0db-8ac5b663447f": {"node_ids": ["00821df1-46e5-4f24-9733-627c63fa7193", "0b7fa9d2-6028-4406-b856-ccb1daad3a58", "938608bd-cfa6-4c4a-a770-fa7ac060dc4c", "fa0483c5-b5ed-490a-b7d3-c6972582bc61", "cf72c77f-d210-49e0-9fbc-868444937960"], "metadata": {}}, "cf090498-501a-4e10-b6fb-411592ea0e5a": {"node_ids": ["dcd36521-18d4-4ede-889d-47fb5fc3bf31", "88403054-34c4-4fdf-b50c-5918c1f3f7e1", "e7643626-84dd-4b5c-9902-528a44aea517", "013731c3-85e2-48ed-9aff-b7ec67c9c047", "535af1ae-4b4b-4082-9a2b-ecb9c394b99e"], "metadata": {}}, "6574ac1a-4a49-45d5-b47c-ac7b20d45bb0": {"node_ids": ["b6e6766d-037f-44f7-b12e-ef6e22966bfc", "4b70d562-31c4-4c4f-9f8b-c14d239ea067", "47cda882-478b-46e5-a57e-2c079162b9bc", "bef3ce88-7a33-4020-8720-0336a725ec9d", "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7"], "metadata": {}}, "d208a291-bfa1-4867-9fa5-3c4617ebd24f": {"node_ids": ["5b44ef94-6ea4-4c20-b24f-b48496a28def"], "metadata": {}}, "8e283248-217b-4607-a828-330db0cb3c4b": {"node_ids": ["0d9f52d2-3071-4ee8-a412-ef19afe74560", "4a71a6f5-0fb3-498e-b65d-2cbb3f867e52", "8fc2ed2a-4859-4c63-99a1-98da3c706bfb", "37fc32c5-6cd8-4dac-b4fe-f058d588625f", "177dd3e4-5571-4ead-9052-bd8f23b37f89"], "metadata": {}}, "cfc6a798-5b46-44dd-a887-491516a74f13": {"node_ids": ["aaa052f9-cbb5-41bf-aa39-144bb278d76f", "3fd74188-bb64-49a6-b643-06b173e30da9", "892913dc-5691-4b8c-b89a-7cbd4b290020", "a8420ba4-1923-44bc-ad6f-f039cb8dac2d", "9e90c49a-c44d-4a82-bd11-1fdbdc5845a4", "b6bc5afb-53ff-400c-92b8-660acb67c15e"], "metadata": {}}, "843dae4f-9177-4be2-bea3-15b167c43370": {"node_ids": ["a4f63a6b-d521-4b74-bce9-8970fa75e32f", "d560d311-94bc-47ae-bbbc-2a83a13270e2", "02531e66-29d7-4df8-89b7-184cb314c3f6", "3dfbe97c-0ec0-466d-95a7-85757c75dc0a", "a766a312-c240-47be-a6b9-6c8a0f30effe"], "metadata": {}}, "6944eebd-6e1a-49f1-82eb-910d01b2d95a": {"node_ids": ["c24ccb24-0ecf-4a0d-b736-aa3765f5e854", "2aa7ecba-c61b-49b5-b220-1fca807df300", "4e140397-4561-4086-b5d8-5b4f8095eeb4", "0a5ce3dd-96b5-4a93-98af-9f3cf6713c4c", "178675e5-5624-4cba-86c4-41443ef806be"], "metadata": {}}, "00df92de-8dba-43a8-af11-19ce18c1df5e": {"node_ids": ["b70a610f-59be-4ed3-9210-3fd4af7e34cb"], "metadata": {}}, "3de7bbe1-118a-45de-ab42-327e9f821b94": {"node_ids": ["8c26192c-4bb1-4fe5-92a4-0cfd6afc672c", "ccdc64e7-8578-402c-adfc-7f9bedebc2fa", "e748def4-5a55-4031-947f-9004d685688c", "7919a516-e2f9-4871-b726-7ef1660feebd", "70f7146f-e6e4-4d84-900e-aac0131533f9"], "metadata": {}}, "d18e9348-dfee-468f-85ad-d27e16ec8090": {"node_ids": ["25f39df5-e777-427c-a9a0-a661f7ca48ab", "f4023064-a0ff-4c94-a537-214611f7c55a", "78598da2-b279-46d4-aaf0-fefca676c30d", "d83d3a1f-ff9e-427f-9e4e-f398a4e065c8", "7159ecb2-ba9a-49a2-9294-ac6a6e52ccf7"], "metadata": {}}, "689d7d2f-c966-411d-9425-fc11d5ac733d": {"node_ids": ["f051495e-b4cf-462e-b42a-67f9b5ebd4c0", "43d26dba-e6a1-48c4-ac6f-513f6d1ceace", "1407828d-734e-4103-b05f-1c3429cf8784", "c17b65e8-1f37-4a18-beea-1450866636f4"], "metadata": {}}, "aa39f2ba-13d0-4adb-a79f-1364c6d8a3de": {"node_ids": ["2ae020f6-3531-47c0-acfb-5c2635f4d78f", "2b9a7b44-a020-4a1e-8471-3b9ca588f56d", "13103365-ee6e-468d-8679-47406168ad1e", "2970cdd8-7cc5-441e-9180-3f0d35aed72b", "ae2718cf-b324-43b2-80af-608864bbd060"], "metadata": {}}, "dac9c22d-586d-41fc-9efe-27c747d1c629": {"node_ids": ["3406b63b-8760-44bc-bad2-60f68ad85a22", "01020a24-b2d0-43f9-9c09-4f28a0ff2557"], "metadata": {}}, "8e7527b6-f6c8-4689-8d08-a92632a126ed": {"node_ids": ["b86032ac-9442-4216-ba6b-9afa9612fbb9", "123060e0-ef82-4211-9ac2-8b7958ed9ffc", "69cf0473-e62a-4861-a3ed-572985701870", "4c056a44-4ed4-4f29-af60-bc75de435d21", "d8b7d0cb-c03d-4972-abaa-0708bbb9beb0"], "metadata": {}}, "73dd9204-cda7-4bbd-b64c-daf82a689ddd": {"node_ids": ["e10709d3-92ff-47c2-8da9-27e20d05e99a", "065f54c3-b5f4-4ba2-b081-840c8b2cfcf8", "43987c9d-fcbe-4f29-bc90-36429a295adb", "852d47eb-cef2-4b84-9efd-e212ab46cff6", "1400cf08-5d42-45ab-a44d-1e465d29fd06"], "metadata": {}}, "adfb9d0c-8f3c-4922-be13-e6b17a2cb838": {"node_ids": ["97a9637e-637b-43a3-9842-34beced18813", "e16e2fef-45e6-4638-b561-ddd00e0ebee7", "2631d6e0-e208-4389-a568-5d229055d8b6", "7de4be5c-f717-43bd-b932-844e2b16d00b", "ba291a7d-f31c-411c-93a0-8fc765f10b12"], "metadata": {}}, "15520ef8-cfe7-4ff5-9864-f4ddda39d0c4": {"node_ids": ["08fa381b-8c67-4078-b3ca-6d13b5c81bf5", "364c343f-141b-46ce-aae0-fb2eb01030c0", "c9b8f352-4f3f-441a-addd-f009db814b6a", "dfe44657-e45d-4d05-850e-65108df04c44", "5dc9fdbf-24a7-4bd6-9bc9-1e2121ffeccd"], "metadata": {}}, "b293d45b-6ae1-4bbc-b549-50431cbbddcf": {"node_ids": ["7a8b215a-9e7a-4085-9a9f-9ca9650f5bde"], "metadata": {}}, "00821df1-46e5-4f24-9733-627c63fa7193": {"node_ids": ["569fcf26-224b-459a-9c7a-28d69b951b76", "6d1dc7fb-4980-4e57-9586-804dba9f54f0", "2808af67-c8e9-418e-90f5-e2906405b7f2", "299d1b08-d852-4fe2-9474-826673130b7a", "91a84ae8-5bd9-40b0-96f7-ec19ecba5e7f"], "metadata": {}}, "0b7fa9d2-6028-4406-b856-ccb1daad3a58": {"node_ids": ["97a1dcc6-a59a-4661-8ffc-9b545bf9dd76", "f71aadfd-9c6b-45bd-a77b-94f1df020654", "0d78b9fc-b73f-438e-87b2-a96cc536ae59", "327e5dac-8077-4076-bb6c-a583b893c8a3", "dfb015cc-9ce0-4787-b95f-c36bc0299591"], "metadata": {}}, "938608bd-cfa6-4c4a-a770-fa7ac060dc4c": {"node_ids": ["44c18575-34fe-4375-9c77-1ef9114c4c7b", "0c9b9eb6-a050-4843-9289-e61b6b3a2c4f", "03f0766f-3c01-439f-9584-20eb51f4c2d8", "c15d4371-261d-44c7-91d9-b2f2cff903f1", "98760a13-7772-4e2d-937c-4aa36a6342d9"], "metadata": {}}, "fa0483c5-b5ed-490a-b7d3-c6972582bc61": {"node_ids": ["eb04e912-9cbc-4e1c-86ac-d85de4ccff08", "d6a0557b-7e76-4a76-8cc1-64bd3aa0f954", "04651f94-3b5b-4c58-b540-df3a6f9bc4d1", "3a2c72d9-1e14-4918-bfa0-042c67e7dfee"], "metadata": {}}, "cf72c77f-d210-49e0-9fbc-868444937960": {"node_ids": ["194e47d5-07a0-403c-be14-d68931a214fd"], "metadata": {}}, "dcd36521-18d4-4ede-889d-47fb5fc3bf31": {"node_ids": ["865e3761-95fb-4411-91d9-ac4d0dfcd283", "35ef8e42-ebd3-4e50-a73a-7713e2e392bf", "9ed73b61-29d1-4d79-a72f-31b0ba4720db", "93be767d-58e9-4eb5-b5a2-ad7da310f65a", "47d6cb48-779a-494b-975f-025463507e59"], "metadata": {}}, "88403054-34c4-4fdf-b50c-5918c1f3f7e1": {"node_ids": ["030bf9d4-f9b9-4807-a491-0d4610cefc62", "7cb6ffbf-a6a0-43dd-8487-32ee9bf09760", "372512df-6ff4-4419-99e5-ad456f68de6b", "6568f52b-9ab7-4cd5-a2f1-f2b8e1bab977", "646b8540-05fd-4678-aff4-24dbf948a091", "8ffe4f91-6837-4828-b359-5f577da1ff25"], "metadata": {}}, "e7643626-84dd-4b5c-9902-528a44aea517": {"node_ids": ["f4506df3-328d-4f3b-9c4e-abfe21c4b0e3", "77a457c2-7d13-4288-abbc-dcf01a46b622", "d14b14ed-9d86-4380-afb6-492ca0707b9e", "762eeb22-f099-43ce-97a4-4489a833421f", "83c13dfa-f71b-4b13-948a-39bb24be72c7"], "metadata": {}}, "013731c3-85e2-48ed-9aff-b7ec67c9c047": {"node_ids": ["96132b0f-5d0c-4595-846a-dfaa5aa4d96c", "099d8859-ce61-4540-ab3f-9498a5174a42", "ce79aab2-672c-4686-9725-b0a6eb169b08", "88de296f-fdfe-4280-a61d-295f376a5dfa", "56bf4afc-039f-40c3-8a55-23add43f8a96"], "metadata": {}}, "535af1ae-4b4b-4082-9a2b-ecb9c394b99e": {"node_ids": ["9eea138e-4fc5-42bf-a900-cdd43e150767"], "metadata": {}}, "b6e6766d-037f-44f7-b12e-ef6e22966bfc": {"node_ids": ["8e8db431-5af6-4297-90bd-cb0af84599ce", "40f356cb-9551-4638-ace0-da392e7add13", "6eff7521-1348-4f47-b7b6-1c821b83f958", "1a1dbca4-bbe4-45ad-bc9c-9698765ed087", "cf283e40-2254-4948-9e2a-828766fc97db"], "metadata": {}}, "4b70d562-31c4-4c4f-9f8b-c14d239ea067": {"node_ids": ["459eccb1-3c03-4448-996f-fe0ba067dcbe", "cf391a4a-f8a5-414f-8052-a04fc17a5661", "5a4f3de1-9319-414a-9290-a52ca78149cb", "2fb55683-a046-4f97-8148-64028440f7f5", "9fbe7842-cbe3-4e3f-b84c-fbed358d25b8"], "metadata": {}}, "47cda882-478b-46e5-a57e-2c079162b9bc": {"node_ids": ["50d2cb4e-539a-439e-8c3b-15d446c31e28", "e099c449-ef0c-4987-942d-5ec5644ff227", "388d4767-5b8d-46de-b7f4-4a2caf2522a2", "06502e61-6ba0-4ef4-a63c-68d5451b5616", "b346816f-0a6e-466f-b893-f155bc546628"], "metadata": {}}, "bef3ce88-7a33-4020-8720-0336a725ec9d": {"node_ids": ["e62f2851-38fd-44c0-b6bc-7eb4b8759ed3", "5660390a-65cd-4b17-a5eb-c4b9096f2ef7", "0432442a-9644-4a91-ad45-7f1da1102faa", "07b09cdb-13ab-4146-a6cf-75b7cf8c5ab9", "a74b2f86-6ba6-4db3-a1d8-b14ef775ea9b"], "metadata": {}}, "a4a04f2c-b9b6-4a24-b2fe-5578f9d7b3a7": {"node_ids": ["b282e0c1-33b9-4da3-93ab-7f38fb16b3df"], "metadata": {}}, "5b44ef94-6ea4-4c20-b24f-b48496a28def": {"node_ids": ["cf7170d0-b23f-4eb2-8665-b82cffcd222d", "d524de96-acb3-4502-97c4-cd3cbec8f6cc", "1335020b-6427-4796-ad05-853b7bd61c16", "ae83b356-b35c-441a-bc88-3cab957b97bb", "98e06e1f-432e-4d60-9c09-f7ce436cf92c"], "metadata": {}}}}