{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion\n",
    "In this section we will cover a retrieval optimization technique called query expansion. This involves adding more context and tags to the user query to be able to produce better more accurate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral7b = ChatOpenAI(model=\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_prompt = \"\"\"\n",
    "[INST]\n",
    "You are an intelligent and insightful assistant.\n",
    "Generate a potential response the following question:\n",
    "Question:\n",
    "{question}\n",
    "Response:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "query_expansion_template = ChatPromptTemplate.from_template(query_expansion_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_chain = query_expansion_template | mistral7b | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_summary_prompt = \"\"\" \n",
    "[INST]\n",
    "You are an intelligent and insgihtful assistant.\n",
    "Generate a summary for the given context.\n",
    "Context:\n",
    "{expanded_query}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "answer_summary_template = ChatPromptTemplate.from_template(answer_summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_summary_chain = answer_summary_template | mistral7b | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_answering_prompt = \"\"\" \n",
    "[INST]\n",
    "You are a helpful assistant. You give grounded and accurate answers based on the context provided within the message.\n",
    "Here is the context\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "\n",
    "Use the provided context to answer the following question:\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Provide only the answer and no other explanation\n",
    "[/INST]\n",
    "\"\"\"\n",
    "context_answering_prompt = ChatPromptTemplate.from_template(context_answering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_answering_chain = context_answering_prompt | mistral7b | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.api.client import Client\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = PdfReader(\"MIV2 - LLM paper.pdf\")\n",
    "docs = [ page.extract_text().strip() for page in doc.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\".\", \"\", \" \", \"\\n\", \"\\n\\n\"],\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "texts = recursive_splitter.split_text(\"\\n\\n\".join(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosisochukwuasuzu/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kosisochukwuasuzu/Developer/ai-startups/test-demos/pdfchat/venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "sentence_t_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "split_texts = []\n",
    "for text in texts:\n",
    "    split_texts+= sentence_t_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = SentenceTransformerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "collection = client.create_collection(\"collection2\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(i) for i in range(len(split_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids), len(split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "collection.add(ids, documents=split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_expansion_chain.invoke({\"question\": \"what evaluations were used in the research paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In the research paper, various evaluation methods were employed to ensure '\n",
      " 'the validity and reliability of the findings. Here are some of the '\n",
      " 'evaluations mentioned:\\n'\n",
      " '\\n'\n",
      " '1. Content analysis: This method was used to analyze the data collected from '\n",
      " 'various sources, such as articles and books, to identify patterns and '\n",
      " 'trends.\\n'\n",
      " '2. Surveys: The researchers used self-report surveys to gather data from '\n",
      " 'participants about their attitudes, beliefs, and behaviors related to the '\n",
      " 'research topic.\\n'\n",
      " '3. Interviews: In-depth interviews were conducted with key informants to '\n",
      " 'gain a deeper understanding of the research topic and to validate the '\n",
      " 'findings from the surveys and content analysis.\\n'\n",
      " '4. Observations: The researchers observed participants in their natural '\n",
      " 'settings to gain insights into their behaviors and interactions.\\n'\n",
      " '5. Document analysis: The researchers analyzed various documents related to '\n",
      " 'the research topic, such as policies and reports, to understand the context '\n",
      " 'and background of the research.\\n'\n",
      " '\\n'\n",
      " 'Overall, the researchers used a combination of qualitative and quantitative '\n",
      " 'methods to evaluate the data and ensure the validity and reliability of the '\n",
      " 'findings.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = collection.query(query_texts=response, n_results=5)[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['. negative results are observed primarily in tasks prompted with a single '\n",
      "  'modality, aligning with the inherent limitations associated with '\n",
      "  'insufficient contextual information. observations during experiments reveal '\n",
      "  'consistent plan generation errors by the model for specific instructions, '\n",
      "  'and even after feedback is provided for a particular mistake, the model '\n",
      "  'tends to repeat 10 table 2 : vimabench simulated environment evaluation '\n",
      "  'single modality1multi modality2 project task 1 task 2 task 3 task 1 task 2 '\n",
      "  'task 3 instruct2act 0. 0 % 70 % 90 % 40 % 63. 3 % 76. 66 % instruct2act w / '\n",
      "  'feedback 0. 0 % 66 % 90 % 46. 66 % 73. 33 % 93. 3 % note : the success rate '\n",
      "  'is calculated by finding the average success of task completion for each '\n",
      "  'task across the 3 partitions. 1for single modality only textual data is '\n",
      "  'provided to the context of the model. 2for multi modality both texts and '\n",
      "  'images are provided as contexts to the model. these errors once the message '\n",
      "  'stream is cleared',\n",
      "  '. 3 experimental setup and procedure to empirically assess the '\n",
      "  'effectiveness of the system, a series of experiments were conducted in both '\n",
      "  'simulated and physical environments, each involving a solitary human '\n",
      "  'participant. a terminal environment was provided to facilitate the '\n",
      "  'transmission of textual instructions and feedback to the system. throughout '\n",
      "  'the experiment, the human participant interacted with the robot, issuing '\n",
      "  'high - level natural language instructions, feedback, and adjustment '\n",
      "  'commands. the robot ’ s language understanding component, powered by an '\n",
      "  'llm, interpreted and responded to these instructions and commands. the '\n",
      "  'planning and execution process remained dynamic, incorporating iterative '\n",
      "  'plan adjustments in response to the singular user ’ s feedback. 3. 1 '\n",
      "  'simulated evaluation the vimabench is a simulation benchmark of multi - '\n",
      "  'modal prompts for robotics [ 5 ]. it interleaves textual and visual tokens',\n",
      "  '. monte carlo tree search, a powerful search algorithm, was employed in '\n",
      "  'this exploration. this involved integrating the world model into the policy '\n",
      "  'for a search algorithm like mcts. concurrently, llms with tree - of - '\n",
      "  'thought ( llm - tot ) approach adopted a tree - based strategy, enabling '\n",
      "  'llms to engage in deliberate decision - making by assessing various '\n",
      "  'reasoning paths and self - evaluating choices to determine the subsequent '\n",
      "  'course of action [ 46 ]. the results of these studies highlight the '\n",
      "  'effectiveness of search - based plan generation over policies induced by '\n",
      "  'llms. nevertheless, given the straightforward nature of the tasks in these '\n",
      "  'experiments, the decision is made to refrain from employing the search - '\n",
      "  'based approach. instead, reliance is placed on the llm - induced policies '\n",
      "  'in their original form',\n",
      "  '. 2for multi modality both texts and images are provided as contexts to the '\n",
      "  'model. these errors once the message stream is cleared. this recurrence is '\n",
      "  'attributed to the absence of a memory system that would retain feedback and '\n",
      "  'context across experiments. the language model demonstrates generalisation '\n",
      "  'to new objects and employs novel approaches in calling the apis beyond the '\n",
      "  'provided examples. although infrequent, the model may respond with natural '\n",
      "  'language outputs outside the defined api, and providing feedback '\n",
      "  'instructions corrects such occurrences. figure 7 : the human user provides '\n",
      "  'the instruction ’ place the green block in the matching colored bowl ’. the '\n",
      "  'llm uses this instruction to generate a plan which is then executed by the '\n",
      "  'physical robot table 3 presents the outcomes of the physical evaluation '\n",
      "  'conducted on the system. the findings indicate that the incorporation of '\n",
      "  'scene descriptions yields an 8',\n",
      "  '. while these set of manipulations are limited this system can be extended '\n",
      "  'to support more complex skills that can be composed together to achieve a '\n",
      "  'larger set of tasks. leveraging the vimabench environment for bench - '\n",
      "  'marking provided valuable insights into the system ’ s performance, '\n",
      "  'enabling an evaluation of its proficiency in comprehending and responding '\n",
      "  'to a diverse range of language instructions. the environment wrapper '\n",
      "  'facilitated real - time feedback and adjustment, aligning the approach with '\n",
      "  'principles of adaptability and continuous improvement. in quantitative '\n",
      "  'metrics, the system demonstrated 8. 6 % performance increase in simulated '\n",
      "  'eval - uations and a 14 % increase in physical evaluations. this shows that '\n",
      "  'the method can be further explored to enable collaborative task planning '\n",
      "  'between humans and robots in physical and virtual environments. however, '\n",
      "  'there are areas for improvement']]\n"
     ]
    }
   ],
   "source": [
    "pprint(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(relevant_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =context_answering_chain.invoke({\"context\": context, \"question\": \"what evaluations were used in the research paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The research paper conducted evaluations in both simulated and physical '\n",
      " 'environments using the Vimabench simulation benchmark and human user '\n",
      " 'interactions.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = collection.query(query_texts=[\"what evaluations were used in the research paper?\"])[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The research paper conducted evaluations in both simulated and physical '\n",
      " 'environments using a series of experiments. In the simulated evaluation, '\n",
      " 'they used the Vimabench simulation benchmark, while in the physical '\n",
      " 'evaluation, they used the VILD1SAM-CLIP methodology. The findings indicate '\n",
      " 'an 8.6% performance increase in simulated evaluations and a 14% increase in '\n",
      " 'physical evaluations. The system demonstrated an ability to comprehend and '\n",
      " 'respond to a diverse range of language instructions, with valuable insights '\n",
      " 'into its performance and areas for improvement.')\n"
     ]
    }
   ],
   "source": [
    "response =context_answering_chain.invoke({\"context\": context, \"question\": \"what evaluations were used in the research paper?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using query expansion by generating extra questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "[INST]\n",
    "You are a helpful research asssistant.\n",
    "Generate 5 additional related questions to the one provided below to help the user get the information they need faster.\n",
    "Question:\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "question_generation_template = ChatPromptTemplate.from_template(question_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question_expansion_chain = question_generation_template | mistral7b | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_questions = new_question_expansion_chain.invoke({\"question\": \"what evaluations were used in the research paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. What specific evaluation methods or techniques were employed in the '\n",
      " 'research study mentioned in the paper?\\n'\n",
      " '2. Can you provide details about the data collection and analysis methods '\n",
      " 'used in the evaluation process?\\n'\n",
      " '3. Were any standardized evaluation tools or scales used in the study and if '\n",
      " 'so, which ones?\\n'\n",
      " '4. How were the evaluation results interpreted and what conclusions were '\n",
      " 'drawn from them?\\n'\n",
      " '5. Were there any limitations or challenges encountered during the '\n",
      " 'evaluation process and how were they addressed?')\n"
     ]
    }
   ],
   "source": [
    "pprint(more_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_questions += \"\\n6. what evaluations were used in the research paper?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. What specific evaluation methods or techniques were employed in the '\n",
      " 'research study mentioned in the paper?\\n'\n",
      " '2. Can you provide details about the data collection and analysis methods '\n",
      " 'used in the evaluation process?\\n'\n",
      " '3. Were any standardized evaluation tools or scales used in the study and if '\n",
      " 'so, which ones?\\n'\n",
      " '4. How were the evaluation results interpreted and what conclusions were '\n",
      " 'drawn from them?\\n'\n",
      " '5. Were there any limitations or challenges encountered during the '\n",
      " 'evaluation process and how were they addressed?\\n'\n",
      " '6. what evaluations were used in the research paper?')\n"
     ]
    }
   ],
   "source": [
    "pprint(more_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = collection.query(query_texts=[more_questions], n_results=5)[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = context_answering_chain.invoke({\"question\":  \"what evaluations were used in the research paper?\", \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The research paper conducted evaluations in both simulated and physical '\n",
      " 'environments using the Vimabench simulation benchmark and a solitary human '\n",
      " \"participant. The system's performance in comprehending and responding to a \"\n",
      " 'diverse range of language instructions was assessed, and the approach was '\n",
      " 'found to be adaptable and continuously improving with real-time feedback and '\n",
      " 'adjustment. The system demonstrated a 8.6% performance increase in simulated '\n",
      " 'evaluations and a 14% increase in physical evaluations. Additional related '\n",
      " 'projects and methods were also evaluated, including tinybot, Palm-E, '\n",
      " 'code-as-policies, progprompt, and LLM-Brain.')\n"
     ]
    }
   ],
   "source": [
    "pprint(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
