{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Finetuning\n",
    "This notebook explores the idea of fientuning language models to behave like agents, for starters we will be considering the use of the fireact finetuning architecture, FireAct focuses on finetuning LLMs to behave like agents following the ReACT framework and for multiple tasks like HotpotQA, StrategyQA and bamboogle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuning an openai model\n",
    "In this section we will cover finetuning an openai model, particularly GPT3.5. We will be using trajectories generated from gpt-4 to finetune our model, and we will be performing all of this using the openai finetuning API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI files\n",
    "OpenAI has this concept of files, the files contain the training data which would be used in finetuning our model, we first upload the data as an openai file object, then we can then start a finetuning operation using the uploaded file as our tuning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create openai client\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the files available in your account\n",
    "files = client.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with all the file data sorted by when it was created\n",
    "pd.DataFrame(sorted(files.data, key=lambda k: -k['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all finetuning jobs\n",
    "jobs = client.fine_tuning.jobs.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the base models and finetuned models this account has access to\n",
    "models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new file to be uploaded to openai\n",
    "file = open(\"data.jsonl\", \"r+\")\n",
    "\n",
    "response = client.files.create(file, purpose='fine-tune', user_provided_filename=file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create finetuning job\n",
    "# file_id = response.file_id\n",
    "file_id = None\n",
    "n_epochs = 20\n",
    "job = client.fine_tuning.jobs.create(model=\"gpt-3.5-turbo\", training_file=file_id, hyperparameters={\"n_epochs\": n_epochs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive job\n",
    "job = client.fine_tuning.jobs.retrieve(job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the events for a particular job\n",
    "events = client.fine_tuning.jobs.list_events(job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also cancel a job \n",
    "cancelled_job = client.fine_tuning.jobs.cancel(job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuning with Llama2\n",
    "In this section we will explore finetuning the LLama2 model. We will explore both full finetuning and LoRa finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Union\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "StrOrOpenAIObject = Union[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenaiDecodingArguments(object):\n",
    "    max_tokens: int = 1800\n",
    "    temperature: float = 0.2\n",
    "    top_p: float = 1.0\n",
    "    n: int = 1\n",
    "    stream: bool = False\n",
    "    stop: Optional[Sequence[str]] = None\n",
    "    presence_penalty: float = 0.0\n",
    "    frequency_penalty: float = 0.0\n",
    "    suffix: Optional[str] = None\n",
    "    logprobs: Optional[int] = None\n",
    "    echo: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_completion(\n",
    "    prompts: Union[str, Sequence[str], dict[str, str], Sequence[dict[str, str]]], \n",
    "    decoding_args: OpenaiDecodingArguments\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_and_path: Optional[str] = field(default=\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"the path to the training data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: str = field(default=512, \n",
    "                                  metadata={\"help\": \"the maximum sequence length, sequences will be right padded or trauncated\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\" \n",
    "    Collects the state dict and saves it on disk\n",
    "    \"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save == True:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(special_tokens_dict: dict,\n",
    "                                         tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                         model: transformers.PreTrainedModel):\n",
    "    \"\"\" \n",
    "    Resize tokenizer and embeddings\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "        \n",
    "        input_embeddings_avg = input_embeddings[: -num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[: -num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        \n",
    "        \n",
    "        input_embeddings[-num_new_tokens: ] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens: ] = output_embeddings_avg\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement trainer for full model finetuning for Llama2 model\n",
    "In this section we will be writing the trainer function for full model finetuning for llama2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    parser = transformers.HfArgumentParser(ModelArguments, DataArguments, TrainingArguments)\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir\n",
    "    )\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False\n",
    "    )\n",
    "    \n",
    "    if \"code\" in model_args.model_name_or_path.lower():\n",
    "        tokenizer.add_eos_token=True\n",
    "        tokenizer.pad_token_id=0,\n",
    "        tokenizer.padding_side=\"left\"\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "    if \"llama\" in model_args.model_name_or_path.lower():\n",
    "        tokenizer.add_special_tokens({\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    data_module = make_supervised_data_module()\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "    trainer.train()\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer, output_dir=training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
